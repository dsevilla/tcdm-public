{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "blessed-detection",
   "metadata": {},
   "source": [
    "# Práctica 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-taiwan",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Objetivo de la práctica y entrega de documentación \n",
    "\n",
    "En esta práctica utilizaremos contenedores Docker para desplegar un\n",
    "cluster Hadoop.\n",
    "\n",
    "Para poder usar docker y completar la práctica, vuestro PC debe cumplir ciertos requisitos:\n",
    "\n",
    "1. 8 GB RAM (mínimo recomendado)\n",
    "2. Preferiblemente, sistema operativo Linux\n",
    "3. Si usáis una MV con Linux, la MV debería tener 2 cores y 6-8 GB de RAM y disponer de aceleración por hardware\n",
    "3. En caso de tener Windows, versión 10 o superior con WSL2 ó Hyper-V activado\n",
    "\n",
    "Instalación de Docker:\n",
    "\n",
    "- Linux: mira en https://docs.docker.com/engine/install/ cómo instalar Docker-engine para tu distribución.\n",
    "- Windows: descarga el instalador desde https://docs.docker.com/desktop/windows/install/\n",
    "\n",
    "\n",
    "**Actividad guiada**\n",
    "\n",
    "1.  Instalar manualmente un cluster Hadoop con contenedores Docker\n",
    "2.  Ejecutar una aplicación Java simple de demostración\n",
    "\n",
    "**Tareas a realizar**\n",
    "\n",
    "1.  Añadir un nodo de Backup y un TimelineServer\n",
    "2.  Añadir y retirar nodos Datanodes/Nodemanagers\n",
    "3.  Hacer que el cluster sea *rack-aware*\n",
    "\n",
    "**Entrega obligatoria**\n",
    "\n",
    "-   Generar una memoria con las capturas de pantalla especificadas, en el que se demuestre que se han realizado\n",
    "    y entendido las tareas propuestas. Este documento tiene que incluir\n",
    "    capturas de pantalla en las que, como mínimo, se muestre lo\n",
    "    siguiente:\n",
    "    1.  Capturas de pantalla que demuestren el funcionamiento del nodo\n",
    "        de Backup y del TimeLineServer.\n",
    "    2.  Capturas de pantalla en las que se vea un nuevo nodo añadido y\n",
    "        uno retirado (*decomisionado*).\n",
    "    3.  Captura de pantalla en la que se vea los nodos separados por\n",
    "        rack.\n",
    "-   Más detalles se muestran al final de la descripción de cada una de\n",
    "    las tareas propuestas.\n",
    "\n",
    "**IMPORTANTE**: El documento no puede consistir en una secuencia de\n",
    "imágenes. Es necesario describir cómo se han ejecutado cada una de las\n",
    "tareas propuestas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-marriage",
   "metadata": {},
   "source": [
    "## Practica: Instalación y despliegue de un cluster Hadoop 3 \n",
    "\n",
    "### **Objetivo**\n",
    "\n",
    "Instalar un cluster Hadoop usando contenedores Docker:\n",
    "\n",
    "-   Un cluster de 5 máquinas: 1 NameNode/ResourceManager y 4\n",
    "    DataNodes/NodeManagers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-hollywood",
   "metadata": {},
   "source": [
    "###  Apartado 1: Creación de imágenes Docker para los diferentes servicios \n",
    "\n",
    "Vamos a crear contenedores específicos para los servicios de\n",
    "NameNode/ResourceManager y DataNode/NodeManager. Antes de nada, crea una\n",
    "red en la que iniciaremos los contenedores Docker:\n",
    "\n",
    "```bash\n",
    "docker network create hadoop-cluster\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-zealand",
   "metadata": {},
   "source": [
    "#### 1.1 Servicio NameNode/ResourceManager \n",
    "\n",
    "Inicia un contenedor para esos servicios ejecutando:\n",
    "\n",
    "```bash\n",
    "docker container run -ti --name namenode --network=hadoop-cluster --hostname namenode --net-alias resourcemanager --expose=8000-10000 -p 9870:9870 -p 8088:8088 dsevilla/hadoop-base /bin/bash\n",
    "```\n",
    "\n",
    "El contenedor se identifica con ambos nombres `namenode` y `resourcemanager` y es accesible desde el host a través de los puertos 9870 (NameNode) y 8088 (ResourceManager).\n",
    "\n",
    "\n",
    "##### 1.1.1 Creación de directorios para los datos del **NameNode**\n",
    "\n",
    "Debemos especificar el directorio o directorios en el que el NameNode guardará la metainformación de HDFS. En un sistema real se deben usar por lo menos dos directorios: uno en el disco local del NameNode (preferible disponer de varios discos configurados en RAID) y otro remoto (por ejemplo, montado por NFS).\n",
    "\n",
    "\n",
    "En nuestro caso, crearemos un único directorio y haremos que sea propiedad del usuario `hdadmin`, que será el que ejecute los demonios del\n",
    "NameNode y del ResourceManager, ejecutando en el docker, como root, los comandos:\n",
    "\n",
    "\n",
    "```bash\n",
    "mkdir -p /var/data/hdfs/namenode\n",
    "chown hdadmin:hadoop /var/data/hdfs/namenode\n",
    "```\n",
    "\n",
    "##### 1.1.2 Configuración los demonios NameNode/ResourceManager\n",
    "\n",
    "\n",
    "Todos los demonios de Hadoop se configuran, principalmente, mediante cuatro ficheros, localizados en `$HADOOP_HOME/etc/hadoop/`, en los que se\n",
    "pueden indicar un gran número de propiedades (véase http://hadoop.apache.org/docs/stable3/hadoop-project-dist/hadoop-common/ClusterSetup.html para más información):\n",
    "\n",
    "- `core-site.xml`: configuración principal, valores por defecto en http://hadoop.apache.org/docs/stable3/hadoop-project-dist/hadoop-common/core-default.xml\n",
    "\n",
    "- `hdfs-site.xml`: configuración del HDFS, valores por defecto en http://hadoop.apache.org/docs/stable3/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml\n",
    "\n",
    "- `yarn-site.xml`: configuración del YARN, valores por defecto en http://hadoop.apache.org/docs/stable3/hadoop-yarn/hadoop-yarn-common/yarn-default.xml\n",
    "\n",
    "- `mapred-site.xml`: configuración del MapReduce, valores por defecto en http://hadoop.apache.org/docs/stable3/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml\n",
    "\n",
    "Para nuestro NameNode/ResourceManager, como usuario hdadmin (`su - hdadmin`), cambia los siguientes ficheros en `$HADOOP_HOME/etc/hadoop/`:\n",
    "\n",
    "- ***core-site.xml***: configuración general de Hadoop\n",
    "\n",
    "```xml\n",
    "<configuration>\n",
    "\n",
    "  <property>\n",
    "    <!-- Nombre del filesystem por defecto -->\n",
    "    <!-- Como queremos usar HDFS tenemos que indicarlo con hdfs:// y el servidor y puerto en el que corre el NameNode -->\n",
    "    <name>fs.defaultFS</name>\n",
    "    <value>hdfs://namenode:9000/</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <!-- Directorio para almacenamiento temporal (debe tener suficiente espacio) -->\n",
    "    <name>hadoop.tmp.dir</name>\n",
    "    <value>/var/tmp/hadoop-${user.name}</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "- ***hdfs-site.xml***: configuración del demonio NameNode (HDFS)\n",
    "\n",
    "```xml\n",
    "<configuration>\n",
    "\n",
    " <property>\n",
    "   <!-- Factor de replicacion de los bloques -->\n",
    "    <name>dfs.replication</name>\n",
    "    <value>3</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "   <!-- Tamano del bloque (por defecto 128m) -->\n",
    "    <name>dfs.blocksize</name>\n",
    "    <value>64m</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <!-- Lista (separada por comas) de directorios donde el namenode guarda los metadatos. -->\n",
    "    <name>dfs.namenode.name.dir</name>\n",
    "    <value>file:///var/data/hdfs/namenode</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <!-- Dirección y puerto del interfaz web del namenode -->\n",
    "    <name>dfs.namenode.http-address</name>\n",
    "    <value>namenode:9870</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "- ***yarn-site.xml***: configuración del demonio ResourceManager (YARN)\n",
    "\n",
    "```xml\n",
    "<configuration>\n",
    "\n",
    "  <property>\n",
    "    <!-- Nombre del equipo que ejecuta el demonio ResourceManager -->\n",
    "    <name>yarn.resourcemanager.hostname</name>\n",
    "    <value>resourcemanager</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <!-- Número máximo de vcores que un ApplicationMaster puede pedir al RM (por defecto: 4) -->\n",
    "    <!-- Peticiones mayores lanzan una InvalidResourceRequestException -->\n",
    "    <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
    "    <value>1</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <!-- Memoria minima (MB) que un ApplicationMaster puede solicitar al RM (por defecto: 1024) -->\n",
    "    <!-- La memoria asignada a un contenedor será múltiplo de esta cantidad -->\n",
    "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
    "    <value>128</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <!-- Memoria maxima (MB) que un ApplicationMaster puede solicitar al RM (por defecto: 8192 MB) -->\n",
    "    <!-- Peticiones mayores lanzan una InvalidResourceRequestException -->\n",
    "    <!-- Puedes aumentar o reducir este valor en funcion de la memoria de la que dispongas -->\n",
    "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
    "    <value>4096</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "- ***mapred-site.xml***: configuración del framework MapReduce de Hadoop\n",
    "\n",
    "```xml\n",
    "<configuration>\n",
    "\n",
    "  <property>\n",
    "    <!-- Framework que realiza el MapReduce -->\n",
    "    <name>mapreduce.framework.name</name>\n",
    "    <value>yarn</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <!-- Configuracion del ApplicationMaster (AM) del MR -->\n",
    "\n",
    "  <property>\n",
    "    <!-- Localizacion del software MR para el AM -->\n",
    "    <name>yarn.app.mapreduce.am.env</name>\n",
    "    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <!-- Numero maximo de cores para el ApplicationMaster (por defecto: 1) -->\n",
    "    <name>yarn.app.mapreduce.am.resource.cpu-vcores</name>\n",
    "    <value>1</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <!-- Memoria que necesita el ApplicationMaster del MR (por defecto: 1536) -->\n",
    "    <name>yarn.app.mapreduce.am.resource.mb</name>\n",
    "    <value>1536</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "\n",
    "  <!-- Configuracion de los maps y reduces del MR -->\n",
    "\n",
    "  <property>\n",
    "    <!-- Ratio del tamaño del heap al tamaño del contenedor para las JVM (por defecto: 0.8)-->\n",
    "    <name>mapreduce.job.heap.memory-mb.ratio</name>\n",
    "    <value>0.8</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <!-- Maps -->\n",
    "  <property>\n",
    "    <!-- Localizacion del software MR para los maps -->\n",
    "    <name>mapreduce.map.env</name>\n",
    "    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <!-- Numero maximo de cores para cada tarea map (por defecto: 1) -->\n",
    "    <name>mapreduce.map.cpu.vcores</name>\n",
    "    <value>1</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <!-- Opciones para las JVM de los maps -->\n",
    "    <name>mapreduce.map.java.opts</name>\n",
    "    <value>-Xmx3072M</value> <!-- \"-Xmx\" define el tamaño máximo de la pila de Java -->\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <!-- Memoria maxima (MB) por map (si -1 se optiene a partir de mapreduce.map.java.opts y mapreduce.job.heap.memory-mb.ratio) -->\n",
    "    <name>mapreduce.map.memory.mb</name>\n",
    "    <value>-1</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <!-- Reduces -->\n",
    "  <property>\n",
    "    <!-- Localizacion del software MR para los reducers -->\n",
    "    <name>mapreduce.reduce.env</name>\n",
    "    <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <!-- Numero maximo de cores para cada tarea reduce (por defecto: 1) -->\n",
    "    <name>mapreduce.reduce.cpu.vcores</name>\n",
    "    <value>1</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <!-- Opciones para las JVM de los reduces -->\n",
    "    <name>mapreduce.reduce.java.opts</name>\n",
    "    <value>-Xmx3072M</value> <!-- Xmx define el tamaño máximo de la pila de Java -->\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <!-- Memoria maxima (MB) por reduce (si -1 se optiene a partir de mapreduce.map.java.opts y mapreduce.job.heap.memory-mb.ratio) -->\n",
    "    <name>mapreduce.reduce.memory.mb</name>\n",
    "    <value>-1</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "</configuration>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-brand",
   "metadata": {},
   "source": [
    "##### 1.1.3 Inicializa el HDFS\n",
    "\n",
    "Es necesario inicializar el sistema HDFS ejecutando, como usuario hdadmin:\n",
    "\n",
    "```bash\n",
    "hdfs namenode -format\n",
    "```\n",
    "\n",
    "\n",
    "Al finalizar el proceso de inicialización, si todo fue bien debería aparecer, entre otros mensajes, lo siguiente:\n",
    "\n",
    "```bash\n",
    "Storage directory /var/data/hdfs/namenode has been successfully formatted.\n",
    "```\n",
    "\n",
    "Revisa ese directorio para comprobar qué se ha creado. Comprueba también\n",
    "que se ha creado un directorio para los logs en `$HADOOP_HOME/logs`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-northern",
   "metadata": {},
   "source": [
    "##### 1.1.4 Inicio de los demonios\n",
    "\n",
    "\n",
    "1. Inicia el demonio NodeManager ejecutando (como usuario `hdadmin`):\n",
    "\n",
    "```bash\n",
    "hdfs --daemon start namenode\n",
    "```\n",
    "\n",
    "Mira los ficheros creados en el directorio de logs (`$HADOOP_HOME/logs`) para comprobar que todo ha ido bien y que no aparecen errores. Ejecuta el comando `jps` para ver que la JVM está corriendo.\n",
    "\n",
    "2. Inicia el demonio ResourceManager ejecutando (como usuario `hdadmin`):\n",
    "\n",
    "```bash\n",
    "yarn --daemon start resourcemanager\n",
    "```\n",
    "\n",
    "Mira de nuevo  los ficheros creados en el directorio de logs (`$HADOOP_HOME/logs`) para comprobar que todo ha ido bien y que no aparecen errores. Ejecuta el comando `jps` para ver que la JVM está corriendo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-waste",
   "metadata": {},
   "source": [
    "##### 1.1.5 Acceso a los interfaces web de los demonios\n",
    "\n",
    "Los diferentes servicios de Hadoop ofrecen una interfaz web de control. Abre un navegador y comprueba los siguientes enlaces:\n",
    "\n",
    "- http://localhost:9870 interfaz web del HDFS.\n",
    "- http://localhost:8088 interfaz web de YARN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-dakota",
   "metadata": {},
   "source": [
    "##### 1.1.6  Parar los demonios\n",
    "\n",
    "Para detener los demonios, ejecutamos las instrucciones anteriores en cambiando start por stop:\n",
    "\n",
    "```bash\n",
    "yarn --daemon stop resourcemanager\n",
    "hdfs --daemon stop namenode\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-fever",
   "metadata": {},
   "source": [
    "##### 1.1.7 Automatización del inicio\n",
    "\n",
    "Para que podamos parar y reiniciar los contenedores y que los demonios sigan funcionando, vamos a salvar el contenedor como imagen, pero antes vamos a preparar un script para iniciar los demonios de forma automática cuando iniciemos el docker. Para ello, crea, **como root**, el fichero `/inicio.sh` en el directorio raíz, con el siguiente contenido:\n",
    "\n",
    "```bash\n",
    "#! /bin/sh\n",
    "export JAVA_HOME=/usr/lib/jvm/default-java\n",
    "export HADOOP_HOME=/opt/bd/hadoop\n",
    "\n",
    "# Inicio el NameNode y el ResourceManager\n",
    "su - hdadmin -c \"$HADOOP_HOME/bin/hdfs --daemon start namenode\"\n",
    "su - hdadmin -c \"$HADOOP_HOME/bin/yarn --daemon start resourcemanager\"\n",
    "\n",
    "# Lazo para mantener activo el contenedor\n",
    "while true; do sleep 10000; done\n",
    "```\n",
    "\n",
    "Además, dale permisos de ejecución al fichero:\n",
    "\n",
    "```bash\n",
    "chmod +x /inicio.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-liechtenstein",
   "metadata": {},
   "source": [
    "##### 1.1.8 Creación de la imagen Docker para el DataNode/NodeManager\n",
    "\n",
    "Sal del contenedor y crea una imagen a partir del mismo, haciendo:\n",
    "\n",
    "```bash\n",
    "docker container commit namenode namenode-image\n",
    "```\n",
    "\n",
    "Comprueba que la imagen se han creado (ejecuta `docker images`) y si todo está bien, borra el contenedor:\n",
    "\n",
    "```bash\n",
    "docker container rm namenode\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-hotel",
   "metadata": {},
   "source": [
    "#### 1.2 Servicio DataNode/NodeManager\n",
    "\n",
    "El proceso es muy similar al del caso anterior. Empieza iniciando un contenedor para esos servicios ejecutando:\n",
    "\n",
    "```bash\n",
    "docker container run -ti --name datanode --network=hadoop-cluster --hostname datanode --expose=8000-10000 --expose=50000-50200 dsevilla/hadoop-base /bin/bash\n",
    "```\n",
    "\n",
    "El contenedor se identifica en la red con el nombre datanode y no publica ningún puerto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-stuart",
   "metadata": {},
   "source": [
    "##### 1.2.1 Creación de directorios para los datos de los DataNodes\n",
    "\n",
    "Debemos especificar el directorio o directorios en los que los DataNodes guardarán los bloques HDFS. Lo ideal es diponer de un nodo con varios discos y crear un directorio en cada uno de los discos locales para mejorar el rendimiento en los accesos. No es conveniente usar RAID, ya que HDFS garantiza la redundancia.\n",
    "\n",
    "En nuestro caso, crearemos un único directorio, ejecutando en el docker, como root, los comandos:\n",
    "\n",
    "```bash\n",
    "mkdir -p /var/data/hdfs/datanode\n",
    "chown hdadmin:hadoop /var/data/hdfs/datanode\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-trigger",
   "metadata": {},
   "source": [
    "##### 1.2.2 Configuración los demonios DataNode/NodeManager\n",
    "\n",
    "Para nuestro DataNode/NodeManager, como usuario hdadmin (`su - hdadmin`), cambia los siguientes ficheros en `$HADOOP_HOME/etc/hadoop/`:\n",
    "\n",
    "\n",
    "- ***core-site.xml***: configuración general de Hadoop, igual que en el NameNode [aquí](#1.1.2-Configuraci%C3%B3n-los-demonios-NameNode/ResourceManager)\n",
    "\n",
    "- ***hdfs-site.xml***: configuración del demonio DataNode (HDFS)\n",
    "\n",
    "```xml\n",
    "<configuration>\n",
    "\n",
    "    <property>\n",
    "      <!-- Lista (separada por comas) de directorios donde los DataNodes guardan los bloques HDFS -->\n",
    "      <name>dfs.datanode.data.dir</name>\n",
    "      <value>file:///var/data/hdfs/datanode</value>\n",
    "      <final>true</final>\n",
    "    </property>\n",
    "\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "- ***yarn-site.xml***: configuración del demonio NodeManager (YARN)\n",
    "\n",
    "```xml\n",
    "<configuration>\n",
    "\n",
    "  <property>\n",
    "    <!-- Nombre del equipo que ejecuta el demonio ResourceManager -->\n",
    "    <name>yarn.resourcemanager.hostname</name>\n",
    "    <value>resourcemanager</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <!-- Activa la auto-deteccion de las capacidades de los nodos (memoria y CPU) -->\n",
    "    <name>yarn.nodemanager.resource.detect-hardware-capabilities</name>\n",
    "    <value>true</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <!-- Número de vcores  que pueden alocarse para contenedores -->\n",
    "    <!-- Si vale -1, se detecta automáticamente (si la auto-deteccion está activada) -->\n",
    "    <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
    "    <value>-1</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <!-- MB de RAM física que puede ser reservada para los containers (por defecto: 8192) -->\n",
    "    <!-- debe ser menor que la RAM fisica, para que funcionen otros servicios -->\n",
    "    <!-- Si vale -1, se detecta automáticamente (si la auto-deteccion está activada) -->\n",
    "    <!-- Se puede cambiar por un valor fijo, p.e. 3072 (3 GB) -->\n",
    "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
    "    <value>-1</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <!-- Deshabilita el chequeo de los límites de uso de la memoria virtual -->\n",
    "    <name>yarn.nodemanager.vmem-check-enabled</name>\n",
    "    <value>false</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <!-- Indica a los NodeManagers que tienen que implementar el servicio de barajado MapReduce -->\n",
    "    <name>yarn.nodemanager.aux-services</name>\n",
    "    <value>mapreduce_shuffle</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <!-- Clase que implementa el servicio de barajado MapReduce -->\n",
    "    <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>\n",
    "    <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "- ***mapred-site.xml***: configuración del framework MapReduce de Hadoop, igual que en el Resourcemanager."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-anthony",
   "metadata": {},
   "source": [
    "##### 1.2.3 Inicio de los demonios\n",
    "\n",
    "1. Inicia el demonio NodeManager ejecutando (como usuario `hdadmin`):\n",
    "\n",
    "```bash\n",
    "hdfs --daemon start datanode\n",
    "```\n",
    "\n",
    "Mira los ficheros creados en el directorio de logs (`$HADOOP_HOME/logs`) para comprobar que no hay errores (debería indicar que no puede conectarse al namenode). Ejecuta el comando `jps` para ver que la JVM está corriendo.\n",
    "\n",
    "2. Inicia el demonio NodeManager ejecutando (como usuario `hdadmin`):\n",
    "\n",
    "```bash\n",
    "yarn --daemon start nodemanager\n",
    "```\n",
    "\n",
    "Mira de nuevo los ficheros creados en el directorio de logs (`$HADOOP_HOME/logs`) para comprobar que no hay errores. Ejecuta el comando `jps` para ver que la JVM está corriendo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-search",
   "metadata": {},
   "source": [
    "##### 1.2.4  Parar los demonios\n",
    "\n",
    "Para detener los demonios, ejecutamos las instrucciones anteriores en cambiando start por stop:\n",
    "\n",
    "```bash\n",
    "yarn --daemon stop nodemanager\n",
    "hdfs --daemon stop datanode\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-industry",
   "metadata": {},
   "source": [
    "##### 1.2.5 Automatización del inicio\n",
    "\n",
    "Para que los demonios se inicien al lanzar el Docker, de forma similar a como lo hicimos con el NameNode/ResourceManager crea, **como root**, el fichero `/inicio.sh` en el directorio raíz, con el siguiente contenido:\n",
    "\n",
    "```bash\n",
    "#! /bin/sh\n",
    "export JAVA_HOME=/usr/lib/jvm/default-java\n",
    "export HADOOP_HOME=/opt/bd/hadoop\n",
    "\n",
    "# Inicio el DataNode y el NodeManager\n",
    "su - hdadmin -c \"$HADOOP_HOME/bin/hdfs --daemon start datanode\"\n",
    "su - hdadmin -c \"$HADOOP_HOME/bin/yarn --daemon start nodemanager\"\n",
    "\n",
    "# Lazo para mantener activo el contenedor\n",
    "while true; do sleep 10000; done\n",
    "```\n",
    "\n",
    "Recuerda darle permisos de ejecución al fichero:\n",
    "\n",
    "```bash\n",
    "chmod +x /inicio.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-holiday",
   "metadata": {},
   "source": [
    "##### 1.2.6 Creación de la imagen Docker para el DataNode/NodeManager\n",
    "\n",
    "\n",
    "Sal del contenedor (`exit`) y crea una imagen a partir del mismo, haciendo:\n",
    "\n",
    "```bash\n",
    "docker container commit datanode datanode-image\n",
    "```\n",
    "\n",
    "Comprueba que la imagen se ha creado (ejecuta `docker images`) y si todo está bien, borra el contenedor:\n",
    "\n",
    "```bash\n",
    "docker container rm datanode\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-directory",
   "metadata": {},
   "source": [
    "### Apartado 2: Inicio del cluster Hadoop con contenedores Docker\n",
    "\n",
    "Vamos ya a iniciar nuestro cluster, formado por 1 NameNode/ResourceManager y 4 DataNodes/NodeManagers (en caso de disponer de poca RAM libre, puedes iniciar solo 3 de estos).\n",
    "\n",
    "1. Inicia el NameNode/ResourceManager (puedes ajustar las especificaciones de CPU y memoria en función del número de cores y RAM del equipo)\n",
    "\n",
    "```bash\n",
    "docker container run -d --name namenode --network=hadoop-cluster --hostname namenode --net-alias resourcemanager --cpus=1 --memory=3072m --expose=8000-10000 -p 9870:9870 -p 8088:8088 namenode-image /inicio.sh\n",
    "```\n",
    "\n",
    "2. Inicia los 4 DataNodes/NodeManagers (puedes ajustar las especificaciones de cpu y memoria en función del número de cores y RAM del equipo)\n",
    "\n",
    "```bash\n",
    "for i in {1..4}; do docker container run -d --name datanode$i --network=hadoop-cluster --hostname datanode$i --cpus=1 --memory=3072m --expose=8000-10000 --expose=50000-50200 datanode-image /inicio.sh; done\n",
    "```\n",
    "\n",
    "3. Comprueba con `docker container ps` que están ejecutándose los 5 contenedores.\n",
    "\n",
    "4. Conéctate al NameNode ejecutando:\n",
    "\n",
    "```bash\n",
    "docker container exec -ti namenode /bin/bash\n",
    "```\n",
    "\n",
    "   y una vez dentro de contenedor, como usuario hdadmin prueba que todo va bien ejecutando:\n",
    "\n",
    "```bash\n",
    "hdfs dfsadmin -report\n",
    "yarn node -list\n",
    "```\n",
    "\n",
    "5. Adicionalmente, comprueba en el interfaz web del NameNode y del NodeManager para ver si es correcto\n",
    "\n",
    "A partir de ahora, para detener los contenedores, basta hacer `docker container stop` y el nombre de cada uno y para volverlos a iniciar, simplemente `docker container start` y el nombre:\n",
    "\n",
    "\n",
    "```bash\n",
    "docker container stop namenode datanode{1..4}\n",
    "docker container start namenode datanode{1..4}\n",
    "```\n",
    "\n",
    "**Por seguridad, para los contenedores antes de apagar el PC o la máquina virtual**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-automation",
   "metadata": {},
   "source": [
    "### Apartado 3: Creación de directorios en HDFS y copia de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-pharmacology",
   "metadata": {},
   "source": [
    "####  3.1 Creación de los directorios de los usuarios en HDFS\n",
    "\n",
    "Dentro del Namenode, como usuario `hdadmin`, crea directorios en HDFS (dentro de `/user`) para el usuario `hdadmin` y para el usuario local `luser` (que es el que va a ejecutar las tareas MapReduce):\n",
    "\n",
    "```bash\n",
    "hdfs dfs -mkdir -p /user/hdadmin\n",
    "hdfs dfs -mkdir -p /user/luser\n",
    "hdfs dfs -chown luser /user/luser\n",
    "hdfs dfs -ls /user\n",
    "```\n",
    "\n",
    "Debemos crear también un directorio `/tmp` y darle los permisos adecuados\n",
    "\n",
    "```bash\n",
    "hdfs dfs -mkdir -p /tmp/hadoop-yarn/staging\n",
    "hdfs dfs -chmod -R 1777 /tmp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-mistress",
   "metadata": {},
   "source": [
    "#### 3.2 Copia de ficheros de usuario\n",
    "\n",
    "\n",
    "1. En el Namenode, conviertete en usuario no privilegiado (haciendo, como root, `su - luser`)\n",
    "\n",
    "2. Comprueba que puedes acceder al HDFS ejecutando:\n",
    "\n",
    "```bash\n",
    "hdfs dfs -ls           \n",
    "```\n",
    "  (Este comando no debería dar ninguna salida, ya que no tenemos nada en ese directorio)\n",
    "\n",
    "3. Descarga de https://umubox.um.es/index.php/s/TDBPLG59W3BrOaD/download unos ficheros de ejemplo que usaremos en esta practica y cópialos al contenedor del NameNode usando:\n",
    "\n",
    "```bash\n",
    "docker container cp ~/Downloads/libros.tar namenode:/tmp \n",
    "```\n",
    "\n",
    "4. En el NameNode \"destarea\" el fichero y copia los datos a HDFS\n",
    "\n",
    "```bash\n",
    "cd /tmp; tar xvf libros.tar\n",
    "hdfs dfs -put libros .\n",
    "hdfs dfs -ls libros\n",
    "```\n",
    "\n",
    "  Una vez copiados borra el directorio libros del disco local (`rm -rf /tmp/libros`) y, como root, el fichero `libros.tar`\n",
    "    \n",
    "\n",
    "5. Mira en el interfaz web del HDFS donde se encuentran los bloques correspondientes al fichero **random_words.txt.bz2**. Comprueba que cada bloque tiene 3 réplicas.\n",
    "   - Abre el navegador y conéctate al interfaz web del NameNode\n",
    "   - Ve al menu \"Utilities\" -> \"Browse the filesystem\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-tribune",
   "metadata": {},
   "source": [
    "### Apartado 4: Prueba de aplicaciones MapReduce simples "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-whole",
   "metadata": {},
   "source": [
    "#### 4.1 Aplicación MapReduce para el cálculo de Pi \n",
    "\n",
    "En el NameNode, **como usuario `hdadmin`**, ejecuta el siguiente ejemplo\n",
    "de aplicación MapReduce:\n",
    "\n",
    "```bash\n",
    "export MAPRED_EXAMPLES=$HADOOP_HOME/share/hadoop/mapreduce\n",
    "yarn jar $MAPRED_EXAMPLES/hadoop-mapreduce-examples-*.jar pi 16 1000\n",
    "```\n",
    "\n",
    "Comprueba en el interfaz web de Yarn la ejecución de esta tarea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-tiger",
   "metadata": {},
   "source": [
    "#### 5.2 Aplicación WordCount\n",
    "\n",
    "Descarga el código Java del wordcount de\n",
    "[aquí](https://github.com/dsevilla/tcdm-public/raw/21-22/practicas/p1/wordcount.tgz).\n",
    "\n",
    "1. Cópialo en el NameNode\n",
    "\n",
    "```bash\n",
    "docker cp ~/Downloads/wordcount.tgz namenode:/home/luser\n",
    "```\n",
    "\n",
    "2. En el NameNode, como usuario `luser` descomprímelo y compílalo usando maven\n",
    "    \n",
    "```bash\n",
    "cd; tar xvzf wordcount.tgz\n",
    "cd wordcount\n",
    "mvn package\n",
    "```\n",
    "\n",
    "3. Ejecútalo con el comando Yarn:\n",
    "\n",
    "```bash\n",
    "yarn jar target/wordcount*.jar libros/p* wordcount-out\n",
    "```\n",
    "\n",
    "  Comprueba en el interfaz web de Yarn la ejecución de esta tarea.\n",
    "\n",
    "4. Trae los ficheros de salida del HDFS al disco local del NameNode:\n",
    "    \n",
    "```bash\n",
    "hdfs dfs -get wordcount-out\n",
    "```\n",
    "\n",
    "5. Comprueba los ficheros de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-retirement",
   "metadata": {},
   "source": [
    "## Tareas a realizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c3959-19ce-4e36-a220-c928946483ab",
   "metadata": {},
   "source": [
    "### Tarea 1: Añadir al cluster un servidor de Backup y un TimeLineServer \n",
    "\n",
    "#### 1. Servidor de Backup\n",
    "\n",
    "El servidor de backup realiza una tarea doble:\n",
    "\n",
    "1.  Mantiene una copia de seguridad permanentemente actualizada de los metadatos del NameNode\n",
    "2.  Realiza tareas de Checkpoint sobre estos metadatos\n",
    "\n",
    "Más información sobre este servicio en https://hadoop.apache.org/docs/stable3/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Backup_Node\n",
    "\n",
    "**Importante**: Antes de iniciar el servicio de backup, inicia el clúster, ve al NameNode y obtén una captura de pantalla en la que se vean los ficheros del directorio de metadatos del NameNode dentro de current (`/var/data/hdfs/namenode/current`) e inclúyela en la memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e35d4d-234c-4471-92c9-9320a1c9d31f",
   "metadata": {},
   "source": [
    "Para añadir el servidor de backup, tenéis que seguir los siguientes\n",
    "pasos (con el cluster funcionando):\n",
    "\n",
    "1. Inicia un nuevo Docker a partir de la imagen hadoop-base de la siguiente forma:\n",
    "\n",
    "```bash\n",
    "docker container run -ti --name backupnode --network=hadoop-cluster --hostname backupnode --cpus=1 --memory=3072m --expose=50100 -p 50105:50105 dsevilla/hadoop-base /bin/bash\n",
    "```\n",
    "\n",
    "2. Crea un directorio donde se guardarán los backups. Haz que el propietario de ese directorio sea hdadmin y crea dentro del mismo la carpeta `dfs/name`\n",
    "\n",
    "\n",
    "3. Como usuario hdadmin, añade al fichero `core-site.xml` las siguentes propiedades\n",
    "\n",
    "a) `fs.defaultFS`: Nombre del filesystem por defecto. Dale el valor `hdfs://namenode:9000/`.\n",
    "\n",
    "b) `hadoop.tmp.dir`: Indica el directorio donde se guardarán las copias de seguridad. Dale el valor del directorio que has creado (sin incluir `dfs/name`).\n",
    "\n",
    "\n",
    "4. Como usuario `hdadmin`, añade al fichero `hdfs-site.xml` las siguentes propiedades\n",
    "\n",
    "a) `dfs.namenode.backup.address`: Dirección y puerto del nodo de backup. Dale el valor `backupnode:50100`.\n",
    "\n",
    "b) `dfs.namenode.backup.http-address`:  Dirección y puerto del servicio web del nodo de backup. Dale el valor `backupnode:50105`.\n",
    "\n",
    "5. Inicia el servidor de backup ejecutando:\n",
    "\n",
    "```bash\n",
    "hdfs namenode -backup\n",
    "```\n",
    "\n",
    "6. Analiza el directorio de backup para ver lo que se ha creado. Compáralo con el directorio con los metadatos del NameNode\n",
    "\n",
    "7. Mira en los mensajes del servicio de backup información que indique que se ha realizado un checkpoint\n",
    "\n",
    "**Nota**: Una vez obtenidos los datos para el ejercicio, puedes parar el servicio de backup. Si quieres poder reiniciarlo de forma facil, sal del contenedor, guarda el Docker como una imagen e inícialo haciendo:\n",
    "\n",
    "\n",
    "```bash\n",
    "docker container run -d --name backupnode --network=hadoop-cluster --hostname backupnode --cpus=1 --memory=3072m --expose=50100 -p 50105:50105 backupnode-image su - hdadmin -c \"JAVA_HOME=/usr/lib/jvm/default-java /opt/bd/hadoop/bin/hdfs namenode -backup\"\n",
    "```\n",
    "\n",
    "Y para comprobar que se está ejecutando correctamente el servicio de backup, haz:\n",
    "\n",
    "```bash\n",
    "docker container logs backupnode\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-alliance",
   "metadata": {},
   "source": [
    "##### Información para la documentación en la memoria\n",
    "\n",
    "Incluir en la memoria:\n",
    "\n",
    "\n",
    "1. Captura de pantalla en la que se vean los mensajes que genera el servicio de backup, destacando aquellos en los que se vea como se hace el checkpoint\n",
    "2. Captura de pantalla en la que se compare el contenido del directorio del backup con el directorio con los metadatos de NameNode, antes y una vez que el servicio de backup se ha completado \n",
    "3. Captura de pantalla del interfaz web del nodo de backup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-mouse",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2. TimeLineServer \n",
    "\n",
    "El servidor de *línea temporal* de YARN mantiene un histórico y proporciona métricas de las aplicaciones ejecutadas mediante YARN (es similar a la funcionalidad del Job History Server porporcionado por MapReduce).\n",
    "\n",
    "Proporciona tanto información genérica acerca de aplicaciones completadas (contenedores en los que se ejecutó la aplicación, intentos de ejecución, el nombre del usuario, de la cola, etc.) como información específica del framework concreto de la aplicación (por ejemplo, el framework MapReduce puede publicar información sobre el número de maps y reduces, u otros contadores). La información es accesible a través de un interfaz web o vía una API REST.\n",
    "\n",
    "El Timeline Server se ejecuta como un demonio standalone que puede correr en un nodo del cluster o colocarse con el ResourceManager. Más información sobre el servicio en\n",
    "https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/TimelineServer.html.\n",
    "\n",
    "\n",
    "Para añadir el TimeLineServer, tenéis que seguir los siguientes pasos (con el cluster funcionando):\n",
    "\n",
    "\n",
    "1. Ve al NameNode/ResourceManager y detén el servicio ResourceManager\n",
    "\n",
    "2. En este sistema, edita el fichero `yarn-site.xml` y añade las siguentes propiedades:\n",
    "\n",
    "a) `yarn.timeline-service.hostname`: Nombre del equipo que ejecutará el demonio de línea de tiempo por defecto. Dale el valor `timelineserver` (llamaremos de esta forma al Docker que ejecutará el servicio).\n",
    "    \n",
    "b) `yarn.timeline-service.enabled`: Indica que si el servicio de linea de tiempo está activo o no. Dale el valor `true`.\n",
    "\n",
    "c) `yarn.system-metrics-publisher.enabled`: Le indica al ResourceManager que publique las metricas de YARN en el timeline server. Dale el valor `true`.\n",
    "\n",
    "\n",
    "3. Reinicia el servicio ResourceManager\n",
    "\n",
    "\n",
    "\n",
    "4. Inicia un nuevo Docker a partir de la imagen hadoop-base de la siguiente forma:\n",
    "\n",
    "\n",
    "```bash\n",
    "docker container run -ti --name timelineserver --network=hadoop-cluster --hostname timelineserver --cpus=1 --memory=3072m --expose=10200 -p 8188:8188 hadoop-base /bin/bash\n",
    "```\n",
    "\n",
    "5. En este nuevo Docker, levanta el servicio timelineserver ejecutando:\n",
    "\n",
    "```bash\n",
    "yarn --daemon start timelineserver\n",
    "```\n",
    "\n",
    "6. Vuelve al NameNode/ResourceManager y ejecuta una aplicación con yarn (la de el cálculo de pi o el wordcount).\n",
    "\n",
    "\n",
    "7. Comprueba en el servidor web del TimeLineServer (http://localhost:8188) que se recoge la información de la ejecución\n",
    "\n",
    "##### **Información para la memoria**\n",
    "\n",
    "Incluir en la memoria:\n",
    "\n",
    "1. Captura de pantalla del interfaz web del TimeLineServer en la que se vea que se ha recogido la información de la ejecución de una o más tareas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-supply",
   "metadata": {},
   "source": [
    "### Tarea 2: Añadir un nuevo DataNode/NodeManager "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-algebra",
   "metadata": {},
   "source": [
    "#### 1. Creación de ficheros de nodos incluidos y excluidos\n",
    "\n",
    "\n",
    "Aunque no es estrictamente necesario para añadir o retirar nodos del cluster, es conveniente tener una lista en la que podamos indicar los nodos que se pueden añadir o retirar del cluster. Para ello, haced lo siguiente en el NameNode (como usuario `hdadmin`):\n",
    "\n",
    "Para los demonios del NameNode y del ResourceManager.\n",
    "\n",
    "Crea cuatro ficheros: `${HADOOP_HOME}/etc/hadoop/dfs.include`, `${HADOOP_HOME}/etc/hadoop/dfs.exclude`, `${HADOOP_HOME}/etc/hadoop/yarn.include`\n",
    "y `${HADOOP_HOME}/etc/hadoop/yarn.exclude` (inicialmente vacíos).\n",
    "\n",
    "En los ficheros `dfs.include` y `yarn.include`, poned los nombres de todos los DataNodes/NodeManagers que querramos que estén en el cluster (datanode1, datanode2, datanode3 y datanode4, un nombre por línea). Deja los ficheros `dfs.exclude` y `yarn.exclude` vacíos.\n",
    "\n",
    "En el fichero de configuración `hdfs-site.xml`, añade dos propiedades:\n",
    "\n",
    "- `dfs.hosts`: nombre de un fichero con lista de hosts que pueden actuar como DataNodes; si el fichero está vacío, cualquier nodo están permitido. Dale como valor, el path completo al fichero `dfs.include`.\n",
    "- `dfs.hosts.exclude`: nombre de un fichero con lista de hosts que no pueden actuar como DataNodes; si el fichero está vacío, ninguno está excluido. Dale como valor, el path completo al fichero `dfs.exclude`.\n",
    "\n",
    "En el fichero `yarn-site.xml`, añade dos propiedades:\n",
    "\n",
    "- `yarn.resourcemanager.nodes.include-path`: nombre de un fichero con la lista de hosts que pueden actuar como NodeManagers; si el fichero está vacío, cualquier nodo están permitido. Dale como valor, el path completo al fichero `yarn.include`.\n",
    "- `yarn.resourcemanager.nodes.exclude-path`: nombre de un fichero con la lista de hosts que no pueden actuar como NodeManagers; si el fichero está vacío, ninguno está excluido. Dale como valor, el path completo al fichero `yarn.exclude`.\n",
    "\n",
    "Reinicia los demonios del NameNode y del ResourceManager.\n",
    "\n",
    "Comprueba en los ficheros de log que se han incluido al HDFS y al YARN\n",
    "los nodos `datanode{1,2,3,4}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-flower",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2. Añadir un datanode/nodemanager\n",
    "\n",
    "Vamos a añadir un nuevo DataNode/NodeManager al cluster:\n",
    "\n",
    "1. En el NameNode, añade el nombre de un nuevo nodo (datanode5) en el fichero `yarn.include` (no lo añadas, de momento, en el `dfs.include`).\n",
    "\n",
    "2. Actualiza el ResourceManager con el nuevo NodeManager ejecutando:\n",
    "\n",
    "```bash\n",
    "yarn rmadmin -refreshNodes\n",
    "```\n",
    "\n",
    "3. Iniciar un nuevo contenedor para hacer de DataNode/NodeManager:\n",
    "\n",
    "```bash\n",
    "docker container run -d --name datanode5 --network=hadoop-cluster --hostname datanode5 --cpus=1 --memory=3072m --expose=8000-10000 --expose=50000-50200 datanode-image /inicio.sh\n",
    "```\n",
    "\n",
    "4. Comprueba usando (en el NameNode como usuario hdadmin) los comandos `hdfs dfsadmin -report` y `yarn node -list` que el nuevo contenedor se ha añadido al YARN pero no al HDFS\n",
    "\n",
    "5. Añade ahora el nombre del nuevo nodo al ficheros `dfs.include`\n",
    "\n",
    "6. Actualiza el NameNode con el nuevo DataNode ejecutando:\n",
    "\n",
    "```bash\n",
    "hdfs dfsadmin -refreshNodes\n",
    "```\n",
    "\n",
    "7. Comprueba de nuevo que ahora el contenedor sí está incluido en el HDFS. Puedes comprobarlo también el interfaz web del NameNode y de  YARN.\n",
    "\n",
    "El nuevo nodo, inicialmente está vacío (no tiene datos de HDFS), con lo\n",
    "que el cluster estará desbalanceado. Se puede forzar el balanceo\n",
    "ejecutando, en el NameNode:\n",
    "\n",
    "```bash\n",
    "hdfs balancer\n",
    "```\n",
    "\n",
    "Para más información, véase https://hadoop.apache.org/docs/stable3/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#balancer\n",
    "\n",
    "##### Información para la memoria\n",
    "\n",
    "Incluir en la memoria capturas de pantalla en las que se vean:\n",
    "\n",
    "1. Las líneas de los ficheros de log del namenode y del resourcemanager que muestran que se han incluido los nodos indicados en los ficheros include (punto 4 del apartado 1 **Creación de ficheros de nodos incluidos y excluidos**)\n",
    "2. Los pasos indicados para añadir un nuevo datanode/nodemanager, con las salidas de los comandos `hdfs dfsadmin -report` y `yarn node -list`. Se debe visualizar que el nodo inicialmente se añade a YARN y no a HDFS (paso 4 del apartado 2) y luego que se añade a ambos servicios.\n",
    "3. Salida de la ejecución del balanceador de carga. Indica también cuántos datos se han movido y cuántos bloques tiene el datanode5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-confidentiality",
   "metadata": {},
   "source": [
    "### Tarea 3: Retirar un DataNode/NodeManager \n",
    "\n",
    "En principio, el apagado de un DataNode/NodeManager puede hacerse directamente y no afecta al cluster. Sin embargo, si queremos hacer un apagado programado de un DataNode/nodeManager es preferible advertir al NameNode previamente.\n",
    "\n",
    "Sigue los siguiente pasos para eliminar, por ejemplo, el datanode4.\n",
    "\n",
    "1. Pon el nombre del nodo o nodos que queremos retirar en los fichero `dfs.exclude` y `yarn.exclude` y ejecutar\n",
    "\n",
    "```bash\n",
    "hdfs dfsadmin -refreshNodes\n",
    "yarn rmadmin -refreshNodes\n",
    "```\n",
    "\n",
    "2. Comprueba que al cabo de un rato, usando el interfaz web y mediante los comandos los comandos `hdfs dfsadmin -report` y `yarn node -list`, que el/los nodo(s) excluido(s) aparece(n) que está(n) Decomissioned en HDFS y YARN\n",
    "\n",
    "Ya podríamos parar los demonios en el nodo decomisionado y parar el\n",
    "contenedor asociado. Si no queremos volver a incluirlo en el cluster:\n",
    "\n",
    "1.  Eliminar el/los nodo(s) de los ficheros include y exclude y ejecutar otra vez\n",
    "\n",
    "```bash\n",
    "hdfs dfsadmin -refreshNodes\n",
    "yarn rmadmin -refreshNodes\n",
    "```\n",
    "\n",
    "##### Información para la memoria\n",
    "\n",
    "Añade a la memoria una captura de pantalla en las que se vea el interfaz web del HDFS mostrando que el datanode4 está decomisionado, y del interfaz web del YARN mostrando que el datanode4 ya no está entre los nodos disponibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-mambo",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tarea 4: Rack awareness\n",
    "\n",
    "Para obtener el máximo rendimiento, es importante configurar Hadoop para para que conozca la topología de nuestra red. Por defecto, Hadoop considera que todos los DataNodes/NodeManagers son iguales y están situados en un único rack, que se identifica como `/default-rack`.\n",
    "\n",
    "Para clusters multirack, debemos indicar a Hadoop en que rack está cada\n",
    "nodo, para mejorar la eficiencia y la fiabilidad.\n",
    "\n",
    "![Arquitectura típica en 2 niveles de un cluster Hadoop. Fuente: T.\n",
    "White, &quot;Hadoop: The Definitive\n",
    "Guide&quot;\\\"](https://raw.githubusercontent.com/dsevilla/tcdm-public/21-22/misc/rack-awareness.png)\n",
    "\n",
    "En la imagen, se muestra una arquitectura típica en 2 niveles de un cluster Hadoop. Esta topología puede describirse en forma de árbol, como /switch1/rack1 y /switch1/rack2, o, simplificando /rack1 y /rack2. Para indicarle esta topología a Hadoop, es necesario utilizar un script que mapee los nombres de los nodos al rack en el que se encuentran.\n",
    "\n",
    "En nuestro caso, vamos a suponer que tenemos 2 racks (rack1 y rack2) y que tenemos dos nodos en cada rack. Haced lo siguiente en el NameNode (como usuario hdadmin):\n",
    "\n",
    "1. Ejecuta el comando `hdfs dfsadmin -printTopology` para ver como es la topología actual. Apunta las IPs (sin los puertos) de los datanodos.\n",
    "\n",
    "2. Apaga los demonios NameNode\n",
    "\n",
    "3. Crea un fichero `$HADOOP_HOME/etc/hadoop/topology.data` que tenga en cada linea la IP de uno de los DataNodes y el rack donde está, como en este ejemplo (cambiando las IPs por las tuyas):\n",
    "\n",
    "```\n",
    "IPdatanode1     /rack1\n",
    "IPdatanode2     /rack1\n",
    "IPdatanode3     /rack2\n",
    "IPdatanode5     /rack2\n",
    "```\n",
    "\n",
    "4. Crea un script de bash `$HADOOP_HOME/etc/hadoop/topology.script` como el siguiente\n",
    "    (fuente: http://wiki.apache.org/hadoop/topology_rack_awareness_scripts). Dale permisos de ejecución (`chmod +x topology.script`).\n",
    "\n",
    "```bash\n",
    "#! /bin/bash\n",
    "\n",
    "HADOOP_CONF=$HADOOP_HOME/etc/hadoop\n",
    "while [ $# -gt 0 ] ; do\n",
    "  nodeArg=$1\n",
    "  exec< ${HADOOP_CONF}/topology.data\n",
    "  result=\"\"\n",
    "  while read line ; do\n",
    "    ar=( $line )\n",
    "    if [ \"${ar[0]}\" = \"$nodeArg\" ] ; then\n",
    "      result=\"${ar[1]}\"\n",
    "    fi\n",
    "  done\n",
    "  shift\n",
    "  if [ -z \"$result\" ] ; then\n",
    "    echo -n \"/default-rack \"\n",
    "  else\n",
    "    echo -n \"$result \"\n",
    "  fi\n",
    "done\n",
    "```\n",
    "\n",
    "5. Define en el fichero `core-site.xml` la propiedad `net.topology.script.file.name` y darle como valor el path completo al script\n",
    "\n",
    "6. Inicia los demonios y comprueba que se han identificado los racks ejecutando `hdfs dfsadmin -printTopology`\n",
    "\n",
    "#### Información para la memoria\n",
    "\n",
    "Añade a la memoria una captura de pantalla en la que se vea ta salida del comando `hdfs dfsadmin -printTopology` mostrando la distribución por racks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
