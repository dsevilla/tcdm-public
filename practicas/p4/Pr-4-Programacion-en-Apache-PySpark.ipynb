{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 4: Programación en Apache Spark\n",
    "\n",
    "Se propone la realización de 4 scripts. Los scripts deben realizarse usando Notebooks o bien como scripts que se puedan enviar con `spark-submit`.\n",
    "\n",
    "### Normas:\n",
    "- Los scripts deben incluir comentarios que expliquen los pasos realizados.\n",
    "- La salida de los scripts debe seguir el formato indicado en cada uno de los ejercicios (incluyendo el nombre y orden de las columnas).\n",
    "- Se debe entregar un fichero comprimido con los scripts de la práctica debidamente comentados.\n",
    "- **TAMBIÉN** un pequeño documento que muestre capturas de la ejecución de los scripts **dentro del clúster**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "Extraer información de los ficheros de StackOverflow (`Posts.parquet`, `Users.parquet`, `Comments.parquet`). Crear un script que haga lo siguiente:\n",
    "\n",
    " a. A partir del fichero `Posts.parquet` obtener el número de respuestas (y la suma de sus `Score`) que ha recibido cada pregunta (posts con `PostTypeId` = 1). Para ello, cuenta cuántos posts con `PostTypeId` = 2 tienen como `ParentId` el `Id` de cada pregunta. Debes obtener un DataFrame de la siguiente forma en el fichero `dfRespuestas.parquet`:\n",
    "\n",
    "|QuestionId|NRespuestas|ScoreTotal|\n",
    "|---------:|----------:|---------:|\n",
    "|4        |13         |120      |\n",
    "|6        |26         |300      |\n",
    "|9        |33         |400      |\n",
    "|11       |15         |150      |\n",
    "|13       |8          |80       |\n",
    "|14       |3          |30       |\n",
    "|17       |6          |60       |\n",
    "\n",
    "\n",
    "b. A partir del fichero `Posts.parquet`, crear un DataFrame que contenga el Id de la pregunta, el `OwnerUserId` y el año de creación (extraído de `CreationDate`), descartando el resto de campos. Ese DataFrame debe tener la siguiente forma, y estar en el fichero `dfPreguntas.parquet`:\n",
    "\n",
    "|QuestionId|UserId|Año |\n",
    "|---------:|-----:|---:|\n",
    "|4         |8     |2008|\n",
    "|6         |9     |2008|\n",
    "|9         |1     |2008|\n",
    "|11        |1     |2008|\n",
    "|13        |9     |2008|\n",
    "|14        |11    |2008|\n",
    "|17        |2     |2008|\n",
    "|22        |9     |2008|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requisitos\n",
    "\n",
    "- Ambos DataFrames se deben salvar en formato Parquet con compresión gzip. Comprueba el número de particiones de cada DataFrame y el número de ficheros generados.\n",
    "- El script debe aceptar argumentos en línea de comandos, es decir, para su ejecución se debe poder indicar la ruta al fichero de entrada y el nombre de los directorios de salida. Por ejemplo, para la ejecución en local:\n",
    "\n",
    "```bash\n",
    "spark-submit --master 'local[*]' --num-executors 4 --driver-memory 4g e1.py Posts.parquet dfRespuestas.parquet dfPreguntas.parquet\n",
    "```\n",
    "\n",
    "### Ejemplo:\n",
    "\n",
    "```python\n",
    "#! /usr/bin/env python3\n",
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "#\n",
    "# Script para extraer información del fichero Posts.parquet de StackOverflow. \n",
    "# a) Obtener el número de respuestas que ha recibido cada pregunta.\n",
    "#    Debes obtener un DataFrame de la siguiente forma:\n",
    "#   +----------+-----------+----------+\n",
    "#   |QuestionId|NRespuestas|ScoreTotal|\n",
    "#   +----------+-----------+----------+\n",
    "#   |    4     |     13    |    120   |\n",
    "#   |    6     |     26    |    300   |\n",
    "#   |    9     |     33    |    400   |\n",
    "#   |   11     |     15    |    150   |\n",
    "#   |   13     |      8    |     80   |\n",
    "#\n",
    "# b) A partir del fichero Posts.parquet, crear un DataFrame que contenga el Id de la pregunta, \n",
    "# el OwnerUserId y el año de creación, descartando el resto de campos del fichero.\n",
    "# Ese DataFrame debe tener la siguiente forma:\n",
    "#\n",
    "#   +----------+------+----+ \n",
    "#   |QuestionId|UserId|Año |\n",
    "#   +----------+------+----+\n",
    "#   |    4     |   8  |2008|\n",
    "#   |    6     |   9  |2008|\n",
    "#   |    9     |   1  |2008|\n",
    "#   |   11     |   1  |2008|\n",
    "#   |   13     |   9  |2008|\n",
    "#\n",
    "# Ejecutar en local con:\n",
    "# spark-submit --master 'local[*]' --driver-memory 4g e1.py Posts.parquet dfRespuestas.parquet dfPreguntas.parquet\n",
    "# Ejecución en un cluster YARN:\n",
    "# spark-submit --master yarn --num-executors 8 --driver-memory 4g e1.py Posts.parquet dfRespuestas.parquet dfPreguntas.parquet\n",
    "\n",
    "def main():\n",
    "    # Comprueba el número de argumentos\n",
    "    # sys.argv[1] es el primer argumento, sys.argv[2] el segundo, etc.\n",
    "    if len(sys.argv) != 4:\n",
    "        print(f\"Uso: {sys.argv[0]} Posts.parquet dfRespuestas.parquet dfPreguntas.parquet\")\n",
    "        exit(-1)\n",
    "\n",
    "    spark: SparkSession = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Ejercicio 1 de Diego\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Cambio la verbosidad para reducir el número de\n",
    "    # mensajes por pantalla\n",
    "    spark.sparkContext.setLogLevel(\"FATAL\")\n",
    "    # Código del programa\n",
    "    ...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "**Nota:** Para hacer pruebas más rápidamente podéis hacer un sampleo del fichero `Posts.parquet` y trabajar con una versión más reducida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ayuda en la realización del ejercicio:\n",
    "\n",
    "- Comprueba que tienes acceso a los ficheros\n",
    "- Cargamos datos en dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "text/plain",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "En el shell de Unix:\n",
    "\n",
    "```bash\n",
    "# Descarga del fichero Posts.parquet\n",
    "wget -qq https://github.com/dsevilla/bd2-data/releases/download/parquet-files-25-26/Posts.parquet\n",
    "\n",
    "# También puedes descargar Users, Comments y Tags si los necesitas\n",
    "wget -qq https://github.com/dsevilla/bd2-data/releases/download/parquet-files-25-26/Users.parquet\n",
    "wget -qq https://github.com/dsevilla/bd2-data/releases/download/parquet-files-25-26/Comments.parquet\n",
    "wget -qq https://github.com/dsevilla/bd2-data/releases/download/parquet-files-25-26/Tags.parquet\n",
    "\n",
    "# Listamos ficheros\n",
    "ls -lh *.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cargar los datos del fichero `Posts.parquet` podéis usar el siguiente código:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def load_posts_data(spark: SparkSession, path_posts: str) -> DataFrame:\n",
    "    posts: DataFrame = (spark\n",
    "        .read\n",
    "        .format(\"parquet\")\n",
    "        .load(path_posts))\n",
    "    \n",
    "    posts.printSchema()\n",
    "    posts.show(10)\n",
    "    print(f\"Total de posts: {posts.count()}\")\n",
    "    \n",
    "    return posts\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "Script que, a partir de los datos en Parquet de la práctica anterior, obtenga para cada usuario y para cada año el total de preguntas realizadas, el total de respuestas recibidas por todas sus preguntas, el total de comentarios a sus preguntas, la media de respuestas por pregunta y el máximo número de respuestas recibidas en una pregunta.\n",
    "- Obtener solo aquellos casos en los que existan valores en ambos DataFrames (inner join).\n",
    "- Cada usuario debe aparecer con su `DisplayName`, obtenido del fichero `Users.parquet`.\n",
    "- Los comentarios se obtienen del fichero `Comments.parquet`, que tiene una columna `PostId` con el Id del post (pregunta o respuesta) que se comenta.\n",
    "- El DataFrame generado debe estar ordenado por el máximo número de respuestas, usuario y año.\n",
    "\n",
    "Ejemplo de salida:\n",
    "\n",
    "\n",
    "|Usuario      |Año |NumPreguntas|TotalRespuestas|TotalComentarios|MediaRespuestas   |MaxRespuestas|\n",
    "|:------------|---:|-----------:|--------------:|---------------:|-----------------:|------------:|\n",
    "|Jon Skeet    |2008|156         |1247           |234             |7.99              |45           |\n",
    "|Marc Gravell |2008|89          |456            |123             |5.12              |32           |\n",
    "|Mehrdad      |2009|34          |234            |45              |6.88              |28           |\n",
    "|Joel         |2008|45          |189            |67              |4.20              |23           |\n",
    "|Greg Hewgill |2008|67          |345            |89              |5.15              |19           |\n",
    "|... |... |...  |...    |...   |...   |...|\n",
    "\n",
    "\n",
    "### Requisitos\n",
    "- El DataFrame obtenido se debe guardar en un único fichero CSV sin comprimir y con cabecera.\n",
    "- Como en el caso anterior, el script debe aceptar argumentos en línea de comandos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ayuda en la realización del ejercicio:**\n",
    "\n",
    "- Lectura de fichero parquet.\n",
    "- Cargamos el fichero con los códigos del país en un diccionario.\n",
    "- Guardar un DataFrame en un único fichero CSV sin comprimir y con cabecera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": "auto"
   },
   "source": [
    "```python\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Leo el fichero de respuestas\n",
    "dfRespuestas: DataFrame = (spark.read.format(\"parquet\")\n",
    "               .option(\"mode\", \"FAILFAST\")\n",
    "               .load(\"dfRespuestas.parquet\"))\n",
    "\n",
    "# Leo el fichero de preguntas\n",
    "dfPreguntas: DataFrame = (spark.read.format(\"parquet\")\n",
    "               .option(\"mode\", \"FAILFAST\")\n",
    "               .load(\"dfPreguntas.parquet\"))\n",
    "\n",
    "# Leo el fichero de usuarios\n",
    "dfUsers: DataFrame = (spark.read.format(\"parquet\")\n",
    "               .option(\"mode\", \"FAILFAST\")\n",
    "               .load(\"Users.parquet\"))\n",
    "\n",
    "# Leo el fichero de comentarios\n",
    "dfComments: DataFrame = (spark.read.format(\"parquet\")\n",
    "               .option(\"mode\", \"FAILFAST\")\n",
    "               .load(\"Comments.parquet\"))\n",
    "\n",
    "dfUsers.printSchema()\n",
    "dfUsers.show(10)\n",
    "\n",
    "# Lo guardamos como un único fichero CSV\n",
    "(dfRespuestas.coalesce(1)\n",
    "       .write.format(\"csv\")\n",
    "       .mode(\"overwrite\")\n",
    "       .option(\"header\", True)\n",
    "       .save(\"resultado_e2\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3\n",
    "\n",
    "Obtener a partir de los ficheros Parquet creados en el ejercicio 1 y del fichero `Tags.parquet` un DataFrame que proporcione, para un grupo de tags especificados, las preguntas ordenadas por número de respuestas, de mayor a menor, junto con una columna que indique el rango (posición de la pregunta en ese tag/año según las respuestas obtenidas):\n",
    "\n",
    "La salida del script debe ser como sigue:\n",
    "\n",
    "|Tag    |Año |QuestionId|NRespuestas|Rango|\n",
    "|:------|---:|---------:|----------:|----:|\n",
    "|python |2008|155       |89         |1    |\n",
    "|python |2008|231       |67         |2    |\n",
    "|python |2008|456       |45         |3    |\n",
    "|python |2008|678       |43         |4    |\n",
    "|python |2008|890       |38         |5    |\n",
    "|python |2009|1234      |92         |1    |\n",
    "|python |2009|2345      |78         |2    |\n",
    "|python |2009|3456      |65         |3    |\n",
    "|... |... |...     |...   |...|\n",
    "|java   |2008|234       |76         |1    |\n",
    "|java   |2008|345       |68         |2    |\n",
    "|java   |2008|567       |54         |3    |\n",
    "|... |... |...     |...   |...|\n",
    "\n",
    "\n",
    "### Requisitos\n",
    "* El DataFrame debe de estar ordenado por tag y año (ascendente) y número de respuestas (descendente).\n",
    "* Utilizad funciones de ventana para obtener el rango.\n",
    "* La salida debe guardarse en un único fichero CSV sin comprimir y con cabecera.\n",
    "* Como en los casos anteriores, el script debe aceptar argumentos en línea de comandos, es decir, para su ejecución deberíamos poder indicar la ruta a los directorios de entrada creados en la práctica 1, la lista de tags a analizar (separados por coma) y el nombre del directorio de salida.\n",
    "\n",
    "**Nota:** El fichero `Tags.parquet` tiene las columnas `Id` (del post) y `TagName`. Una pregunta puede tener múltiples tags asociados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 4\n",
    "\n",
    "Obtener a partir del fichero Parquet con la información de preguntas (QuestionId, UserId y Año) creado en el ejercicio 1, un DataFrame que nos muestre el número de preguntas realizadas por cada usuario (dado por su nombre, `DisplayName` de `Users.parquet`) por cada tag y por cada año. Adicionalmente, debe mostrar el aumento o disminución del número de preguntas para cada usuario con respecto al año anterior.\n",
    "\n",
    "El DataFrame generado tiene que ser como este:\n",
    "\n",
    "|Usuario      |Tag    |Año |NPreguntas|Dif |\n",
    "|:------------|:------|---:|---------:|---:|\n",
    "|Jon Skeet    |python |2008|12        |0   |\n",
    "|Jon Skeet    |python |2009|18        |6   |\n",
    "|Jon Skeet    |python |2010|15        |-3  |\n",
    "|Jon Skeet    |java   |2008|33        |0   |\n",
    "|Jon Skeet    |java   |2009|49        |16  |\n",
    "|Jon Skeet    |java   |2010|19        |-30 |\n",
    "|Marc Gravell |python |2008|23        |0   |\n",
    "|Marc Gravell |python |2009|28        |5   |\n",
    "|Marc Gravell |python |2010|31        |3   |\n",
    "|Joel Coehoorn|c#     |2008|12        |0   |\n",
    "|Joel Coehoorn|c#     |2009|45        |33  |\n",
    "|Greg Hewgill |java   |2009|8         |0   |\n",
    "|Greg Hewgill |java   |2010|15        |7   |\n",
    "|... |...   |...   |...     |...|\n",
    "\n",
    "### Requisitos\n",
    "* El DataFrame debe de estar ordenado por Usuario, Tag y año.\n",
    "* Para obtener la diferencia con el año anterior, utilizad funciones de ventana (window functions).\n",
    "* La salida debe guardarse en un único fichero CSV sin comprimir y con cabecera.\n",
    "* Como en los casos anteriores, el script debe aceptar argumentos en línea de comandos, es decir, para su ejecución deberíamos poder indicar la ruta al directorio de entrada creado en la práctica 1 y el nombre del directorio de salida.\n",
    "* Opcionalmente, se puede añadir un filtro para mostrar solo los usuarios que tengan al menos un número mínimo de preguntas totales (por ejemplo, 10 o más preguntas en total)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": "text/python",
   "file_extension": ".py",
   "mimetype": "text/python",
   "name": "python",
   "pygments_lexer": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
