{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a los DataFrames\n",
    "\n",
    "En este tema veremos:\n",
    "  - Cómo crear un DataFrame\n",
    "  - Algunas operaciones básicas sobre DataFrames\n",
    "      - Mostrar filas\n",
    "      - Seleccionar columnas\n",
    "      - Renombrar, añadir y eliminar columnas\n",
    "      - Eliminar valores nulos y filas duplicadas\n",
    "      - Reemplazar valores\n",
    "  - Guardar los DataFrames en diferentes formatos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de DataFrames\n",
    "Un DataFrame puede crearse de distintas formas:\n",
    "\n",
    "  - A partir de una secuencia de datos\n",
    "  - A partir de objetos de tipo Row\n",
    "  - A partir de un RDD o DataSet\n",
    "  - Leyendo los datos de un fichero\n",
    "      - Igual que Hadoop, Spark soporta diferentes filesystems: local, HDFS, Amazon S3\n",
    "          - En general, soporta cualquier fuente de datos que se pueda leer con Hadoop\n",
    "      - Spark puede acceder a diferentes tipos de ficheros: texto plano, CSV, JSON, [Parquet](https://parquet.apache.org/), [ORC](https://orc.apache.org/), Sequence, etc\n",
    "        -   Soporta ficheros comprimidos\n",
    "  - Accediendo a bases de datos relacionales o NoSQL\n",
    "    -   MySQL, Postgres, etc. mediante JDBC/ODBC\n",
    "    -   Hive, HBase, Cassandra, MongoDB, AWS Redshift, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "RunningInCOLAB: bool = 'google.colab' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RunningInCOLAB:\n",
    "    !sudo apt-get update ; sudo apt-get install -y default-jre-headless\n",
    "    %env JAVA_HOME=/usr/lib/jvm/default-java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando DataFrames a partir de una secuencia o lista de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Elegir el máster de Spark dependiendo de si se ha definido la variable de entorno HADOOP_CONF_DIR o YARN_CONF_DIR\n",
    "SPARK_MASTER: str = (\n",
    "    \"yarn\" if \"HADOOP_CONF_DIR\" in os.environ or \"YARN_CONF_DIR\" in os.environ else \"local[*]\"\n",
    ")\n",
    "\n",
    "# Creamos un objeto SparkSession (o lo obtenemos si ya está creado)\n",
    "spark: SparkSession = (\n",
    "    SparkSession.builder.appName(\"Mi aplicacion TCDM\")\n",
    "    .config(\"spark.rdd.compress\", \"true\")\n",
    "    .config(\"spark.executor.memory\", \"3g\")\n",
    "    .config(\"spark.driver.memory\", \"3g\")\n",
    "    .master(SPARK_MASTER)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc: SparkContext = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pp\n",
    "\n",
    "pp(sc._conf.getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Creando un DataFrame desde un rango y añadiéndole dos columnas\n",
    "df: DataFrame = spark.range(1, 7, 2).toDF(\"n\")\n",
    "df.show()\n",
    "\n",
    "# Añadiendo dos columnas al DataFrame\n",
    "# La expresión para la columna puede incluir operadores.\n",
    "df.withColumn(\"n1\", col(\"n\") + 1).withColumn(\"n2\", 2 * col(\"n1\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando DataFrames con esquema\n",
    "A la hora de crear un DataFrame, es conveniente especificar el esquema del mismo:\n",
    "\n",
    "  - El esquema define los nombres y tipos de datos de las columnas.\n",
    "  - Se usa un objeto de tipo `StructType` para definir el nombre y tipo de las columnas, y un objeto de tipo `StructField` para definir el nombre y tipo de una columna.\n",
    "  - Los tipos de datos que utiliza Spark están definidos en:\n",
    "      - Para PySpark: https://spark.apache.org/docs/latest/sql-ref-datatypes.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import DataType, FloatType, StringType, StructField, StructType\n",
    "\n",
    "# Definimos el esquema del DataFrame\n",
    "esquemaNotas: DataType = StructType(\n",
    "    fields=[\n",
    "        StructField(name=\"nombre\", dataType=StringType(), nullable=False),\n",
    "        StructField(name=\"nota\", dataType=FloatType(), nullable=True),\n",
    "        StructField(name=\"calificación\", dataType=StringType(), nullable=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Crear Row con campos nombrados explícitamente\n",
    "filas: list[Row] = [\n",
    "    Row(nombre=\"Pepe\", nota=5.1, calificación=\"Aprobado\"),\n",
    "    Row(nombre=\"Juan\", nota=4.0, calificación=\"Suspenso\"),\n",
    "    Row(nombre=\"Manuel\", nota=None, calificación=None),\n",
    "]\n",
    "\n",
    "dfNotas: DataFrame = spark.createDataFrame(filas, schema=esquemaNotas)\n",
    "dfNotas.show()\n",
    "dfNotas.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando DataFrames a partir de un fichero de texto\n",
    "\n",
    "Cada línea del fichero se guarda como una fila (e incluso detecta automáticamente si el fichero está comprimido):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/dsevilla/tcdm-public/refs/heads/25-26/datos/quijote.txt.gz\",\n",
    "    \"quijote.txt.gz\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OJO: Se supone que el usuario que se usa es \"luser\" y que tiene permisos para escribir en el directorio /user/luser en HDFS.\n",
    "if SPARK_MASTER == \"yarn\":\n",
    "    !hdfs dfs -put quijote.txt.gz /user/luser/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota sobre YARN y HDFS**:\n",
    "\n",
    "En modo cluster (YARN), los archivos deben estar en **HDFS** (Hadoop Distributed File System) para que todos los workers del cluster puedan accederlos.\n",
    "\n",
    "- `/user/luser/` es el directorio home del usuario `luser` en HDFS\n",
    "- Debe existir y tener permisos de escritura: `hdfs dfs -mkdir -p /user/luser`\n",
    "- En modo `local[*]` no es necesario, Spark lee del filesystem local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfQuijote: DataFrame = spark.read.text(\"quijote.txt.gz\")\n",
    "dfQuijote.show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando DataFrames a partir de un fichero CSV\n",
    "\n",
    "Como ejemplo vamos a utilizar el fichero de preguntas y respuestas de Stack Overflow en Español, que hemos utilizado en otras ocasiones. Es un fichero CSV, con unos campos que son:\n",
    "\n",
    "- `Id`: integer: La identificación de la pregunta o respuesta\n",
    "- `AcceptedAnswerId`: integer: La identificación de la respuesta aceptada (si existe)\n",
    "- `AnswerCount`: integer: El número de respuestas\n",
    "- `Body`: string: El cuerpo de la pregunta o respuesta\n",
    "- `ClosedDate`: timestamp: Fecha de cierre de la pregunta (si está cerrada)\n",
    "- `CommentCount`: integer: Número de comentarios\n",
    "- `CommunityOwnedDate`: timestamp: (no se usará)  \n",
    "- `ContentLicense`: string: Licencia de contenido\n",
    "- `CreationDate`: timestamp: La fecha de creación\n",
    "- `FavoriteCount`: integer: Número de favoritos\n",
    "- `LastActivityDate`: timestamp: (no se usará)\n",
    "- `LastEditDate`: timestamp: (no se usará)\n",
    "- `LastEditorDisplayName`: string: (no se usará)\n",
    "- `LastEditorUserId`: integer: (no se usará)\n",
    "- `OwnerDisplayName`: string: El nombre del propietario (si se borró el usuario) \n",
    "- `OwnerUserId`: integer: El identificador del propietario\n",
    "- `ParentId`: integer: El identificador de la pregunta padre (si es una respuesta)\n",
    "- `PostTypeId`: integer: El tipo de post (1 = pregunta, 2 = respuesta, etc.)\n",
    "- `Score`: integer: La puntuación de la pregunta o respuesta\n",
    "- `Tags`: string: El conjunto de etiquetas\n",
    "- `Title`: string: El título de la pregunta\n",
    "- `ViewCount`: integer: El número de visitas\n",
    "\n",
    "Los campos se encuentran separados por el símbolo `\",\"`, y el carácter de escape de comillas es el propio carácter de comillas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leemos el fichero infiriendo el esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from tarfile import TarInfo\n",
    "\n",
    "# URLs de descarga\n",
    "file: str = \"es.stackoverflow.csv.tar.gz\"\n",
    "URL: str = f\"https://github.com/dsevilla/bd2-data/releases/download/parquet-files-25-26/{file}\"\n",
    "\n",
    "# Descargar el fichero tar.gz\n",
    "urllib.request.urlretrieve(URL, file)\n",
    "if not Path(file).exists():\n",
    "    raise FileNotFoundError(\"No se pudieron descargar los datos de StackOverflow\")\n",
    "\n",
    "# Extraer el fichero Posts.csv del tar.gz\n",
    "with tarfile.open(file, \"r:gz\") as tar:\n",
    "    member: TarInfo | None = next(filter(lambda m: m.name == \"Posts.csv\", tar.getmembers()), None)\n",
    "    if member is not None:\n",
    "        try:\n",
    "            tar.extract(member=member, filter='data')\n",
    "        except TypeError:\n",
    "            # Compatibilidad con versiones anteriores sin parámetro filter\n",
    "            tar.extractall(members=[member])\n",
    "\n",
    "assert Path(\"Posts.csv\").exists(), \"No se pudo extraer Posts.csv del fichero tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OJO: Se supone que el usuario que se usa es \"luser\" y que tiene permisos para escribir en el directorio /user/luser en HDFS.\n",
    "if SPARK_MASTER == \"yarn\":\n",
    "    !hdfs dfs -put Posts.csv /user/luser/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfSEInfered: DataFrame = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"mode\", \"FAILFAST\")\n",
    "    .option(\"sep\", \",\")\n",
    "    .option(\"escape\", '\"')\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"lineSep\", \"\\r\\n\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"nullValue\", \"\")\n",
    "    .load(\"Posts.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas opciones:\n",
    "\n",
    "1. ``mode``: especifica qué hacer cuando se encuentra registros corruptos\n",
    "    - ``PERMISSIVE``: pone todos los campos a null cuando se encuentra un registro corrupto (valor por defecto)\n",
    "    - ``DROPMALFORMED``: elimina las filas con registros corruptos\n",
    "    - ``FAILFAST``: da un error cuando se encuentra un registro corrupto\n",
    "2. ``sep``: separador entre campos (por defecto \",\")\n",
    "3. ``inferSchema``: especifica si se deben inferir el tipo de las columnas (por defecto \"false\")\n",
    "3. ``lineSep``: separador de líneas (por defecto \"\\n\"). Lo hemos cambiado a \"\\r\\n\" porque el fichero se ha creado en Windows, aunque da un warning, funciona correctamente\n",
    "4. ``header``: si \"true\" se toma la primera fila como cabecera (por defecto \"false\")\n",
    "5. ``nullValue``: carácter o cadena que representa un NULL en el fichero (por defecto \"\")\n",
    "6. ``compression``: tipo de compresión utilizada (por defecto \"none\")\n",
    "  \n",
    "Las opciones son similares para otros tipos de ficheros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Vemos 5 filas\n",
    "dfSEInfered.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Vemos como se ha inferido el esquema\n",
    "dfSEInfered.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Otra forma de verlo\n",
    "dfSEInfered.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leemos especificando el esquema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El esquema inferido tiene ciertos fallos, como considerar algunos campos como strings cuando deberían ser enteros, o tipos Timestamp en vez de Date. Por ello, vamos a especificar el esquema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    DataType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "\n",
    "# Defino el esquema para los elementos de la tabla\n",
    "# StructType -> Permite definir un esquema para el DF a partir de una lista de StructFields\n",
    "# StructField -> Definen el nombre y tipo de cada columna, así como si es nullable o no (campo True)\n",
    "dfSE_Schema: DataType = StructType(\n",
    "    [\n",
    "        StructField(\"Id\", IntegerType(), False),\n",
    "        StructField(\"AcceptedAnswerId\", IntegerType(), True),\n",
    "        StructField(\"AnswerCount\", IntegerType(), True),\n",
    "        StructField(\"Body\", StringType(), True),\n",
    "        StructField(\"ClosedDate\", TimestampType(), True),\n",
    "        StructField(\"CommentCount\", IntegerType(), True),\n",
    "        StructField(\"CommunityOwnedDate\", TimestampType(), True),\n",
    "        StructField(\"ContentLicense\", StringType(), True),\n",
    "        StructField(\"CreationDate\", TimestampType(), True),\n",
    "        StructField(\"FavoriteCount\", IntegerType(), True),\n",
    "        StructField(\"LastActivityDate\", TimestampType(), True),\n",
    "        StructField(\"LastEditDate\", TimestampType(), True),\n",
    "        StructField(\"LastEditorDisplayName\", StringType(), True),\n",
    "        StructField(\"LastEditorUserId\", IntegerType(), True),\n",
    "        StructField(\"OwnerDisplayName\", StringType(), True),\n",
    "        StructField(\"OwnerUserId\", IntegerType(), True),\n",
    "        StructField(\"ParentId\", IntegerType(), True),\n",
    "        StructField(\"PostTypeId\", IntegerType(), True),\n",
    "        StructField(\"Score\", IntegerType(), True),\n",
    "        StructField(\"Tags\", StringType(), True),\n",
    "        StructField(\"Title\", StringType(), True),\n",
    "        StructField(\"ViewCount\", IntegerType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Creo el DataFrame con el esquema definido\n",
    "dfSE: DataFrame = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"mode\", \"FAILFAST\")\n",
    "    .option(\"inferSchema\", \"false\")\n",
    "    .option(\"sep\", \",\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"nullValue\", \"\")\n",
    "    .option(\"lineSep\", \"\\r\\n\")\n",
    "    .option(\"escape\", '\"')\n",
    "    .schema(dfSE_Schema)\n",
    "    .load(\"Posts.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajo con ficheros Parquet\n",
    "\n",
    "Al igual que hemos hecho en esta asignatura y en otras, trabajaremos con el fichero `.parquet`. Hemos visto que se puede especificar el esquema de un fichero CSV, pero como vimos el fichero Parquet ya lo lleva implícito. Probémoslo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "# Descargar el fichero Parquet si no existe\n",
    "# (esto se debe hacer porque Spark no puede leer directamente desde URLs HTTPS)\n",
    "parquet_file = \"Posts.parquet\"\n",
    "if not Path(parquet_file).exists():\n",
    "    URL_PARQUET = (\n",
    "        \"https://github.com/dsevilla/bd2-data/releases/download/parquet-files-25-26/Posts.parquet\"\n",
    "    )\n",
    "    print(f\"Descargando {parquet_file}...\")\n",
    "    urllib.request.urlretrieve(URL_PARQUET, parquet_file)\n",
    "    print(\"Descarga completada.\")\n",
    "\n",
    "# Leer el fichero Parquet desde el filesystem local\n",
    "dfSE: DataFrame = spark.read.format('parquet').option('mergeSchema', 'true').load(parquet_file)\n",
    "\n",
    "dfSE.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSE.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Por qué cachear el DataFrame?**\n",
    "\n",
    "Vamos a realizar múltiples operaciones sobre `dfSE` (mostrar, seleccionar, filtrar, etc.), por lo que lo mantenemos en memoria con `.cache()` para evitar recalcularlo desde el archivo CSV cada vez que lo usemos.\n",
    "\n",
    "**Cuándo usar cache()**:\n",
    "- Cuando vas a reutilizar el mismo DataFrame en múltiples acciones\n",
    "- Después de transformaciones costosas (joins, agregaciones)\n",
    "- **No cachear** DataFrames muy grandes que no caben en memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfSE.sort(\"Id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfSE.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones básicas con DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostrar filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# show(n) permite mostrar las primeras n filas (por defecto, n=20)\n",
    "dfSE.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Podemos indicar que no trunque los campos largos\n",
    "dfSE.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# take(n) devuelve las n primeras filas como una lista Python de objetos Row\n",
    "lista: list[Row] = dfSE.take(5)\n",
    "pp(lista[1])\n",
    "# collect() devuelve todo el DataFrame como una lista Python de objetos Row\n",
    "# Si el DataFrame es muy grande podría colapsar al Driver\n",
    "# lista2 = dfSE.collect()\n",
    "# print(lista2[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# sample(withReplacement, fraction, seed=None) devuelve un nuevo Dataframe con una fracción de las filas\n",
    "dfSESampled: DataFrame = dfSE.sample(False, 0.1, seed=None)\n",
    "print(f\"N de filas original = {dfSE.count()}; n de filas muestreadas = {dfSESampled.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# limit(n) limita a n el número de filas obtenidas\n",
    "dfSE_10filas: DataFrame = dfSE.sample(False, 0.1, seed=None).limit(10)\n",
    "print(f\"N de filas muestreadas = {dfSE_10filas.count()}\")\n",
    "dfSE_10filas.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar una operación sobre cada una de las filas\n",
    "El método `foreach` aplica una función a cada una de las filas\n",
    "\n",
    "- El DataFrame no se modifica y no se crea ningún otro DataFrame\n",
    "- El `foreach`se ejecuta en los workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "def printid(f: Row) -> None:\n",
    "    print(f[\"Id\"])\n",
    "\n",
    "\n",
    "dfSE_10filas.foreach(printid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importante sobre `foreach()`**:\n",
    "\n",
    "La función que pasas a `foreach()` se ejecuta en los **workers remotos**, no en el driver donde corre el notebook. Por eso **NO verás el output** de los `print()` en la consola (en el caso de ejecutar en *clúster*).\n",
    "\n",
    "Para ver resultados:\n",
    "1. Usa `.take()` o `.collect()` primero para traer datos al driver\n",
    "2. O usa `foreach()` para efectos secundarios (escribir a BD, enviar a API, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccionar columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Crea un nuevo DataFrame seleccionando columnas por nombre\n",
    "dfIdBody: DataFrame = dfSE.select(\"Id\", \"Body\")\n",
    "dfIdBody.show(5)\n",
    "\n",
    "print(f\"El objeto dfIdCuerpo es de tipo {type(dfIdBody)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Otra forma de indicar a las columnas\n",
    "dfIdBody2: DataFrame = dfSE.select(dfSE.Id, dfSE.Body)\n",
    "dfIdBody2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# También es posible indicar objetos de tipo Column\n",
    "from pyspark.sql.column import Column\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "colId: Column = col(\"Id\")\n",
    "colCreaDate: Column = col(\"CreationDate\")\n",
    "print(f\"El objeto colId es de tipo {type(colId)}.\")\n",
    "print(f\"El objeto colCreaDate es de tipo {type(colCreaDate)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Y crear un DataFrame a partir de objetos Column, renombrando columnas\n",
    "dfIdFechaCuerpo: DataFrame = dfSE.select(\n",
    "    colId, colCreaDate.alias(\"Fecha_Creación\"), dfSE.Body.alias(\"Cuerpo\")\n",
    ")\n",
    "dfIdFechaCuerpo.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# El DataFrame anterior usando expresiones\n",
    "dfIdFechaCuerpoExpr: DataFrame = dfSE.select(\n",
    "    expr(\"Id AS ID\"), expr(\"CreationDate AS `Fecha_Creación`\"), expr(\"Body AS Cuerpo\")\n",
    ")\n",
    "dfIdFechaCuerpoExpr.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Se pueden usar expresiones más complejas\n",
    "dfSE.selectExpr(\n",
    "    \"*\", \"(AnswerCount IS NOT NULL) as respuestaValida\"  # Selecciona todas las columnas\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renombrar, añadir y eliminar columnas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Renombramos la columna CreationDate\n",
    "# NOTA: withColumnRenamed() NO modifica dfSE, crea un NUEVO DataFrame\n",
    "# Los DataFrames son inmutables, pero aquí reasignamos la variable dfSE\n",
    "dfSE_renamed: DataFrame = dfSE.withColumnRenamed(\"CreationDate\", \"Fecha_de_creación\")\n",
    "dfSE_renamed.select(\n",
    "    \"Fecha_de_creación\", dfSE_renamed.ViewCount.alias(\"Número_de_vistas\"), \"Score\", \"PostTypeId\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Añadimos una nueva columna con todos sus valores iguales a 1\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# lit() convierte un literal de Python al formato interno de Spark (IntegerType en este caso)\n",
    "# withColumn() crea un NUEVO DataFrame, no modifica el original (inmutabilidad)\n",
    "dfSE_with_ones: DataFrame = dfSE_renamed.withColumn(\"unos\", lit(1))\n",
    "dfSE_with_ones.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Elimina una columna con drop\n",
    "# drop() también crea un NUEVO DataFrame (inmutabilidad)\n",
    "dfSE_clean: DataFrame = dfSE_with_ones.drop(col(\"unos\"))\n",
    "dfSE_clean.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concepto clave: Inmutabilidad de DataFrames**\n",
    "\n",
    "Observa que cada operación (`withColumnRenamed`, `withColumn` y `drop`) **crea un nuevo DataFrame**. \n",
    "\n",
    "Los DataFrames son **inmutables** - nunca se modifican en su lugar. Esto permite:\n",
    "- Optimizaciones del motor Spark\n",
    "- Seguridad en entornos distribuidos\n",
    "- Posibilidad de cachear y reutilizar DataFrames sin efectos secundarios\n",
    "\n",
    "En el resto del notebook, para simplificar, reasignaremos la variable `dfSE`, pero recuerda que técnicamente son DataFrames distintos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminar valores nulos y duplicados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diferencia entre `\"any\"` y `\"all\"`**:\n",
    "\n",
    "- `dropna(\"any\")`: Elimina la fila si **al menos una columna** es NULL (más restrictivo)\n",
    "- `dropna(\"all\")`: Elimina la fila solo si **todas las columnas** son NULL (menos restrictivo)\n",
    "\n",
    "Con `subset`:\n",
    "- `dropna(\"any\", subset=[...])`: Elimina si **alguna de las columnas especificadas** es NULL\n",
    "- `dropna(\"all\", subset=[...])`: Elimina solo si **todas las columnas especificadas** son NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Eliminamos todas las filas que tengan null en alguna de sus columnas\n",
    "dfNoNulls: DataFrame = dfSE.dropna(\"any\")\n",
    "print(f\"Numero de filas inicial: {dfSE.count()}; número de filas sin null: {dfNoNulls.count()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Elimina las filas que tengan null en todas sus columnas\n",
    "dfNingunNull: DataFrame = dfSE.dropna(\"all\")\n",
    "print(f\"Número de filas con todo a null: {dfSE.count() - dfNingunNull.count()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Elimina las filas duplicadas\n",
    "dfSinDuplicadas: DataFrame = dfSE.dropDuplicates()\n",
    "print(f\"Número de filas duplicadas: {dfSE.count() - dfSinDuplicadas.count()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Elimina las filas duplicadas en alguna columna\n",
    "dfSinUserDuplicado: DataFrame = dfSE.dropDuplicates([\"OwnerUserId\"])\n",
    "print(f\"Número de usuarios únicos: {dfSinUserDuplicado.count()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Ejemplos con subset de columnas\n",
    "\n",
    "# Elimina filas donde ViewCount O AcceptedAnswerId son null (al menos una)\n",
    "dfNoNullViewCountAcceptedAnswerId: DataFrame = dfSE.dropna(\n",
    "    \"any\", subset=[\"ViewCount\", \"AcceptedAnswerId\"]\n",
    ")\n",
    "print(\n",
    "    \"Número de filas con ViewCount y AcceptedAnswerId ambos no nulos: {}.\".format(\n",
    "        dfNoNullViewCountAcceptedAnswerId.count()\n",
    "    )\n",
    ")\n",
    "\n",
    "# Elimina filas donde ViewCount y AcceptedAnswerId son AMBOS null\n",
    "dfAtLeastOneNotNull: DataFrame = dfSE.dropna(\"all\", subset=[\"ViewCount\", \"AcceptedAnswerId\"])\n",
    "print(\n",
    "    \"Número de filas con ViewCount O AcceptedAnswerId al menos uno no nulo: {}.\".format(\n",
    "        dfAtLeastOneNotNull.count()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reemplazar valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Reemplazamos los null en los campos ViewCount y AnswerCount\n",
    "dfSE: DataFrame = dfSE.fillna(0, subset=[\"ViewCount\", \"AnswerCount\"])\n",
    "dfSE.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Ejemplo más realista: Normalizar valores de PostTypeId\n",
    "# 1 = \"Question\", 2 = \"Answer\"\n",
    "# (se podría utilizar replace si la conversión de valores fuera del mismo tipo)\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "print(\"Antes de la transformación:\")\n",
    "dfSE.select(\"Id\", \"PostTypeId\").show(10)\n",
    "\n",
    "print(\"\\nDespués de la transformación (1→'Question', 2→'Answer'):\")\n",
    "dfSE_normalized: DataFrame = dfSE.withColumn(\n",
    "    \"PostTypeId\",\n",
    "    when(col(\"PostTypeId\") == 1, \"Question\")\n",
    "    .when(col(\"PostTypeId\") == 2, \"Answer\")\n",
    "    .otherwise(col(\"PostTypeId\").cast(StringType())),\n",
    ")\n",
    "dfSE_normalized.select(\"Id\", \"PostTypeId\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardando DataFrames\n",
    "\n",
    "Al igual que con la lectura, Spark puede guardar los DataFrames en múltiples formatos\n",
    "\n",
    "- CSV, JSON, Parquet, Hadoop...\n",
    "\n",
    "También puede escribir en bases de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modos de escritura**:\n",
    "\n",
    "Al guardar DataFrames, puedes especificar el modo con `.mode()`:\n",
    "\n",
    "- `\"overwrite\"`: Sobrescribe el directorio si ya existe (cuidado: borra los datos anteriores)\n",
    "- `\"append\"`: Añade los datos al final sin borrar lo existente\n",
    "- `\"ignore\"`: No hace nada si el directorio ya existe (silencioso)\n",
    "- `\"error\"` o `\"errorifexists\"` (default): Lanza error si ya existe\n",
    "\n",
    "**Comparación de formatos**:\n",
    "\n",
    "| Formato | Ventajas | Desventajas | Cuándo usar |\n",
    "|---------|----------|-------------|-------------|\n",
    "| **CSV** | Legible por humanos, universal | Grande, sin esquema tipado, lento | Intercambio simple, datos pequeños |\n",
    "| **JSON** | Flexible, soporta anidación | Grande, más lento | APIs, logs, datos semi-estructurados |\n",
    "| **Parquet** | Comprimido, columnar, muy rápido, preserva tipos | No legible directamente | **Big Data** (recomendado) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Guardo el DataFrame dfSE en formato JSON\n",
    "dfSE.write.format(\"json\").mode(\"overwrite\").save(\"dfSE.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "text/plain",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -lh dfSE.json\n",
    "head dfSE.json/part-*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Guardo el DataFrame usando Parquet\n",
    "dfSE.write.format(\"parquet\").mode(\"overwrite\").option(\"compression\", \"gzip\").save(\"dfSE.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(dfSE.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "text/plain",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Parquet usa por defecto formato comprimido snappy\n",
    "ls -lh dfSE.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crean tantos ficheros como particiones tenga el DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfSE2: DataFrame = dfSE.repartition(2)\n",
    "# Guardo el DataFrame  usando Parquet, con compresión gzip\n",
    "dfSE2.write.format(\"parquet\").mode(\"overwrite\").option(\"compression\", \"gzip\").save(\"dfSE2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "text/plain",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -lh dfSE2.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Particionado\n",
    "Permite particionar los ficheros guardados por el valor de una columna\n",
    "\n",
    "- Se crea un directorio por cada valor diferente en la columna de particionado\n",
    "    - Todos los datos asociados a ese valor se guardan en ese directorio\n",
    "- Permite simplificar el acceso a los valores asociados a una clave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Guardo el DataFrame particionado por el PostTypeId (usando Parquet)\n",
    "dfSE.write.format(\"parquet\").mode(\"overwrite\").partitionBy(\"PostTypeId\").save(\n",
    "    \"dfSE-particionado.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "text/plain",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls dfSE-particionado.parquet\n",
    "ls -lh dfSE-particionado.parquet/PostTypeId=2\n",
    "rm -rf dfSE-particionado.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recordatorio: Lazy Evaluation\n",
    "\n",
    "La mayoría de operaciones que hemos visto son **transformaciones lazy** (perezosas):\n",
    "- `.select()`, `.withColumn()`, `.drop()`, `.filter()`, `.dropna()`, etc.\n",
    "\n",
    "**No se ejecutan inmediatamente** - Spark solo construye un plan de ejecución.\n",
    "\n",
    "Las **acciones** que disparan la ejecución son:\n",
    "- `.show()`, `.count()`, `.collect()`, `.take()`\n",
    "- `.write.save()` (guardar a disco)\n",
    "\n",
    "Esto permite que Spark optimice todo el pipeline antes de ejecutar.\n",
    "\n",
    "---\n",
    "\n",
    "## Errores Comunes\n",
    "\n",
    "### 1. **`Py4JJavaError` o Java no encontrado**\n",
    "- **Causa**: Spark no encuentra Java o la versión es incompatible (necesita Java 8, 11 o 17)\n",
    "- **Solución**: Instalar Java 17 y configurar `JAVA_HOME`\n",
    "\n",
    "### 2. **`AnalysisException: Path does not exist`**\n",
    "- **Causa**: En modo YARN, el archivo no está en HDFS\n",
    "- **Solución**: Copiar con `hdfs dfs -put archivo.csv /user/luser/`\n",
    "\n",
    "### 3. **Out of Memory**\n",
    "- **Causa**: Cachear DataFrames muy grandes o hacer `.collect()` de millones de filas\n",
    "- **Solución**: Usar `.take(n)` o `.sample()`, no cachear innecesariamente\n",
    "\n",
    "### 4. **Inferencia de esquema lenta**\n",
    "- **Causa**: `inferSchema=true` debe leer todo el archivo\n",
    "- **Solución**: Especificar el esquema explícitamente con `StructType`\n",
    "\n",
    "### 5. **Variables no actualizadas después de transformaciones**\n",
    "- **Causa**: Olvidar reasignar la variable (los DataFrames son inmutables)\n",
    "- **Solución**: `df = df.withColumn(...)` o usar nombres distintos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
