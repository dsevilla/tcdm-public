{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkU_F8hZLeOL"
   },
   "source": [
    "# Operaciones básicas en Spark\n",
    "- Spark opera con colecciones **inmutables y distribuidas** de elementos, manipulándolos en paralelo\n",
    "    - API estructurada: DataFrames y DataSets\n",
    "    - API de bajo nivel: RDDs (ya obsoleto ya que el API estructurada es más eficiente y más de alto nivel)\n",
    "\n",
    "-   Operaciones sobre estas colecciones\n",
    "    -   Creación\n",
    "    -   Transformaciones (ordenación, filtrado, etc.)\n",
    "    -   Realización acciones para obtener resultados\n",
    "\n",
    "-   Spark automáticamente distribuye los datos y paraleliza las operaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bT8r0anjLeOW"
   },
   "source": [
    "### Ejemplo: creación de un DataFrame a partir de un fichero CSV\n",
    "En este ejemplo, Spark infiere el esquema de los datos de forma automática\n",
    "\n",
    "  - Es preferible especificar el esquema de forma explícita, como veremos más adelante\n",
    "\n",
    "También se especifica que la primera línea es la cabecera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset de ejemplo: Vuelos internacionales 2015\n",
    "\n",
    "El archivo `2015-summary.csv` contiene un resumen de vuelos internacionales en 2015:\n",
    "- **DEST_COUNTRY_NAME**: País de destino\n",
    "- **ORIGIN_COUNTRY_NAME**: País de origen\n",
    "- **count**: Número de vuelos entre ese par de países\n",
    "\n",
    "Es un dataset pequeño (~256 filas) ideal para aprender Spark sin sobrecargar recursos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "format": "text/plain",
    "id": "EwPKO0reLeOY",
    "outputId": "d4c99bfe-6467-46a2-97f1-f22f84e0ba06"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/dsevilla/tcdm-public/24-25/datos/2015-summary.csv\",\n",
    "    \"2015-summary.csv\",\n",
    ")\n",
    "!ls -lh 2015-summary.csv\n",
    "!head 2015-summary.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7SQV5O4sOcOn",
    "outputId": "2a2f7173-fb21-4a31-d3a5-d8c95cd4a396"
   },
   "outputs": [],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cZW1zRyPOo2e"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creamos un objeto SparkSession (o lo obtenemos si ya está creado)\n",
    "spark: SparkSession = (\n",
    "    SparkSession.builder.appName(\"Mi aplicacion\")\n",
    "    # Reducir particiones de shuffle para datasets pequeños (default: 200)\n",
    "    # (sólo se pone como ejemplo de cómo cambiar parámetros)\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "    .master(\"local[*]\")  # Usar todos los cores disponibles localmente\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# SparkContext solo es necesario para trabajar con RDDs (API antigua, no recomendada)\n",
    "# Con DataFrames/SQL normalmente no lo necesitas\n",
    "sc: SparkContext = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "id": "6nIgZApNLeOc"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "datosVuelos2015: DataFrame = (\n",
    "    spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"2015-summary.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3rdmzrqSLeOf",
    "outputId": "bf28e6c8-903d-487e-8faf-749d10c5a784"
   },
   "outputs": [],
   "source": [
    "datosVuelos2015.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DLGLlv8pLeOh",
    "outputId": "533cfc62-6e81-444a-e0d4-20c1befd0744"
   },
   "outputs": [],
   "source": [
    "datosVuelos2015.show()\n",
    "print(datosVuelos2015.count())\n",
    "\n",
    "assert datosVuelos2015.count() == 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t4TRlMpzLeOh",
    "outputId": "fcda2346-d37d-43da-c4e6-2f3a2ad0a50e"
   },
   "outputs": [],
   "source": [
    "datosVuelos2015.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYZBVIqiLeOi"
   },
   "source": [
    "### Rows\n",
    "\n",
    "Las filas de un DataFrame son objetos de tipo `Row`\n",
    "\n",
    "- API de Row en Python: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Row.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "id": "eZZFV6aJLeOm"
   },
   "outputs": [],
   "source": [
    "# Obtenemos las dos primeras fila del DataFrame\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "rows1_2: list[Row] = datosVuelos2015.take(2)\n",
    "print(rows1_2)\n",
    "print(type(rows1_2))\n",
    "print(type(rows1_2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "id": "lNQfTYQWLeOn"
   },
   "outputs": [],
   "source": [
    "# Obtén la primera fila como un diccionario Python\n",
    "print(rows1_2[0].asDict())\n",
    "print(type(rows1_2[0].asDict()))\n",
    "\n",
    "assert type(rows1_2[0].asDict()) is dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Cuándo es útil `Row.asDict()`?**\n",
    "- **Integración con código Python puro**: convertir datos Spark a estructuras nativas\n",
    "- **Serialización a JSON**: para APIs REST o almacenamiento\n",
    "- **Debugging**: inspeccionar valores específicos de forma legible\n",
    "- **Procesamiento individual**: cuando necesitas trabajar con una fila concreta fuera de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1QwipCULeOp"
   },
   "source": [
    "### Particiones\n",
    "\n",
    "Spark divide las filas DataFrame en un conjunto de particiones\n",
    "\n",
    "-   El número de particiones por defecto es función del tamaño del cluster (número total de cores en todos los ejecutores) y del tamaño de los datos (número de bloques de los ficheros en HDFS)\n",
    "-   Para RDDs se puede especificar otro valor en el momento de crearlos\n",
    "-   También se puede modificar una vez creados\n",
    "\n",
    "**¿Por qué importan las particiones?**\n",
    "- **Demasiadas particiones** → overhead de coordinación entre tareas\n",
    "- **Muy pocas particiones** → no se aprovecha el paralelismo disponible\n",
    "- **Regla general**: 2-4 particiones por CPU core disponible en el cluster\n",
    "- **Para shuffle operations**: `spark.sql.shuffle.partitions` (default: 200, puede ser excesivo para datos pequeños)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "id": "oCKT8sLhLeOp"
   },
   "outputs": [],
   "source": [
    "print(f\"Número de particiones: {datosVuelos2015.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Creo un nuevo DataFrame con 4 particiones\n",
    "datosVuelos2015_4P: DataFrame = datosVuelos2015.repartition(4)\n",
    "print(f\"Número de particiones: {datosVuelos2015_4P.rdd.getNumPartitions()}\")\n",
    "\n",
    "assert datosVuelos2015_4P.rdd.getNumPartitions() == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2rp6vPfLeOr"
   },
   "source": [
    "### Transformaciones\n",
    "\n",
    "Operaciones que transforman los datos\n",
    "\n",
    "  - No modifican los datos de origen (*inmutabilidad*)\n",
    "  - Se computan de forma \"perezosa\" (*lazyness*) - ver sección siguiente\n",
    "\n",
    "Dos tipos:\n",
    "\n",
    "  - **Transformaciones estrechas (narrow)**\n",
    "    - Cada partición de entrada contribuye a una única partición de salida\n",
    "    - No se modifica el número de particiones\n",
    "    - Normalmente se realizan en memoria\n",
    "    - **Ejemplos**: `select()`, `filter()`, `withColumn()`, `map()`\n",
    "    \n",
    "  - **Transformaciones anchas (wide)**\n",
    "    - Cada partición de salida depende de varias (o todas) particiones de entrada\n",
    "    - Suponen un barajado de datos (shuffle)\n",
    "    - Pueden implicar un cambio en el número de particiones\n",
    "    - Pueden suponer escrituras en disco\n",
    "    - **Ejemplos**: `groupBy()`, `sort()`/`orderBy()`, `join()`, `distinct()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "id": "boqxgnc7LeOs"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de una transformación narrow: reemplazar valores\n",
    "# replace() modifica valores en todas las columnas de tipo string\n",
    "datosVuelos2015_EEUU: DataFrame = datosVuelos2015.replace(\"United States\", \"Estados Unidos\")\n",
    "\n",
    "# Verificar el cambio (narrow: solo filtra filas)\n",
    "print(\"Antes del replace:\")\n",
    "datosVuelos2015.filter(datosVuelos2015.DEST_COUNTRY_NAME == \"United States\").show(3)\n",
    "\n",
    "print(\"\\nDespués del replace:\")\n",
    "datosVuelos2015_EEUU.filter(datosVuelos2015_EEUU.DEST_COUNTRY_NAME == \"Estados Unidos\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "id": "RoFCSMYxLeOt"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de una transformación wide: ordenar\n",
    "# sort() requiere shuffle porque todas las particiones necesitan compararse\n",
    "datosVuelos2015_Ord: DataFrame = datosVuelos2015_EEUU.sort(\"count\", ascending=False)\n",
    "\n",
    "# cache() mantiene el DataFrame en memoria para reutilizarlo sin recalcular\n",
    "# Útil cuando vas a usar el mismo DataFrame varias veces (ej: en acciones múltiples)\n",
    "datosVuelos2015_Ord.cache()\n",
    "\n",
    "print(\"Top 5 rutas con más vuelos:\")\n",
    "datosVuelos2015_Ord.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepto clave: Lazy Evaluation (Evaluación Perezosa)\n",
    "\n",
    "**Las transformaciones NO se ejecutan inmediatamente**\n",
    "\n",
    "Cuando escribes:\n",
    "```python\n",
    "df_filtrado = df.filter(col(\"edad\") > 18)\n",
    "df_ordenado = df_filtrado.orderBy(\"nombre\")\n",
    "```\n",
    "\n",
    "Spark **NO ejecuta** estas operaciones todavía. En su lugar:\n",
    "\n",
    "1. **Construye un plan de ejecución** (DAG - Grafo Dirigido Acíclico)\n",
    "2. **Optimiza el plan** (elimina pasos innecesarios, reordena operaciones)\n",
    "3. **Solo ejecuta** cuando llamas una **acción** (ej: `.show()`, `.count()`, `.collect()`)\n",
    "\n",
    "**Ventajas:**\n",
    "- Spark puede optimizar todo el pipeline antes de ejecutar\n",
    "- Evita cálculos innecesarios si solo necesitas una muestra\n",
    "- Permite fusionar transformaciones para mayor eficiencia\n",
    "\n",
    "**Ejemplo**: Si haces `.filter().map().filter()`, Spark puede combinar los dos filtros en uno solo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsvBBIjmLeOu"
   },
   "source": [
    "### Acciones\n",
    "\n",
    "Obtienen un resultado, forzando a que se realicen las transformaciones pendientes\n",
    "\n",
    "  - En el momento de disparar la *acción* se crea un *plan* con las transformaciones necesarias para obtener los datos solicitados\n",
    "    - Se crea un Grafo Dirigido Acíclico (DAG) conectando las transformaciones\n",
    "    - Spark optimiza ese grafo, para eliminar transformaciones innecesarias o unir las que sea posible\n",
    "  - Las acciones traducen el DAG en un plan de ejecución\n",
    "\n",
    "Tipos de acciones\n",
    "\n",
    "  - Acciones para mostrar datos por consola\n",
    "  - Acciones para convertir datos Spark en datos del lenguaje\n",
    "  - Acciones para escribir datos a disco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "id": "T9H6efssLeOv"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de acciones\n",
    "from pprint import pp\n",
    "\n",
    "print(f\"Número de filas en la tabla: {datosVuelos2015_Ord.count()}\")\n",
    "\n",
    "pp(datosVuelos2015_Ord.take(3))\n",
    "\n",
    "datosVuelos2015_Ord.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando el plan de ejecución\n",
    "\n",
    "Spark permite inspeccionar el plan físico que ejecutará"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver el plan de ejecución físico optimizado\n",
    "print(\"Plan de ejecución para datosVuelos2015_Ord:\")\n",
    "datosVuelos2015_Ord.explain(mode=\"formatted\")\n",
    "\n",
    "# También puedes usar mode=\"simple\", \"extended\", o \"cost\" para más detalles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark UI**: Para una visualización más rica del DAG y métricas de ejecución:\n",
    "- Accede a `http://localhost:4040` mientras tu aplicación Spark está ejecutándose\n",
    "- Muestra: stages, tasks, storage, environment, SQL queries con planes visuales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo completo: Pipeline típico en Spark\n",
    "\n",
    "Veamos un ejemplo que integra todo lo aprendido: transformaciones narrow y wide, acciones, y optimización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "# Pipeline completo: análisis de países con más tráfico aéreo\n",
    "resultado: DataFrame = (\n",
    "    # 1. Leer datos con esquema inferido\n",
    "    spark.read.option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(\"2015-summary.csv\")\n",
    "    # 2. Filtrar solo rutas con tráfico significativo (transformación narrow)\n",
    "    .filter(col(\"count\") > 100)\n",
    "    # 3. Agrupar por país de destino (transformación wide - shuffle)\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\n",
    "    # 4. Calcular total de vuelos por destino (agregación)\n",
    "    .agg(spark_sum(\"count\").alias(\"total_vuelos\"))\n",
    "    # 5. Ordenar por total descendente (transformación wide - shuffle)\n",
    "    .orderBy(col(\"total_vuelos\").desc())\n",
    "    # 6. Limitar a top 10 (transformación narrow)\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "# NOTA: Hasta aquí NO se ha ejecutado nada (lazy evaluation)\n",
    "# Solo al llamar .show() se dispara la ejecución completa\n",
    "\n",
    "print(\"Top 10 países destino por volumen de vuelos (rutas con >100 vuelos):\\n\")\n",
    "resultado.show()  # Acción que dispara todo el pipeline\n",
    "\n",
    "# Verificar que obtuvimos exactamente 10 resultados\n",
    "assert resultado.count() == 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análisis del pipeline anterior:**\n",
    "\n",
    "1. **Transformaciones narrow**: `filter()`, `limit()` - no requieren shuffle\n",
    "2. **Transformaciones wide**: `groupBy()` + `agg()`, `orderBy()` - requieren shuffle\n",
    "3. **Optimización automática**: Spark reordena operaciones para minimizar datos procesados\n",
    "4. **Lazy evaluation**: Todo el plan se ejecuta solo al llamar `.show()`\n",
    "\n",
    "**Tip**: Usar `.explain()` en el resultado muestra cómo Spark optimizó el pipeline"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
