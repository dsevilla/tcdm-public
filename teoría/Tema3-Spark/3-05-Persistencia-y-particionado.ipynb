{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistencia y particionado\n",
    "En este tema trataremos dos aspectos de Apache Spark\n",
    "\n",
    "- **Persistencia**: cómo guardar DataFrames y RDDs de forma que no tengan que ser recalculados\n",
    "- **Particionado**: cómo especificar y cambiar las particiones de un DataFrame o RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistencia\n",
    "\n",
    "Problema al usar un DataFrame o un RDD varias veces:\n",
    "\n",
    "-   Spark recomputa el RDD y sus dependencias cada vez que se ejecuta una acción\n",
    "-   Muy costoso (especialmente en problemas iterativos)\n",
    "\n",
    "Solución\n",
    "\n",
    "-   Conservar el DataFrame o RDD en memoria y/o disco\n",
    "-   Métodos `cache()` o `persist()`\n",
    "\n",
    "### Niveles de persistencia (definidos en [`pyspark.StorageLevel`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.StorageLevel.html) y [`org.apache.spark.storage.StorageLevel`](https://spark.apache.org/docs/3.5.1/api/scala/org/apache/spark/storage/StorageLevel$.html))\n",
    "\n",
    " Nivel                | Espacio  | CPU     | Memoria/Disco   | Descripción\n",
    " :------------------: | :------: | :-----: | :-------------: | ------------------\n",
    " MEMORY_ONLY          |   Alto   |   Bajo  |     Memoria     | Guarda el RDD como un objeto Java no serializado en la JVM. Si el RDD no cabe en memoria, algunas particiones no se *cachearán* y serán recomputadas \"al vuelo\" cada vez que se necesiten. \n",
    " MEMORY_AND_DISK      |   Alto   |   Medio |     Ambos       | Guarda el RDD como un objeto Java no serializado en la JVM. Si el RDD no cabe en memoria, las particiones que no quepan se guardan en disco y se leen del mismo cada vez que se necesiten.\n",
    " DISK_ONLY            |   Bajo   |   Alto  |     Disco       | Guarda las particiones del RDD solo en disco.\n",
    " OFF_HEAP             |   Bajo   |   Alto  |   Memoria       | Similar a MEMORY_ONLY_SER pero guarda el RDD serializado usando memoria *off-heap* (fuera del heap de la JVM) lo que puede reducir el overhead del recolector de basura.\n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "### Nivel de persistencia\n",
    "\n",
    "-   El nivel por defecto para DataFrames es MEMORY_ONLY\n",
    "\n",
    "-   En Python, los datos siempre se guardan en memoria serializados (usando *pickle*)\n",
    "\n",
    "    - Es posible especificar serialización (la forma en la que se serializan los datos para mantenerlos en memoria o en disco). Por defecto se utiliza el serializador \"Pickle\" en Python\n",
    "\n",
    "    \n",
    "### Recuperación de fallos\n",
    "\n",
    "-   Si falla un nodo con datos almacenados, el DataFrame o RDD se recomputa\n",
    "\n",
    "    -   Añadiendo `_2` (ó `_3`) al nivel de persistencia (por ejemplo, MEMORY_ONLY_2), se guardan 2 copias del RDD\n",
    "        \n",
    "### Gestión de la cache\n",
    "\n",
    "-   Algoritmo LRU (Least Recently Used) para gestionar la cache\n",
    "\n",
    "    -   Para niveles *solo memoria*, los RDDs viejos se eliminan y se recalculan\n",
    "    -   Para niveles *memoria y disco*, las particiones que no caben se escriben a disco\n",
    "    \n",
    "### Importante:\n",
    "\n",
    "- La persistencia debe usarse solo cuando sea necesaria, puesto que puede implicar un coste importante\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistencia con DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Elegir el máster de Spark dependiendo de si se ha definido la variable de entorno HADOOP_CONF_DIR o YARN_CONF_DIR\n",
    "SPARK_MASTER: str = (\n",
    "    \"yarn\" if \"HADOOP_CONF_DIR\" in os.environ or \"YARN_CONF_DIR\" in os.environ else \"local[*]\"\n",
    ")\n",
    "print(f\"Usando Spark Master en {SPARK_MASTER}\")\n",
    "\n",
    "# Creamos un objeto SparkSession (o lo obtenemos si ya está creado)\n",
    "spark: SparkSession = (\n",
    "    SparkSession.builder.appName(\"Mi aplicacion\")\n",
    "    .config(\"spark.rdd.compress\", \"true\")\n",
    "    .config(\"spark.executor.memory\", \"6g\")\n",
    "    .config(\"spark.driver.memory\", \"6g\")\n",
    "    .master(SPARK_MASTER)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc: SparkContext = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from collections.abc import Iterator\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "\n",
    "def generate_random_data(n: int) -> Iterator[Row]:\n",
    "    \"\"\"Generador que produce Rows con datos aleatorios sin almacenarlos previamente.\"\"\"\n",
    "    row_type = Row(\"n\", \"x\")\n",
    "    yield from (row_type(i, float(np.random.random())) for i in range(n))\n",
    "\n",
    "\n",
    "DF1: DataFrame = spark.createDataFrame(generate_random_data(100000))\n",
    "\n",
    "DF1.printSchema()\n",
    "print(f\"Cacheado: {DF1.is_cached}.\")\n",
    "print(f\"Nivel sin persistencia: {DF1.storageLevel}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "DF1.cache()\n",
    "print(f\"Cacheado: {DF1.is_cached}.\")\n",
    "print(f\"Nivel de persistencia por defecto: {DF1.storageLevel}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# La persistencia no se hereda en las transformaciones\n",
    "DF2: DataFrame = DF1.groupBy(\"x\").count()\n",
    "print(f\"Cacheado: {DF2.is_cached}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# Para cambiar el nivel de persistencia, primero tenemos que quitarlo de la cache\n",
    "DF1.unpersist()\n",
    "\n",
    "DF1.persist(StorageLevel.MEMORY_ONLY_2)\n",
    "print(f\"Cacheado: {DF1.is_cached}.\")\n",
    "print(f\"Número de particiones: {DF1.rdd.getNumPartitions()}.\")\n",
    "print(f\"Nuevo nivel de persistencia: {DF1.storageLevel}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistencia con RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark import RDD\n",
    "\n",
    "rdd: RDD[int] = sc.parallelize(range(1000), 10)\n",
    "print(f\"Cacheado: {rdd.is_cached}\")\n",
    "print(f\"Particiones: {rdd.getNumPartitions()}\")\n",
    "print(f\"Nivel de persistencia sin cachear: {rdd.getStorageLevel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd.cache()\n",
    "\n",
    "print(f\"Cacheado: {rdd.is_cached}\")\n",
    "print(f\"Nivel de persistencia por defecto: {rdd.getStorageLevel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particionado en Spark\n",
    "\n",
    "El particionado es crucial para el rendimiento de Spark. Afecta a:\n",
    "- **Paralelismo**: más particiones → más tareas paralelas\n",
    "- **Shuffles**: operaciones como joins y aggregaciones reorganizan datos entre particiones\n",
    "- **Escritura de archivos**: cada partición genera uno o más archivos\n",
    "- **Localidad de datos**: mantener datos relacionados juntos reduce la comunicación de red\n",
    "\n",
    "### Conceptos clave\n",
    "\n",
    "**Para DataFrames:**\n",
    "- `spark.sql.shuffle.partitions` (por defecto 200): número de particiones en operaciones *wide* (joins, aggregaciones)\n",
    "- Se puede cambiar dinámicamente: `spark.conf.set(\"spark.sql.shuffle.partitions\", 100)`\n",
    "\n",
    "**Para RDDs:**\n",
    "- `spark.default.parallelism`: número de particiones por defecto en transformaciones\n",
    "- Propiedad: `sc.defaultParallelism`\n",
    "\n",
    "**Funciones útiles:**\n",
    "- `df.rdd.getNumPartitions()`: obtener número de particiones\n",
    "- `df.repartition(n)`: crear exactamente n particiones (provoca shuffle completo)\n",
    "- `df.coalesce(n)`: reducir particiones sin shuffle completo (más eficiente)\n",
    "- `df.repartition(n, \"columna\")`: particionar por columna específica\n",
    "\n",
    "### Cuándo preocuparse por el particionado\n",
    "\n",
    "1. **Escritura de archivos** → muchas particiones = muchos archivos pequeños\n",
    "2. **Joins grandes** → mal particionado = shuffles lentos\n",
    "3. **Agregaciones con datos desbalanceados** → algunas particiones muy grandes\n",
    "4. **Out of Memory** → demasiados datos en una partición"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caso 1: Particionado al escribir archivos Parquet\n",
    "\n",
    "El problema más común: escribir DataFrames con muchas particiones genera miles de archivos pequeños (*small files problem*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Creamos un DataFrame de ejemplo\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "\n",
    "def data_gen() -> Iterable[Row]:\n",
    "    for _ in range(1000):\n",
    "        yield from [\n",
    "            Row(usuario_id=1, país=\"ES\", año=2023, posts=10),\n",
    "            Row(usuario_id=2, país=\"FR\", año=2023, posts=5),\n",
    "            Row(usuario_id=3, país=\"ES\", año=2024, posts=8),\n",
    "            Row(usuario_id=4, país=\"IT\", año=2023, posts=12),\n",
    "            Row(usuario_id=5, país=\"FR\", año=2024, posts=15),\n",
    "            Row(usuario_id=6, país=\"ES\", año=2024, posts=20),\n",
    "        ]\n",
    "\n",
    "\n",
    "dfUsuarios: DataFrame = spark.createDataFrame(data_gen())\n",
    "print(f\"Número de particiones por defecto: {dfUsuarios.rdd.getNumPartitions()}\")\n",
    "dfUsuarios.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# MALO: escribir con muchas particiones crea muchos archivos pequeños\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "output_dir = \"/tmp/usuarios_mal_particionado\"\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "\n",
    "dfUsuarios.write.parquet(output_dir)\n",
    "\n",
    "# Veamos cuántos archivos se crearon\n",
    "archivos: list[str] = [f for f in os.listdir(output_dir) if f.endswith('.parquet')]\n",
    "print(f\"Se crearon {len(archivos)} archivos pequeños\")\n",
    "print(f\"Archivos: {archivos[:5]}...\")  # Mostramos solo los primeros 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUENO: consolidar particiones antes de escribir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "output_dir_bueno = \"/tmp/usuarios_bien_particionado\"\n",
    "if os.path.exists(output_dir_bueno):\n",
    "    shutil.rmtree(output_dir_bueno)\n",
    "\n",
    "# Reducir a 2 particiones antes de escribir\n",
    "dfUsuarios.coalesce(2).write.parquet(output_dir_bueno)\n",
    "\n",
    "archivos_buenos: list[str] = [f for f in os.listdir(output_dir_bueno) if f.endswith('.parquet')]\n",
    "print(f\"Se crearon {len(archivos_buenos)} archivos optimizados\")\n",
    "print(f\"Archivos: {archivos_buenos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEJOR: Particionar por columnas para consultas eficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "output_dir_particionado = \"/tmp/usuarios_particionado\"\n",
    "if os.path.exists(output_dir_particionado):\n",
    "    shutil.rmtree(output_dir_particionado)\n",
    "\n",
    "# Particionar por año y país crea una estructura de directorios\n",
    "dfUsuarios.write.partitionBy(\"año\", \"país\").parquet(output_dir_particionado)\n",
    "\n",
    "print(\"Estructura de directorios creada:\")\n",
    "for root, dirs, files in os.walk(output_dir_particionado):\n",
    "    level: int = root.replace(output_dir_particionado, '').count(os.sep)\n",
    "    indent: str = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    if level < 2:  # Solo mostramos 2 niveles\n",
    "        subindent: str = ' ' * 2 * (level + 1)\n",
    "        for file in files[:2]:  # Solo primeros 2 archivos\n",
    "            print(f'{subindent}{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Ventaja: Spark solo lee las particiones necesarias (predicate pushdown)\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_leido: DataFrame = spark.read.parquet(output_dir_particionado)\n",
    "\n",
    "# Solo lee la carpeta año=2024/pais=ES/\n",
    "df_filtrado: DataFrame = df_leido.filter((col(\"año\") == 2024) & (col(\"país\") == \"ES\"))\n",
    "\n",
    "print(\"Al filtrar por año y país, Spark solo lee esas particiones:\")\n",
    "df_filtrado.show(5)\n",
    "\n",
    "# Podemos ver qué particiones se leyeron en el plan de ejecución\n",
    "df_filtrado.explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": "auto"
   },
   "source": [
    "### Caso 2: Particionado para joins eficientes\n",
    "\n",
    "Cuando hacemos un join, Spark debe reorganizar los datos (shuffle). Reparticionar por la clave del join puede mejorar el rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Creamos dos DataFrames para hacer join\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "dfPreguntas: DataFrame = spark.createDataFrame(\n",
    "    [Row(pregunta_id=i, usuario_id=i % 100, texto=f\"Pregunta {i}\") for i in range(1000)]\n",
    ")\n",
    "\n",
    "dfRespuestas: DataFrame = spark.createDataFrame(\n",
    "    [Row(respuesta_id=i, pregunta_id=i % 500, respuesta=f\"Respuesta {i}\") for i in range(2000)]\n",
    ")\n",
    "\n",
    "print(f\"Preguntas: {dfPreguntas.rdd.getNumPartitions()} particiones\")\n",
    "print(f\"Respuestas: {dfRespuestas.rdd.getNumPartitions()} particiones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join sin optimizar: Spark hace shuffle con 200 particiones (por defecto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "resultado_normal: DataFrame = dfPreguntas.join(dfRespuestas, \"pregunta_id\", \"inner\")\n",
    "print(f\"Join normal: {resultado_normal.rdd.getNumPartitions()} particiones\")\n",
    "resultado_normal.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": "auto"
   },
   "source": [
    "Optimizado: reparticionamos ambos DataFrames por la clave del join.\n",
    "Esto co-localiza los datos relacionados en las mismas particiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "dfPreguntas_part: DataFrame = dfPreguntas.repartition(50, \"pregunta_id\")\n",
    "dfRespuestas_part: DataFrame = dfRespuestas.repartition(50, \"pregunta_id\")\n",
    "\n",
    "resultado_optimizado: DataFrame = dfPreguntas_part.join(dfRespuestas_part, \"pregunta_id\", \"inner\")\n",
    "print(f\"Join optimizado: {resultado_optimizado.rdd.getNumPartitions()} particiones\")\n",
    "resultado_optimizado.show(5)\n",
    "\n",
    "# El join será más eficiente porque los datos ya están co-localizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": "auto"
   },
   "source": [
    "### Caso 3: Control del número de particiones en shuffles\n",
    "\n",
    "Por defecto, Spark usa 200 particiones en operaciones *wide*. Esto puede ser demasiado o insuficiente dependiendo del tamaño de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Ver el valor actual de spark.sql.shuffle.partitions\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "print(f\"Particiones por defecto en shuffles: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "\n",
    "# Hacer una agregación (operación wide)\n",
    "resultado_agg: DataFrame = dfUsuarios.groupBy(\"país\", \"año\").sum(\"posts\")\n",
    "print(f\"Particiones después de groupBy: {resultado_agg.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Ajustar el número de particiones para datasets pequeños\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 10)\n",
    "\n",
    "resultado_agg_optimizado: DataFrame = dfUsuarios.groupBy(\"país\", \"año\").sum(\"posts\")\n",
    "print(\n",
    "    f\"Particiones después de groupBy optimizado: {resultado_agg_optimizado.rdd.getNumPartitions()}\"\n",
    ")\n",
    "\n",
    "# Regla general:\n",
    "# - Datos pequeños (< 1GB): 10-50 particiones\n",
    "# - Datos medianos (1-10GB): 50-200 particiones\n",
    "# - Datos grandes (> 10GB): 200+ particiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": "auto"
   },
   "source": [
    "### Caso 4: Adaptive Query Execution (AQE)\n",
    "\n",
    "AQE es una característica moderna de Spark que optimiza automáticamente el número de particiones durante la ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Habilitar AQE (en Spark 3.2+ está habilitado por defecto)\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "\n",
    "print(\"Configuración de AQE:\")\n",
    "print(f\"  - AQE habilitado: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
    "print(f\"  - Coalesce automático: {spark.conf.get('spark.sql.adaptive.coalescePartitions.enabled')}\")\n",
    "print(f\"  - Manejo de skewed joins: {spark.conf.get('spark.sql.adaptive.skewJoin.enabled')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Con AQE, Spark ajustará automáticamente las particiones\n",
    "# Ejemplo: después de un filtro que reduce mucho los datos\n",
    "\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)  # Empezamos con 200\n",
    "\n",
    "resultado_filtrado: DataFrame = dfUsuarios.filter(col(\"país\") == \"ES\").groupBy(\"año\").sum(\"posts\")\n",
    "\n",
    "print(\"Particiones iniciales configuradas: 200\")\n",
    "print(f\"Particiones finales (después de AQE): {resultado_filtrado.rdd.getNumPartitions()}\")\n",
    "print(\"\\nAQE combinó particiones pequeñas automáticamente\")\n",
    "\n",
    "resultado_filtrado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": "auto"
   },
   "source": [
    "### Caso 5: Bucketing para tablas persistentes\n",
    "\n",
    "Bucketing es útil cuando haremos joins repetidos sobre las mismas columnas. Pre-organiza los datos en \"buckets\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Crear tablas con bucketing\n",
    "# Los datos se organizan en buckets por usuario_id\n",
    "\n",
    "# Primero limpiamos tablas existentes\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS preguntas_bucketed\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS respuestas_bucketed\")\n",
    "\n",
    "# Crear tablas con bucketing\n",
    "dfPreguntas.write.bucketBy(50, \"usuario_id\").sortBy(\"pregunta_id\").mode(\"overwrite\").saveAsTable(\n",
    "    \"preguntas_bucketed\"\n",
    ")\n",
    "\n",
    "# Crear DataFrame de respuestas con usuario_id\n",
    "dfRespuestas_conUsuario: DataFrame = dfRespuestas.withColumn(\"usuario_id\", col(\"pregunta_id\") % 100)\n",
    "\n",
    "dfRespuestas_conUsuario.write.bucketBy(50, \"usuario_id\").sortBy(\"respuesta_id\").mode(\n",
    "    \"overwrite\"\n",
    ").saveAsTable(\"respuestas_bucketed\")\n",
    "\n",
    "print(\"Tablas creadas con bucketing por usuario_id.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "text/plain"
   },
   "outputs": [],
   "source": [
    "# Los joins sobre tablas bucketed son mucho más eficientes\n",
    "# No necesitan shuffle porque los datos ya están co-localizados\n",
    "\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "df_preg: DataFrame = spark.table(\"preguntas_bucketed\")\n",
    "df_resp: DataFrame = spark.table(\"respuestas_bucketed\")\n",
    "\n",
    "resultado_bucketed: DataFrame = df_preg.join(df_resp, \"usuario_id\", \"inner\")\n",
    "\n",
    "print(\"Join sobre tablas bucketed:\")\n",
    "resultado_bucketed.show(5)\n",
    "\n",
    "# Podemos ver en el plan de ejecución que NO hay shuffle\n",
    "print(\"\\nPlan de ejecución (nota: sin Exchange/Shuffle):\")\n",
    "resultado_bucketed.explain(\"simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caso 6: Manejo de datos desbalanceados (Data Skew)\n",
    "\n",
    "Cuando algunos valores de una clave tienen muchos más datos que otros, puede causar que algunas tareas tomen mucho más tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: crear datos desbalanceados (un usuario con muchos posts)\n",
    "\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "data_skewed = []\n",
    "# Usuario 1 tiene 10,000 posts (skew!)\n",
    "data_skewed.extend([Row(usuario_id=1, post=f\"Post {i}\") for i in range(10000)])\n",
    "# Usuarios 2-100 tienen solo 10 posts cada uno\n",
    "for user in range(2, 101):\n",
    "    data_skewed.extend([Row(usuario_id=user, post=f\"Post {i}\") for i in range(10)])\n",
    "\n",
    "dfSkewed: DataFrame = spark.createDataFrame(data_skewed)\n",
    "\n",
    "print(f\"Total de filas: {dfSkewed.count()}\")\n",
    "print(\"\\nDistribución de posts por usuario:\")\n",
    "dfSkewed.groupBy(\"usuario_id\").count().orderBy(col(\"count\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sin AQE, el skew causa que una tarea sea muy lenta\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 10)\n",
    "\n",
    "# Hacer un groupBy - una partición tendrá 10,000 filas, las demás ~10\n",
    "resultado_sin_aqe: DataFrame = dfSkewed.groupBy(\"usuario_id\").count()\n",
    "print(f\"Particiones: {resultado_sin_aqe.rdd.getNumPartitions()}\")\n",
    "print(\"Una partición tendrá 10,000 filas (usuario 1), causando desbalance\")\n",
    "resultado_sin_aqe.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con AQE, Spark detecta y maneja el skew automáticamente\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"10MB\")\n",
    "\n",
    "resultado_con_aqe: DataFrame = dfSkewed.groupBy(\"usuario_id\").count()\n",
    "print(\"AQE detectará y manejará la partición desbalanceada automáticamente\")\n",
    "resultado_con_aqe.show(5)\n",
    "\n",
    "# En un join con skew, AQE dividirá la partición grande en varias más pequeñas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apéndice: Operaciones avanzadas con RDDs\n",
    "\n",
    "Las siguientes operaciones son útiles para casos específicos con RDDs, pero raramente necesarias con DataFrames modernos.\n",
    "\n",
    "### Trabajando a nivel de partición con RDDs\n",
    "\n",
    "Permite aplicar operaciones una vez por partición en lugar de una vez por elemento (útil para operaciones costosas como conexiones a bases de datos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: mapPartitions con RDDs\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from pyspark import RDD\n",
    "\n",
    "rdd_nums: RDD[int] = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9], 4)\n",
    "print(\"Datos por partición:\")\n",
    "print(rdd_nums.glom().collect())\n",
    "\n",
    "\n",
    "def suma_y_cuenta(iter: Iterable[int]) -> list[tuple[int, int]]:\n",
    "    \"\"\"Suma y cuenta elementos de una partición\"\"\"\n",
    "    suma = 0\n",
    "    cuenta = 0\n",
    "    for i in iter:\n",
    "        suma += i\n",
    "        cuenta += 1\n",
    "    return [(suma, cuenta)]\n",
    "\n",
    "\n",
    "# Aplica la función una vez por partición (no por elemento)\n",
    "resultado: RDD[tuple[int, int]] = rdd_nums.mapPartitions(suma_y_cuenta)\n",
    "print(\"\\nSuma y cuenta por partición:\")\n",
    "print(resultado.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo con partitionBy para RDDs clave-valor\n",
    "rdd_pairs: RDD[tuple[int, int]] = sc.parallelize([(1, 10), (2, 20), (1, 15), (3, 30), (2, 25)], 3)\n",
    "\n",
    "print(f\"Particiones originales: {rdd_pairs.getNumPartitions()}\")\n",
    "print(\"Datos por partición:\")\n",
    "print(rdd_pairs.glom().collect())\n",
    "\n",
    "# Particionar por clave (agrupa claves iguales en la misma partición)\n",
    "rdd_particionado = rdd_pairs.partitionBy(2)\n",
    "print(f\"\\nParticiones después de partitionBy(2): {rdd_particionado.getNumPartitions()}\")\n",
    "print(\"Datos por partición (claves iguales juntas):\")\n",
    "print(rdd_particionado.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen: Mejores prácticas de particionado\n",
    "\n",
    "### Siempre hacer:\n",
    "\n",
    "1. **Usar `coalesce()` antes de escribir** para evitar archivos pequeños\n",
    "   ```python\n",
    "   df.coalesce(10).write.parquet(\"output/\")\n",
    "   ```\n",
    "\n",
    "2. **Particionar datos por columnas de filtrado frecuente**\n",
    "   ```python\n",
    "   df.write.partitionBy(\"año\", \"país\").parquet(\"output/\")\n",
    "   ```\n",
    "\n",
    "3. **Habilitar AQE** (en Spark 3.2+ está habilitado por defecto)\n",
    "   ```python\n",
    "   spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "   ```\n",
    "\n",
    "4. **Ajustar `spark.sql.shuffle.partitions` según el tamaño de los datos**\n",
    "   - Pequeño (< 1GB): 10-50 particiones\n",
    "   - Mediano (1-10GB): 50-200 particiones\n",
    "   - Grande (> 10GB): 200+ particiones\n",
    "\n",
    "### Evitar:\n",
    "\n",
    "1. **Demasiadas particiones pequeñas** → overhead de scheduling\n",
    "2. **Muy pocas particiones grandes** → poco paralelismo, riesgo de OOM\n",
    "3. **Repartición innecesaria** → shuffle costoso sin beneficio\n",
    "4. **Particionamiento excesivo al escribir** → demasiadas carpetas/archivos\n",
    "\n",
    "### Cuándo reparticionar:\n",
    "\n",
    "- **Antes de joins grandes**: reparticionar por la clave del join\n",
    "- **Después de filtros fuertes**: usar `coalesce()` para reducir particiones vacías\n",
    "- **Antes de escribir**: consolidar con `coalesce()` para archivos optimizados\n",
    "- **Con datos desbalanceados**: dejar que AQE lo maneje automáticamente\n",
    "\n",
    "### Herramientas de diagnóstico:\n",
    "\n",
    "```python\n",
    "# Ver el plan de ejecución\n",
    "df.explain(\"formatted\")\n",
    "\n",
    "# Ver número de particiones\n",
    "df.rdd.getNumPartitions()\n",
    "\n",
    "# Ver configuración actual\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza de archivos y tablas temporales\n",
    "import shutil\n",
    "\n",
    "# Eliminar directorios temporales\n",
    "for path in [\n",
    "    \"/tmp/usuarios_mal_particionado\",\n",
    "    \"/tmp/usuarios_bien_particionado\",\n",
    "    \"/tmp/usuarios_particionado\",\n",
    "]:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Eliminado: {path}.\")\n",
    "\n",
    "# Eliminar tablas temporales\n",
    "spark.sql(\"DROP TABLE IF EXISTS preguntas_bucketed\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS respuestas_bucketed\")\n",
    "print(\"Tablas temporales eliminadas.\")\n",
    "\n",
    "print(\"\\nLimpieza completada.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
