{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones con DataFrames\n",
    "Veremos distintas operaciones que se pueden hacer con los DataFrames:\n",
    "\n",
    "  - Filtrado de filas\n",
    "  - Ordenación y agrupamiento\n",
    "  - Joins\n",
    "  - Funciones escalares y agregados\n",
    "  - Manejo de tipos complejos\n",
    "  - Funciones de ventana\n",
    "  - Funciones definidas por el usuario\n",
    " \n",
    "Acabaremos viendo como usar consultas SQL sobre DataFrames\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "RunningInCOLAB: bool = 'google.colab' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RunningInCOLAB:\n",
    "    !sudo apt-get update ; sudo apt-get install -y default-jre-headless\n",
    "    %env JAVA_HOME=/usr/lib/jvm/default-java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Elegir el máster de Spark dependiendo de si se ha definido la variable de entorno HADOOP_CONF_DIR o YARN_CONF_DIR\n",
    "SPARK_MASTER: str = (\n",
    "    \"yarn\" if \"HADOOP_CONF_DIR\" in os.environ or \"YARN_CONF_DIR\" in os.environ else \"local[*]\"\n",
    ")\n",
    "\n",
    "# Creamos un objeto SparkSession (o lo obtenemos si ya está creado)\n",
    "\n",
    "sb: SparkSession.Builder = (\n",
    "    SparkSession.builder.appName(\"Mi aplicacion\")\n",
    "    .config(\"spark.rdd.compress\", \"true\")\n",
    "    .config(\"spark.executor.memory\", \"6g\")\n",
    "    .config(\"spark.driver.memory\", \"6g\")\n",
    ")\n",
    "sb = sb.config(\"spark.sql.warehouse.dir\", \".\") if SPARK_MASTER != \"yarn\" else sb\n",
    "sb = sb.master(SPARK_MASTER)\n",
    "\n",
    "spark: SparkSession = sb.getOrCreate()\n",
    "\n",
    "sc: SparkContext = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract on-the-fly using streaming\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "dfSE_file: str = \"dfSE.parquet.tar.gz\"\n",
    "dfSE_parquet_remote_url: str = (\n",
    "    f\"https://github.com/dsevilla/tcdm-public/releases/download/dfse-parquet-tar-gz/{dfSE_file}\"\n",
    ")\n",
    "\n",
    "print(\"Downloading and extracting dfSE.parquet...\")\n",
    "\n",
    "# Open URL and wrap it in a tarfile reader directly\n",
    "with urllib.request.urlopen(dfSE_parquet_remote_url) as response:\n",
    "    # Create a tarfile object that reads from the network stream\n",
    "    with tarfile.open(fileobj=response, mode=\"r|gz\") as tar:\n",
    "        # Extract all members while streaming\n",
    "        tar.extractall()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Recupero el DataFrame leyéndolo del formato parquet\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "dfSE: DataFrame = spark.read.format(\"parquet\").option(\"mode\", \"FAILFAST\").load(\"dfSE.parquet\")\n",
    "dfSE.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfSE.show(5)\n",
    "dfSE.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dfSE.count() == 410346"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones de filtrado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecciona los post que tengan la palabra Italiano en su cuerpo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.column import Column\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "colCuerpo: Column = col(\"Body\")\n",
    "dfConItaliano: DataFrame = dfSE.filter(colCuerpo.like(\"%Italiano%\"))\n",
    "\n",
    "n_italiano: int = dfConItaliano.count()\n",
    "print(f\"Número de posts con la palabra Italiano: {n_italiano}\\n\")\n",
    "\n",
    "assert n_italiano == 32\n",
    "\n",
    "print(\"Una de las filas\")\n",
    "dfConItaliano.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Obtenemos las preguntas (PostTypeId == 1) que tienen una respuesta aceptada (AcceptedAnswerId != null)\n",
    "# Nota: where() es un alias de filter()\n",
    "\n",
    "postTypeIdCol: Column = col(\"PostTypeId\")\n",
    "acceptedAnswerIdCol: Column = col(\"AcceptedAnswerId\")\n",
    "\n",
    "questionsWithAcceptedAnswersDf: DataFrame = dfSE.where(\n",
    "    (postTypeIdCol == 1) & (acceptedAnswerIdCol.isNotNull())\n",
    ").withColumnRenamed(\"CreationDate\", \"Fecha_de_creación\")\n",
    "\n",
    "print(f\"Número de preguntas con respuesta aceptada: {questionsWithAcceptedAnswersDf.count()}.\")\n",
    "\n",
    "questionsWithAcceptedAnswersDf.cache()\n",
    "\n",
    "(\n",
    "    questionsWithAcceptedAnswersDf.select(\n",
    "        \"Fecha_de_creación\", postTypeIdCol.alias(\"Tipo Post\"), acceptedAnswerIdCol\n",
    "    ).show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos con las entradas correspondientes a junio de 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "fechaCreacionCol: Column = col(\"Fecha_de_creación\")\n",
    "\n",
    "# Comparar fechas\n",
    "dfPregConRespAceptJun16: DataFrame = questionsWithAcceptedAnswersDf.filter(\n",
    "    (fechaCreacionCol >= date(2016, 6, 1)) & (fechaCreacionCol <= date(2016, 6, 30))\n",
    ")\n",
    "\n",
    "dfPregConRespAceptJun16.select(fechaCreacionCol, postTypeIdCol, acceptedAnswerIdCol).show(\n",
    "    truncate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadimos una columna que contenga el ratio entre el número de vistas y el score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "colNumVistas: Column = col(\"ViewCount\")\n",
    "colPuntos: Column = col(\"Score\")\n",
    "dfPregConRespAceptyRatio: DataFrame = questionsWithAcceptedAnswersDf.withColumn(\n",
    "    \"ratio\", when(colPuntos != 0, colNumVistas / colPuntos)\n",
    ")\n",
    "\n",
    "# Muestra algunas columnas con ratio > 35\n",
    "colRatio: Column = col(\"ratio\")\n",
    "(\n",
    "    dfPregConRespAceptyRatio.filter(colRatio > 35)\n",
    "    .select(fechaCreacionCol, colNumVistas, colPuntos, colRatio)\n",
    "    .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones de ordenación y agrupamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Ordenamos por viewCount\n",
    "questionsWithAcceptedAnswersDf.orderBy(colNumVistas.desc()).select(\n",
    "    fechaCreacionCol, colNumVistas\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Creamos una agrupación por la columna OwnerUserId\n",
    "from pyspark.sql.group import GroupedData\n",
    "\n",
    "colUserId: Column = col(\"OwnerUserId\")\n",
    "grupoPorUsuario: GroupedData = questionsWithAcceptedAnswersDf.groupBy(colUserId)\n",
    "print(type(grupoPorUsuario))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(\"DataFrame con el número de posts por usuario.\")\n",
    "dfPostPorUsuario: DataFrame = grupoPorUsuario.count()\n",
    "dfPostPorUsuario.printSchema()\n",
    "\n",
    "colNPosts: Column = col(\"count\")\n",
    "dfPostPorUsuario.select(\n",
    "    colUserId.alias(\"Número de usuario\"), colNPosts.alias(\"Número de posts\")\n",
    ").orderBy(colNPosts, ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(\"DataFrame con la media de vistas por usuario:\")\n",
    "dfAvgPorUsuario: DataFrame = grupoPorUsuario.avg(\"ViewCount\").withColumnRenamed(\n",
    "    \"avg(ViewCount)\", \"Media_vistas\"\n",
    ")\n",
    "dfAvgPorUsuario.orderBy(\"Media_vistas\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método agg permite hacer varias operaciones de agrupamiento, expresadas como un diccionario `nombre_columna:operación`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(\"Obtenemos las tablas anteriores con una sola operación.\")\n",
    "dfCountyAvg: DataFrame = grupoPorUsuario.agg({\"OwnerUserId\": \"count\", \"ViewCount\": \"avg\"})\n",
    "dfCountyAvg.printSchema()\n",
    "\n",
    "colCount: Column = col(\"count(OwnerUserId)\")\n",
    "colMedia: Column = col(\"avg(ViewCount)\")\n",
    "dfCountyAvg.select(\n",
    "    colUserId.alias(\"Número de usuario\"),\n",
    "    colCount.alias(\"Número de posts\"),\n",
    "    colMedia.alias(\"Media de vistas\"),\n",
    ").orderBy(colUserId).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agrupación sobre dos columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfSE.groupBy(colUserId, postTypeIdCol).count().orderBy(colUserId.asc(), postTypeIdCol.desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una descripción de las funciones que se pueden usar con GroupedData está en https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensiones del groupBy\n",
    "\n",
    "Funciones `rollup` y `cube`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rollup\n",
    "\n",
    "Incluye filas adicionales con agregados por la primera columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Contar para cada usuario el número de preguntas (PostTypeId = 1) y el número de respuestas (PostTypeId = 2)\n",
    "rollupPorUsuarioyTipoPost: GroupedData = dfSE.rollup(\"OwnerUserId\", \"PostTypeId\")\n",
    "print(type(rollupPorUsuarioyTipoPost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# DataFrame con el número de post por usuario y tipo pregunta\n",
    "# Los campos a null son de agregación, por ejemplo:\n",
    "# null null = todos los posts\n",
    "# 4    null = todos los posts del usuario con id 4\n",
    "# 4    1    = todos los post de tipo 1 del usuario 4\n",
    "dfPostPorUsuarioyTipo: DataFrame = rollupPorUsuarioyTipoPost.count()\n",
    "dfPostPorUsuarioyTipo.printSchema()\n",
    "dfPostPorUsuarioyTipo.select(\n",
    "    colUserId.alias(\"Número de usuario\"),\n",
    "    postTypeIdCol.alias(\"Tipo de post\"),\n",
    "    colNPosts.alias(\"Número de posts\"),\n",
    ").orderBy(colUserId, postTypeIdCol).show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cubes\n",
    "\n",
    "Similar al Rollups, pero recorriendo todas las dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "grupoPorUsuarioyTipoPost: GroupedData = dfSE.cube(\"OwnerUserId\", \"PostTypeId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# DataFrame con el número de post por usuario y tipo pregunta\n",
    "# Los campos a null son de agregación, por ejemplo:\n",
    "# null null = todos los posts\n",
    "# null 1    = todos los posts de tipo 1\n",
    "# 4    null = todos los posts del usuario con id 4\n",
    "# 4    1    = todos los posts de tipo 1 del usuario 4\n",
    "dfPostPorUsuarioyTipo: DataFrame = grupoPorUsuarioyTipoPost.count()\n",
    "dfPostPorUsuarioyTipo.printSchema()\n",
    "dfPostPorUsuarioyTipo.select(\n",
    "    colUserId.alias(\"Número de usuario\"),\n",
    "    postTypeIdCol.alias(\"Tipo de post\"),\n",
    "    colNPosts.alias(\"Número de posts\"),\n",
    ").orderBy(colUserId, postTypeIdCol).show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins\n",
    "Spark ofrece la posibilidad de realizar múltiples tipos de joins\n",
    "\n",
    "  - inner, outer, left outer, right outer, left semi, left anti, cross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscamos unir las preguntas con respuesta aceptada con la respuesta que se ha elegido como aceptada. Unimos el campo `AcceptedAnswerId` de las preguntas con el campo `id` de las respuestas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfPreguntas: DataFrame = (\n",
    "    questionsWithAcceptedAnswersDf.select(colUserId, colCuerpo, acceptedAnswerIdCol)\n",
    "    .withColumnRenamed(\"OwnerUserId\", \"Usuario pregunta\")\n",
    "    .withColumnRenamed(\"Body\", \"Pregunta\")\n",
    "    .withColumnRenamed(\"AcceptedAnswerId\", \"ID Resp Aceptada\")\n",
    ")\n",
    "\n",
    "colId: Column = col(\"Id\")\n",
    "dfRespuestas: DataFrame = (\n",
    "    dfSE.select(colId, colUserId, colCuerpo)\n",
    "    .where(postTypeIdCol == 2)\n",
    "    .withColumnRenamed(\"Id\", \"ID Respuesta\")\n",
    "    .withColumnRenamed(\"OwnerUserId\", \"Usuario respuesta\")\n",
    "    .withColumnRenamed(\"Body\", \"Respuesta\")\n",
    ")\n",
    "\n",
    "nPreguntas: int = dfPreguntas.count()\n",
    "AnswerCount: int = dfRespuestas.count()\n",
    "print(f\"Número de preguntas con respuesta aceptada = {nPreguntas}.\")\n",
    "print(f\"Número de respuestas = {AnswerCount}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Expresión para el join\n",
    "joinExpression: Column = dfPreguntas[\"ID Resp Aceptada\"] == dfRespuestas[\"ID Respuesta\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inner join. Sólo se incluyen las filas para las que la joinExpression es *true*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "joinType = \"inner\"\n",
    "dfInner: DataFrame = dfPreguntas.join(dfRespuestas, joinExpression, joinType)\n",
    "nFilas: int = dfInner.count()\n",
    "print(f\"Número de filas = {nFilas}.\")\n",
    "dfInner.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outer join. Incluye todas las filas de ambos DataFrames. En el caso de que no haya equivalente en alguno de los DataFrame, se meten nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "joinType = \"outer\"\n",
    "dfOuter: DataFrame = dfPreguntas.join(dfRespuestas, joinExpression, joinType)\n",
    "nFilas: int = dfOuter.count()\n",
    "print(f\"Número de filas = {nFilas}.\")\n",
    "dfOuter.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left Outer join. Incluye todas las filas del DataFrame de la izquierda (primer DataFrame). Si no hay equivalencia en el de la derecha, se pone null.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "joinType = \"left_outer\"\n",
    "dfLOuter: DataFrame = dfPreguntas.join(dfRespuestas, joinExpression, joinType)\n",
    "nFilas = dfLOuter.count()\n",
    "print(f\"Número de filas = {nFilas}.\")\n",
    "dfLOuter.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right Outer join. Incluye todas las filas del DataFrame de la derecha (segundo DataFrame). Si no hay equivalencia en el de la izquierda, se pone null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "joinType = \"right_outer\"\n",
    "dfROuter: DataFrame = dfPreguntas.join(dfRespuestas, joinExpression, joinType)\n",
    "nFilas: int = dfROuter.count()\n",
    "print(f\"Número de filas = {nFilas}.\")\n",
    "dfROuter.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left Semi join. El resultado incluyen los valores del primer DataFrame que existen en el segundo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "joinType = \"left_semi\"\n",
    "dfLSemi: DataFrame = dfRespuestas.join(dfPreguntas, joinExpression, joinType)\n",
    "nFilas: int = dfLSemi.count()\n",
    "print(f\"Número de filas = {nFilas}.\")\n",
    "dfLSemi.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left Anti join. El resultado incluyen los valores del primer DataFrame que **NO** existen en el segundo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "joinType = \"left_anti\"\n",
    "dfLAnti: DataFrame = dfRespuestas.join(dfPreguntas, joinExpression, joinType)\n",
    "nFilas: int = dfLAnti.count()\n",
    "print(f\"Número de filas = {nFilas}.\")\n",
    "dfLAnti.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Cross join\n",
    "# Producto cartesiano, une cada fila del primer DataFrame con todas las del segundo\n",
    "# NO DEBE USARSE, EXTREMADAMENTE COSTOSO\n",
    "dfCross: DataFrame = dfRespuestas.crossJoin(dfPreguntas)\n",
    "# nFilas = dfCross.count()\n",
    "# print(\"Número de filas = {0}.\".format(nFilas))\n",
    "# dfCross.show(100)\n",
    "\n",
    "# Número de filas = 17456332965."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de broadcast join y explicación del plan\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Supongamos que dfRespuestas es pequeño respecto a dfPreguntas (o viceversa)\n",
    "dfInner_broadcast: DataFrame = dfPreguntas.join(broadcast(dfRespuestas), joinExpression, \"inner\")\n",
    "dfInner_broadcast.explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notas prácticas sobre joins\n",
    "- Usa broadcast joins cuando una de las tablas es pequeña: evita shuffles grandes.\n",
    "- Inspecciona el plan físico con `explain()` para entender costes.\n",
    "- Evita `crossJoin` salvo en demos controladas: es muy costoso.\n",
    "- En `left_semi` y `left_anti`, solo se devuelven columnas de la tabla de la izquierda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones escalares y agregados\n",
    "\n",
    "Spark ofrece un ámplio abanico de funciones para operar con los DataFrames:\n",
    "- Funciones matemáticas: ``abs``, ``log``, ``hypot``, etc.\n",
    "- Operaciones con strings: ``length``, ``concat``, etc.\n",
    "- Operaciones con fechas: ``year``, ``date_add``, etc.\n",
    "- Operaciones de agregación: ``min``, ``max``, ``count``, ``avg``, ``sum``, ``sumDistinct``, ``stddev``, ``variance``, ``kurtosis``, ``skewness``, ``first``, ``last``, ``window``, etc.\n",
    "\n",
    "Una descripción de estas funciones se puede encontrar en <https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, datediff\n",
    "\n",
    "colUltimaActividad: Column = col(\"LastActivityDate\")\n",
    "fechaCreacionCol: Column = col(\"Fecha_de_creación\")\n",
    "# Buscamos la pregunta con respuesta aceptada que estuvo más tiempo activa\n",
    "# (con la mayor diferencia entre los valores de LastActivityDate y Fecha de creacion)\n",
    "masActiva: Row | None = (\n",
    "    questionsWithAcceptedAnswersDf.withColumn(\n",
    "        \"tiempoActiva\", datediff(colUltimaActividad, fechaCreacionCol)\n",
    "    )\n",
    "    .orderBy(\"tiempoActiva\", ascending=False)\n",
    "    .head()\n",
    ")\n",
    "print(f\"La pregunta \\n\\n{masActiva.Body}\\n\\nestuvo activa {masActiva.tiempoActiva} días.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "\n",
    "# Obtenemos el número de post por semana de cada usuario\n",
    "# Agrupamos por OwnerUserId y una ventana de fechas de creación de 1 semana\n",
    "questionsWithAcceptedAnswersDf.groupBy(\n",
    "    colUserId, window(fechaCreacionCol, \"1 week\").alias(\"Semana\")\n",
    ").count().sort(\"count\", ascending=False).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Buscar la media y máximo de la columna \"Score\" de todas las filas y el número total del DataFrame completo.\n",
    "dfSE.select(F.avg(colPuntos), F.max(colPuntos), F.count(colPuntos)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Otra forma usando describe\n",
    "dfSE.select(colPuntos).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipos complejos\n",
    "\n",
    "Spark permite trabajar con tres tipos de datos complejos: `structs`, `arrays` y `maps`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structs\n",
    "\n",
    "DataFrames dentro de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, struct\n",
    "\n",
    "# Creamos un nuevo DF con una columna que combina dos columnas existentes\n",
    "colId: Column = col(\"Id\")\n",
    "colNumVistas: Column = col(\"ViewCount\")\n",
    "colNRespuestas: Column = col(\"AnswerCount\")\n",
    "dfStruct: DataFrame = dfSE.select(\n",
    "    colId,\n",
    "    colNumVistas,\n",
    "    colNRespuestas,\n",
    "    struct(colNumVistas, colNRespuestas).alias(\"Vistas_Respuestas\"),\n",
    ")\n",
    "dfStruct.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfStruct.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Obtenemos un campo de la columna compuesta\n",
    "dfStruct.select(col(\"Vistas_Respuestas\").getField(\"ViewCount\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrays\n",
    "\n",
    "Permiten trabajar con datos como si fuera un array Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ejemplo*\n",
    "\n",
    "Obtener el número de *tags* para cada pregunta con respuesta aceptada y eliminar los símbolos ``<`` y ``>``:\n",
    "\n",
    "  - Las \"tags\" de cada pregunta se guardan concatenadas, separadas por < y >\n",
    "\n",
    "E.g.: `<english-comparison><translation><phrase-request>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Obtenemos un DataFrame sin tags nulas\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "dfSE.show(10)\n",
    "dfNoNullTags: DataFrame = dfSE.dropna(\"any\", subset=[\"Tags\"])\n",
    "dfNoNullTags.select(\"Tags\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Añado una columna con las etiquetas separadas\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "colTags: Column = col(\"Tags\")\n",
    "dfTags: DataFrame = dfNoNullTags.withColumn(\"tag_array\", split(colTags, \"><\"))\n",
    "dfTags.select(col(\"tag_array\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfTags.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "# Normalizamos el array de tags: minúsculas y sin símbolos < >\n",
    "colTag_array: Column = col(\"tag_array\")\n",
    "dfTagsClean: DataFrame = dfTags.withColumn(\n",
    "    \"tag_array_clean\", F.transform(colTag_array, lambda t: F.lower(F.regexp_replace(t, \"[<>]\", \"\")))\n",
    ")\n",
    "\n",
    "dfTagsClean.select(colTag_array.alias(\"tag_array\"), F.col(\"tag_array_clean\")).show(5, False)\n",
    "\n",
    "\n",
    "# Ejemplo de filtrado de elementos del array: quedarnos solo con 'python'\n",
    "dfTagsWithPython: DataFrame = dfTagsClean.filter(\n",
    "    F.array_contains('tag_array_clean', 'python')\n",
    ").select('Id', 'Title', F.col('tag_array_clean').alias('tag_array_clean_with_python'))\n",
    "dfTagsWithPython.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size\n",
    "\n",
    "# Mostramos el número de etiquetas de cada entrada\n",
    "colTag_array: Column = col(\"tag_array_clean\")\n",
    "dfTagsClean.select(colTag_array, size(colTag_array)).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Mostramos la primera etiqueta de cada entrada\n",
    "dfTagsClean.selectExpr(\"tag_array_clean\", \"tag_array_clean[0]\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "# Miramos si en las tags aparece la palabra \"mysql\"\n",
    "dfTagsClean.withColumn(\"Con_mysql\", array_contains(colTag_array, \"mysql\")).select(\n",
    "    colTag_array, col(\"Con_mysql\")\n",
    ").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# Convertimos cada etiqueta en una fila\n",
    "# Sugerencia: si hubiera nulos, usar explode_outer para no perder filas\n",
    "dfTagsRows: DataFrame = dfTagsClean.withColumn(\"Tags2\", explode(colTag_array))\n",
    "dfTagsRows.select('Id', 'Title', colTags, col(\"Tags2\")).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Número de entradas con la etiqueta `lisp`. Nótese que es equivalente a las entradas que tienen `lisp` además de otras, ya que al buscarla en la columna *exploded* si una fila tenía esa etiqueta, aparece, y sólo una vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Número de entradas con la etiqueta <lisp> = {}.\".format(\n",
    "        dfTagsRows.filter(col(\"Tags2\") == \"lisp\").count()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de ventana\n",
    "\n",
    "Similares a las de funciones de agregación, permiten operar en grupos de filas devolviendo un único valor para cada fila. Esto permite, entre otras cosas:\n",
    "\n",
    "  - Obtener medias móviles\n",
    "  - Calcular sumas acumuladas\n",
    "  - Acceder a los valores de una fila por encima de la actual\n",
    "\n",
    "Básicamente, una función de ventana (window function) calcula un valor para cada fila de entrada de una tabla en base a un grupo de filas, denominado *frame*.\n",
    "\n",
    "Como funciones de ventana se puede usar las funciones de agregación ya comentadas y otras funciones adicionales (``cume_dist``, ``dense_rank``, ``lag``, ``lead``, ``ntile``, ``percent_rank``, ``rank``, ``row_number``) especificadas como *Window functions* en <https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/window.html>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo 1\n",
    "A partir del DataFrame ``dfPregConRespAcept``, mostrar la puntuación (columna \"Score\") máxima por usuario, y, para cada pregunta, la diferencia de su puntuación con el máximo del usuario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window, WindowSpec\n",
    "\n",
    "# Especificamos la ventana que particiona las filas por la columna OwnerUserId\n",
    "ventana: WindowSpec = Window.partitionBy(colUserId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una columna con los máximos valores de `Score` por usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "colMaxPuntos: Column = F.max(colPuntos).over(ventana)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos un nuevo DataFrame incluyendo la puntuación máxima por usuario y la diferencia entre este máximo y la puntuación de cada pregunta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "questionsWithAcceptedAnswersDf.select(\n",
    "    colUserId, colId.alias(\"Pregunta\"), colPuntos, colMaxPuntos.alias(\"maxPorUsuario\")\n",
    ").withColumn(\"Diferencia\", colMaxPuntos - colPuntos).orderBy(colUserId, colId).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo 2\n",
    "Mostrar para cada usuario y pregunta del DataFrame ``dfPregConRespAcept`` el número de días que pasaron desde la anterior pregunta del usuario hasta la actual, y desde esta hasta la siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Especificamos la ventana que particiona las filas por la columna OwnerUserId y las ordena por fecha de creación\n",
    "from pyspark.sql.window import WindowSpec\n",
    "\n",
    "ventana: WindowSpec = Window.partitionBy(colUserId).orderBy(fechaCreacionCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Creamos una columna que referencia a la pregunta anterior por fecha\n",
    "colAnterior: Column = F.lag(fechaCreacionCol, 1).over(ventana)\n",
    "# Creamos una columna que referencia a la pregunta posterior por fecha\n",
    "colPosterior: Column = F.lead(fechaCreacionCol, 1).over(ventana)\n",
    "\n",
    "# Mostramos para cada usuario y pregunta el id de la pregunta anterior y posterior\n",
    "questionsWithAcceptedAnswersDf.select(\n",
    "    colUserId,\n",
    "    colId,\n",
    "    fechaCreacionCol.alias(\"Fecha de creación\"),\n",
    "    F.datediff(fechaCreacionCol, colAnterior).alias(\"Días desde\"),\n",
    "    F.datediff(colPosterior, fechaCreacionCol).alias(\"Días hasta\"),\n",
    ").orderBy(colUserId, colId).show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Media móvil de 3 observaciones y suma acumulada del `Score` por usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.column import Column\n",
    "from pyspark.sql.window import Window, WindowSpec\n",
    "\n",
    "# Ventana ordenada por fecha, por usuario\n",
    "ventana_ordenada: WindowSpec = Window.partitionBy(colUserId).orderBy(fechaCreacionCol)\n",
    "\n",
    "# Media móvil (3 últimas filas incluyendo la actual)\n",
    "ventana_media3: WindowSpec = ventana_ordenada.rowsBetween(-2, 0)\n",
    "media_movil_3: Column = F.avg(colPuntos).over(ventana_media3)\n",
    "questionsWithAcceptedAnswersDf.select(\n",
    "    colUserId,\n",
    "    colId.alias(\"Pregunta\"),\n",
    "    fechaCreacionCol.alias(\"Fecha\"),\n",
    "    colPuntos.alias(\"Score\"),\n",
    "    media_movil_3.alias(\"media_movil_3\"),\n",
    ").orderBy(colUserId, fechaCreacionCol).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suma acumulada del `Score` por usuario (orden cronológico)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window, WindowSpec\n",
    "\n",
    "ventana_acum: WindowSpec = (\n",
    "    Window.partitionBy(colUserId)\n",
    "    .orderBy(fechaCreacionCol)\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")\n",
    "suma_acumulada: Column = F.sum(colPuntos).over(ventana_acum)\n",
    "\n",
    "questionsWithAcceptedAnswersDf.select(\n",
    "    colUserId,\n",
    "    colId.alias(\"Pregunta\"),\n",
    "    fechaCreacionCol.alias(\"Fecha\"),\n",
    "    colPuntos.alias(\"Score\"),\n",
    "    suma_acumulada.alias(\"score_acumulado\"),\n",
    ").orderBy(colUserId, fechaCreacionCol).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones definidas por el usuario (UDFs)\n",
    "\n",
    "Si queremos una función que no está implementada, podemos crear nuestra propia función que opere sobre columnas.\n",
    "\n",
    "  - Las UDFs en Python pueden ser bastante ineficientes, debido a la serialización de datos a Python\n",
    "  - Preferible programarlas en Scala o Java (se pueden usar desde Python)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo\n",
    "\n",
    "Usar UDFs para obtener el número de *tags* para cada pregunta y cambiar los ``<`` y ``>`` por vacíos, y el ``><`` por una coma.\n",
    "\n",
    "Para contar el número de tags, basta con contar el número de apariciones de ``><`` en el string y sumar 1. (La cuenta se hace sobre el campo sin modificar porque se hace a la vez ambos UDFs. Si se hiciera en dos pasos, se podía contar los `,`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "colTags: Column = col(\"Tags\")\n",
    "# Obtenemos un DataFrame sin tags nulas\n",
    "dfNoNullTags: DataFrame = dfSE.dropna(\"any\", subset=[\"Tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Se puede hacer de dos formas, o bien con una función o\n",
    "# con una anotación @udf\n",
    "\n",
    "# Definimos una función que devuelva el número de tags en un string\n",
    "\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def udfCuentaTags(tags: str) -> int:\n",
    "    return tags.count(\"><\") + 1\n",
    "\n",
    "\n",
    "# Definimos una función que reemplace < y > por vacíos, y el ``><`` por una coma.\n",
    "def reemplazaTags(tags: str) -> str:\n",
    "    return tags.replace(\"><\", \",\").replace(\"<\", \"\").replace(\">\", \"\")\n",
    "\n",
    "\n",
    "# Creamos udfs a partir de esta última función\n",
    "udfReemplazaTags = udf(reemplazaTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfNoNullTags.select(\n",
    "    udfReemplazaTags(colTags).alias(\"Etiquetas\"), udfCuentaTags(colTags).alias(\"nEtiquetas\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Llamo a las UDFs Scala usando una expresión (si estuvieran definidas en Scala)\n",
    "# dfNoNullTags.selectExpr(\"udfReemplazaTagsSc(Tags) AS Etiquetas\",\n",
    "#                              \"udfCuentaTagsSc(Tags) AS nEtiquetas\")\\\n",
    "#                  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de sentencias SQL\n",
    "\n",
    "Las sentencias SQL ejecutadas desde Spark se trasladan a operaciones sobre DataFrames\n",
    "\n",
    " - Se pueden ejecutar sentencias remotas a través del servidor JDBC/ODBC [Thrift](https://spark.apache.org/docs/latest/sql-programming-guide.html#distributed-sql-engine)\n",
    " - También puede trabajar con datos almacenados en [Apache Hive](https://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables)\n",
    "\n",
    "Para usar sentencias SQL sobre un DataFrame, este tiene que registrarse como una *tabla* o *vista*\n",
    "\n",
    " - la vista puede crearse como temporal (desaparece al terminar la sesión) o global (se mantiene entre sesiones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Registra el DataFrame dfPregConRespAcept como una vista temporal\n",
    "questionsWithAcceptedAnswersDf.createOrReplaceTempView(\"tabla_PregConRespAcept\")\n",
    "\n",
    "# Crea una tabla con los datos guardados en Parquet\n",
    "spark.sql(\n",
    "    \"\"\"CREATE TABLE tabla_SE\n",
    "             USING PARQUET OPTIONS (path 'dfSE.parquet')\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM tabla_SE\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Ejecuta un comando SQL sobre la tabla\n",
    "dfUser100: DataFrame = spark.sql(\n",
    "    \"\"\"SELECT OwnerUserId,Id FROM tabla_SE\n",
    "                         WHERE OwnerUserId >= 100\"\"\"\n",
    ")\n",
    "dfUser100.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Podemos ver las tablas creadas\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Podemos crear un nuevo DataFrame a partir de una de la tablas\n",
    "dfFromTable: DataFrame = spark.sql(\"SELECT * FROM tabla_PregConRespAcept\")\n",
    "dfFromTable.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS tabla_PregConRespAcept\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS tabla_SE\")\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
