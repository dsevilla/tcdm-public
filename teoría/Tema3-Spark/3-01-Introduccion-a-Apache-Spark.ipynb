{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmaXjOYU9BZ5"
   },
   "source": [
    "# Introducción a [Apache Spark](https://spark.apache.org/), curso 25-26\n",
    "\n",
    "Objetivo del notebook: crear una SparkSession, entender la arquitectura básica (Driver/Ejecutors), diferenciar API estructurada (DataFrames) vs RDDs, y ejecutar transformaciones/acciones con explicación del plan de ejecución.\n",
    "\n",
    "![Spark](https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iY1A1qo59BaB"
   },
   "source": [
    "## Plataforma de computación cluster rápida\n",
    "\n",
    "- Extiende el modelo MapReduce y soporta eficientemente otros tipos de computación:\n",
    "  - Queries interactivas (SQL/DataFrames)\n",
    "  - Procesado streaming (*Structured Streaming*)\n",
    "- Soporta computaciones en memoria y optimizaciones a nivel de motor (Catalyst + Tungsten)\n",
    "- Mejora frente a MapReduce clásico para cargas complejas (aceleración significativa en ETL e interactivo; el factor depende del caso)\n",
    "\n",
    "### Propósito general\n",
    "\n",
    "- Modos de funcionamiento: batch, interactivo y streaming\n",
    "- Reduce el número de herramientas a emplear y mantener\n",
    "\n",
    "Modos de despliegue habituales: `local[*]`, Standalone, YARN, Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdhFGnOE9BaC"
   },
   "source": [
    "### Historia\n",
    "\n",
    "- Iniciado en 2009 en el UC Berkeley RAD Lab (AMPLab)\n",
    "  - Motivado por la ineficiencia de MapReduce para trabajos iterativos e interactivos\n",
    "- Mayores contribuidores: [Databricks](https://databricks.com/), Yahoo! e Intel\n",
    "- Declarado open source en marzo de 2010\n",
    "- Transferido a la Apache Software Foundation en junio de 2013, TLP en febrero de 2014\n",
    "- Uno de los proyectos Big Data más activos\n",
    "- Versión 1.0 lanzada en mayo de 2014\n",
    "\n",
    "Hoy (Spark 4.x) incorpora mejoras como Adaptive Query Execution (AQE), Pandas UDFs mejoradas, ANSI SQL mode y optimizaciones en el planificador. Notas: https://spark.apache.org/releases/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4678NCtQ9BaE"
   },
   "source": [
    "#### Características de Spark\n",
    "\n",
    "- Soporta gran variedad de workloads: batch, queries interactivas (SQL/DataFrames), streaming (Structured Streaming), machine learning (MLlib), y grafos (GraphFrames en el ecosistema)\n",
    "- APIs en Scala, Java, Python, SQL y R\n",
    "- Shells interactivos en Scala, Python, SQL y R\n",
    "- Se integra con otras soluciones Big Data: HDFS, S3, Kafka, Cassandra, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZIsv7Aq9BaF"
   },
   "source": [
    "### La pila Spark\n",
    "![sparkstack](http://persoal.citius.usc.es/tf.pena/TCDM/figs/sparkstack.png)\n",
    "\n",
    "(Fuente: H. Karau, A. Konwinski, P. Wendell, M. Zaharia, \"Learning Spark\", O'Reilly, 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dE0EWa-N9BaF"
   },
   "source": [
    "## APIs del Spark Core\n",
    "Spark ofrece dos APIs:\n",
    "\n",
    "- API estructurada o de alto nivel (SQL/DataFrames/Datasets)\n",
    "- API de bajo nivel (RDD)\n",
    "\n",
    "Cada API ofrece diferentes tipos de datos y trade-offs:\n",
    "\n",
    "- Se recomienda usar la API estructurada por su mayor rendimiento y expresividad SQL\n",
    "- La API de bajo nivel permite mayor control sobre particionado y algoritmos custom\n",
    "- La API de alto nivel utiliza internamente primitivas de bajo nivel\n",
    "\n",
    "## Tipos de datos en la API estructurada\n",
    "\n",
    "### Datasets\n",
    "Colección distribuida de objetos del mismo tipo (tipados)\n",
    "\n",
    "- Introducida en Spark ≥ 1.6\n",
    "- Disponible en Scala y Java (no en Python/R por tipado dinámico)\n",
    "\n",
    "### DataFrames\n",
    "Colección distribuida organizada en columnas con nombre\n",
    "\n",
    "- Conceptualmente: tabla relacional / pandas DataFrame\n",
    "- Disponible en Scala, Java, Python y R\n",
    "- En Java/Scala: un DataFrame es un Dataset[Row]\n",
    "\n",
    "## Tipos de datos en la API de bajo nivel\n",
    "### RDDs (Resilient Distributed Datasets)\n",
    "\n",
    "- Lista distribuida de objetos; API base de Spark 1.x\n",
    "- Útiles cuando se requiere control fino (particiones, operaciones no expresables en SQL)\n",
    "\n",
    "## Mejor rendimiento de la API estructurada\n",
    "\n",
    "- DataFrames/Datasets aprovechan Catalyst (optimizador de consultas) y Tungsten (motor de ejecución)\n",
    "\n",
    "<img src=\"https://databricks.com/wp-content/uploads/2015/02/Screen-Shot-2015-02-16-at-9.46.39-AM.png\" alt=\"Mejora de rendimiento\" style=\"width: 650px;\"/>\n",
    "\n",
    "Fuente: [Recent performance improvements in Apache Spark: SQL, Python, DataFrames, and More](https://databricks.com/blog/2015/04/24/recent-performance-improvements-in-apache-spark-sql-python-dataframes-and-more.html)\n",
    "\n",
    "### ¿Cuándo usar cada API?\n",
    "- DataFrame (por defecto): ETL, SQL, agregaciones, joins; mejor rendimiento\n",
    "- RDD: lógica no expresable en SQL, control de particiones, algoritmos personalizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_Wb31ZH9BaH"
   },
   "source": [
    "## Conceptos clave\n",
    "![sparkcontext](http://persoal.citius.usc.es/tf.pena/TCDM/figs/sparkcontext.png)\n",
    "\n",
    "Términos básicos:\n",
    "- Job: conjunto de stages disparado por una acción\n",
    "- Stage: grupo de tareas sin shuffle entre ellas\n",
    "- Task: unidad de trabajo sobre una partición\n",
    "- Partición: porción del dataset procesada en paralelo\n",
    "- DAG: grafo acíclico de transformaciones (evaluación perezosa)\n",
    "- Dependencias: narrow (sin shuffle) vs wide (con shuffle)\n",
    "\n",
    "(Fuente: H. Karau, A. Konwinski, P. Wendell, M. Zaharia, \"Learning Spark\", O'Reilly, 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RsHsyhL9BaK"
   },
   "source": [
    "#### Driver\n",
    "\n",
    "- Crea un `SparkContext`\n",
    "- Convierte el programa de usuario en tareas:\n",
    "  - `DAG` de operaciones lógico -> plan de ejecución físico\n",
    "- Planifica las tareas en los ejecutores\n",
    "\n",
    "#### SparkSession y SparkContext\n",
    "\n",
    "- `SparkSession`: punto de entrada de todas las funcionalidades de Spark\n",
    "  - Permite especificar la configuración de la aplicación Spark\n",
    "  - En notebooks/shell puede existir ya creada (variable `spark`)\n",
    "- `SparkContext`: realiza la conexión con el cluster\n",
    "  - Se obtiene desde `SparkSession` (`spark.sparkContext`)\n",
    "  - Punto de entrada para la API de bajo nivel\n",
    "  - En el notebook (o shell), puede estar predefinida como `sc`\n",
    "\n",
    "- Creación en un script Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CP82EWfZ9BaH"
   },
   "outputs": [],
   "source": [
    "%pip install -q pyspark\n",
    "\n",
    "import sys\n",
    "\n",
    "import pyspark\n",
    "\n",
    "print(\"PySpark\", pyspark.__version__, \"con Python\", sys.version.split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugerBGuGH4qL"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear u obtener una SparkSession (en local). En clúster, no fijar master aquí.\n",
    "spark: SparkSession = (\n",
    "    SparkSession.builder.appName(\"TCDM-IntroduccionSpark-3-01\")\n",
    "    .master(\"local[*]\")  # solo para entorno local/notebook\n",
    "    .config(\n",
    "        \"spark.sql.shuffle.partitions\", \"8\"\n",
    "    )  # demo de parámetro: particiones por defecto en local\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc: SparkContext = spark.sparkContext\n",
    "\n",
    "print(\"Spark version:\", spark.version, \"| Master:\", sc.master, \"| App:\", sc.appName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTjzXOuD9BaN"
   },
   "source": [
    "#### Executors\n",
    "\n",
    "- Ejecutan las tareas individuales y devuelven los resultados al Driver\n",
    "- Proporcionan almacenamiento en memoria para los datos de las tareas\n",
    "\n",
    "#### Cluster Manager\n",
    "\n",
    "- Componente enchufable en Spark\n",
    "- Modos típicos: Standalone, YARN, Kubernetes (Mesos en desuso)\n",
    "- El modo `local[*]` emula ejecución local sin ejecutores separados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbgfWQ0t9BaO"
   },
   "source": [
    "## Instalación de Spark\n",
    "\n",
    "- Para **Scala**:\n",
    "  1. Descargar Apache Spark de http://spark.apache.org/downloads.html\n",
    "     - “Pre-built for Hadoop 3.x and later” incorpora binarios compatibles\n",
    "     - También se puede construir desde el código fuente\n",
    "  2. Extraer el fichero descargado\n",
    "- Para **Python (PySpark)**:\n",
    "  1. Instalar con `pip`: `pip3 install pyspark` (añade JARs y dependencias necesarias. Quizá necesario iniciar un entorno virtual)\n",
    "  2. (Opcional) Añadir `~/.local/bin` al `PATH` si usas instalación de usuario\n",
    "  3. (Opcional en clúster) Si accedes a HDFS/YARN: `export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop`\n",
    "\n",
    "## Ejecución de Spark\n",
    "1. Consolas interactivas\n",
    "    - Scala: `spark-shell`\n",
    "    - Python: `pyspark`\n",
    "        - Con IPython: `PYSPARK_DRIVER_PYTHON=ipython pyspark`\n",
    "        - Con Jupyter: `PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\" pyspark`\n",
    "    - R: `sparkR`\n",
    "    - SQL: `spark-sql`\n",
    "    - [Apache Zeppelin](https://zeppelin.apache.org/)\n",
    "2. Script con `spark-submit`\n",
    "    - Ej.: `spark-submit --master local[4] --name TCDM-Intro app.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "id": "ffS6avcC9BaQ"
   },
   "outputs": [],
   "source": [
    "# Ejemplo: muestra la versión de PySpark\n",
    "try:\n",
    "    print(f\"Versión de PySpark {spark.version}.\")\n",
    "except NameError:\n",
    "    print(\"SparkSession no inicializada; ejecuta la celda de creación de spark.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cerrar la sesión de Spark al finalizar\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"SparkSession detenida.\")\n",
    "except Exception as e:\n",
    "    print(\"No se pudo detener SparkSession:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3E-wz2xF9BaS"
   },
   "source": [
    "## Documentación\n",
    "La documentación oficial sobre Apache Spark está en https://spark.apache.org/docs/latest/\n",
    "\n",
    "APIs por lenguaje:\n",
    "\n",
    "  - Python: https://spark.apache.org/docs/latest/api/python/\n",
    "  - Scala: https://spark.apache.org/docs/latest/api/scala/\n",
    "  - Java: https://spark.apache.org/docs/latest/api/java/\n",
    "\n",
    "Guías útiles:\n",
    "- SQL programming guide: https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "- Funciones SQL (Python): https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html\n",
    "- Pandas API on Spark (opcional): https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
