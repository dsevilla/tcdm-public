{
  "paragraphs": [
    {
      "text": "%md\nIntroducción a [Apache Spark](http://spark.apache.org/)\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d",
      "user": "anonymous",
      "dateUpdated": "2021-10-11 14:16:35.057",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12.0,
        "editorHide": true,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eIntroducción a \u003ca href\u003d\"http://spark.apache.org/\"\u003eApache Spark\u003c/a\u003e\u003c/h1\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1508243612734_42507456",
      "id": "20170329-135315_716642539",
      "dateCreated": "2017-10-17 12:33:32.000",
      "dateStarted": "2021-10-11 14:16:35.062",
      "dateFinished": "2021-10-11 14:16:35.069",
      "status": "FINISHED"
    },
    {
      "text": "%md\n### Plataforma de computación cluster rápida\n\n-   Extiende modelo MapReduce soportando de manera eficiente otros tipos\n    de computación\n\n    -   queries interactivas\n\n    -   procesado streaming\n\n-   Soporta computaciones en memoria\n\n-   Mejora a MapReduce para aplicaciones complejas (10-20x más rápido)\n\n#### Propósito general\n\n-   Modos de funcionamiento batch, interactivo o streaming\n\n-   Reduce el número de herramientas a emplear y mantener",
      "user": "anonymous",
      "dateUpdated": "2021-10-11 14:16:35.161",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 6.0,
        "editorHide": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 346.0,
              "optionOpen": false
            }
          }
        },
        "enabled": true,
        "fontSize": 9.0,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003ePlataforma de computación cluster rápida\u003c/h3\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003eExtiende modelo MapReduce soportando de manera eficiente otros tipos\u003cbr/\u003ede computación\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\n      \u003cp\u003equeries interactivas\u003c/p\u003e\u003c/li\u003e\n      \u003cli\u003e\n      \u003cp\u003eprocesado streaming\u003c/p\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eSoporta computaciones en memoria\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eMejora a MapReduce para aplicaciones complejas (10-20x más rápido)\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003ePropósito general\u003c/h4\u003e\n\u003cul\u003e\n  \u003cli\u003e\n  \u003cp\u003eModos de funcionamiento batch, interactivo o streaming\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eReduce el número de herramientas a emplear y mantener\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1508243612752_34042980",
      "id": "20170329-135340_1848752981",
      "dateCreated": "2017-10-17 12:33:32.000",
      "dateStarted": "2021-10-11 14:16:35.165",
      "dateFinished": "2021-10-11 14:16:35.178",
      "status": "FINISHED"
    },
    {
      "text": "%md\n#### Historia\n\n-   Iniciado en el 2009 en el UC Berkeley RAD Lab (AMPLab)\n\n    -   Motivado por la ineficiencia de MapReduce para trabajos\n        iterativos e interactivos\n\n-   Mayores contribuidores: [Databricks](https://databricks.com/),\n    Yahoo! e Intel\n\n-   Declarado open source en marzo del 2010\n\n-   Transferido a la Apache Software Foundation en junio de 2013, TLP en\n    febrero de 2014\n\n-   Uno de los proyectos Big Data más activos\n\n-   Versión 1.0 lanzada en mayo de 2014",
      "user": "anonymous",
      "dateUpdated": "2021-10-11 14:16:35.265",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 6.0,
        "editorHide": true,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch4\u003eHistoria\u003c/h4\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003eIniciado en el 2009 en el UC Berkeley RAD Lab (AMPLab)\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\n      \u003cp\u003eMotivado por la ineficiencia de MapReduce para trabajos\u003cbr/\u003eiterativos e interactivos\u003c/p\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eMayores contribuidores: \u003ca href\u003d\"https://databricks.com/\"\u003eDatabricks\u003c/a\u003e,\u003cbr/\u003eYahoo! e Intel\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eDeclarado open source en marzo del 2010\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eTransferido a la Apache Software Foundation en junio de 2013, TLP en\u003cbr/\u003efebrero de 2014\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eUno de los proyectos Big Data más activos\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eVersión 1.0 lanzada en mayo de 2014\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1508243612752_34042980",
      "id": "20170329-135351_1564714588",
      "dateCreated": "2017-10-17 12:33:32.000",
      "dateStarted": "2021-10-11 14:16:35.269",
      "dateFinished": "2021-10-11 14:16:35.283",
      "status": "FINISHED"
    },
    {
      "text": "%md\n#### Características de Spark\n\n-   Soporta gran variedad de workloads: batch, queries interactivas,\n    streaming, machine learning, procesado de grafos\n\n-   APIs en Scala, Java, Python, SQL y R\n\n-   Shells interactivos en Scala, Python, SQL y R\n\n-   Se integra con otras soluciones BigData: HDFS, Cassandra, etc.",
      "user": "anonymous",
      "dateUpdated": "2021-10-11 14:16:35.368",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "editorHide": true,
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "fontSize": 9.0,
        "tableHide": false,
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch4\u003eCaracterísticas de Spark\u003c/h4\u003e\n\u003cul\u003e\n  \u003cli\u003e\n  \u003cp\u003eSoporta gran variedad de workloads: batch, queries interactivas,\u003cbr/\u003estreaming, machine learning, procesado de grafos\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eAPIs en Scala, Java, Python, SQL y R\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eShells interactivos en Scala, Python, SQL y R\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eSe integra con otras soluciones BigData: HDFS, Cassandra, etc.\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1508243612753_33658231",
      "id": "20170329-135406_1457797731",
      "dateCreated": "2017-10-17 12:33:32.000",
      "dateStarted": "2021-10-11 14:16:35.373",
      "dateFinished": "2021-10-11 14:16:35.383",
      "status": "FINISHED"
    },
    {
      "text": "%md\n### La pila Spark\n\u003chr /\u003e\n![sparkstack](http://persoal.citius.usc.es/tf.pena/TCDM/figs/sparkstack.png)\n(Fuente: H. Karau, A. Konwinski, P. Wendell, M. Zaharia, \"Learning Spark\", O\u0027Reilly, 2015)\n\u003chr /\u003e",
      "user": "anonymous",
      "dateUpdated": "2021-10-11 14:16:35.472",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12.0,
        "editorHide": true,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eLa pila Spark\u003c/h3\u003e\n\u003chr /\u003e\n\u003cp\u003e\u003cimg src\u003d\"http://persoal.citius.usc.es/tf.pena/TCDM/figs/sparkstack.png\" alt\u003d\"sparkstack\" /\u003e\u003cbr/\u003e(Fuente: H. Karau, A. Konwinski, P. Wendell, M. Zaharia, \u0026ldquo;Learning Spark\u0026rdquo;, O\u0026rsquo;Reilly, 2015)\u003cbr/\u003e\u003chr /\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1508243612753_33658231",
      "id": "20170329-135424_1926415155",
      "dateCreated": "2017-10-17 12:33:32.000",
      "dateStarted": "2021-10-11 14:16:35.476",
      "dateFinished": "2021-10-11 14:16:35.485",
      "status": "FINISHED"
    },
    {
      "text": "%md\n## APIs del Spark Core\nSpark ofrece dos APIs:\n\n - API estructurada o de alto nivel\n - API de bajo nivel\n\nCada API ofrece diferentes tipos de datos:\n\n - Se recomienda usar la API estructurada por su mayor rendimiento\n - La API de bajo nivel permite un mayor control sobre la distribución de los datos\n - La API de alto nivel utiliza las primitivas de bajo nivel\n\n## Tipos de datos en la API estructurada\n\n### Datasets\nColección distribuida de objetos del mismo tipo\n\n- Introducida en Spark \u003e 1.6\n- El API para Datasets sólo está disponible en Scala y Java \n- No está disponible en Python ni R debido al tipado dinámico de estos lenguages\n\n### DataFrames\nUn DataFrame es un DataSet organizado en columnas con nombre\n\n- Conceptualmente equivalente a una tabla en una base de datos relacional o un dataframe en Python Pandas o R\n- El API para DataFrames está disponible en Scala, Java, Python y R\n- En [Java](http://spark.apache.org/docs/latest/api/java/index.html \"Interface Row\") y [Scala](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row \"trait Row extends Serializable\"), un DataFrame es un DataSet de objetos de tipo *Row*\n\n\n## Tipos de datos en la API de bajo nivel\n### RDDs (Resilient Distributed Datasets)\n\nLista distribuida de objetos\n- Tipos de datos básico de Spark v1.X\n\n\n## Mejor rendimiento de la API estructurada\n\n- Spark con DataFrames y DataSets se aprovecha del uso de datos con estructura para optimizar el rendimiento utilizando el optimizador de consultas [Catalyst](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html \"Deep Dive into Spark SQL’s Catalyst Optimizer\")  y el motor de ejecución [Tungsten](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html \"Project Tungsten: Bringing Apache Spark Closer to Bare Metal\").\n\n\u003cimg src\u003d\"https://databricks.com/wp-content/uploads/2015/02/Screen-Shot-2015-02-16-at-9.46.39-AM.png\" alt\u003d\"Mejora de rendimiento\" style\u003d\"width: 650px;\"/\u003e\n\nFuente: [Recent performance improvements in Apache Spark: SQL, Python, DataFrames, and More](https://databricks.com/blog/2015/04/24/recent-performance-improvements-in-apache-spark-sql-python-dataframes-and-more.html \"Recent performance improvements in Apache Spark: SQL, Python, DataFrames, and More\")\n ",
      "user": "anonymous",
      "dateUpdated": "2021-10-11 14:16:35.576",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/markdown",
        "colWidth": 12.0,
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorHide": true,
        "tableHide": false,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eAPIs del Spark Core\u003c/h2\u003e\n\u003cp\u003eSpark ofrece dos APIs:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eAPI estructurada o de alto nivel\u003c/li\u003e\n  \u003cli\u003eAPI de bajo nivel\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCada API ofrece diferentes tipos de datos:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eSe recomienda usar la API estructurada por su mayor rendimiento\u003c/li\u003e\n  \u003cli\u003eLa API de bajo nivel permite un mayor control sobre la distribución de los datos\u003c/li\u003e\n  \u003cli\u003eLa API de alto nivel utiliza las primitivas de bajo nivel\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eTipos de datos en la API estructurada\u003c/h2\u003e\n\u003ch3\u003eDatasets\u003c/h3\u003e\n\u003cp\u003eColección distribuida de objetos del mismo tipo\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eIntroducida en Spark \u0026gt; 1.6\u003c/li\u003e\n  \u003cli\u003eEl API para Datasets sólo está disponible en Scala y Java\u003c/li\u003e\n  \u003cli\u003eNo está disponible en Python ni R debido al tipado dinámico de estos lenguages\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eDataFrames\u003c/h3\u003e\n\u003cp\u003eUn DataFrame es un DataSet organizado en columnas con nombre\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eConceptualmente equivalente a una tabla en una base de datos relacional o un dataframe en Python Pandas o R\u003c/li\u003e\n  \u003cli\u003eEl API para DataFrames está disponible en Scala, Java, Python y R\u003c/li\u003e\n  \u003cli\u003eEn \u003ca href\u003d\"http://spark.apache.org/docs/latest/api/java/index.html\" title\u003d\"Interface Row\"\u003eJava\u003c/a\u003e y \u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row\" title\u003d\"trait Row extends Serializable\"\u003eScala\u003c/a\u003e, un DataFrame es un DataSet de objetos de tipo \u003cem\u003eRow\u003c/em\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eTipos de datos en la API de bajo nivel\u003c/h2\u003e\n\u003ch3\u003eRDDs (Resilient Distributed Datasets)\u003c/h3\u003e\n\u003cp\u003eLista distribuida de objetos\u003cbr/\u003e- Tipos de datos básico de Spark v1.X\u003c/p\u003e\n\u003ch2\u003eMejor rendimiento de la API estructurada\u003c/h2\u003e\n\u003cul\u003e\n  \u003cli\u003eSpark con DataFrames y DataSets se aprovecha del uso de datos con estructura para optimizar el rendimiento utilizando el optimizador de consultas \u003ca href\u003d\"https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\" title\u003d\"Deep Dive into Spark SQL’s Catalyst Optimizer\"\u003eCatalyst\u003c/a\u003e y el motor de ejecución \u003ca href\u003d\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\" title\u003d\"Project Tungsten: Bringing Apache Spark Closer to Bare Metal\"\u003eTungsten\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cimg src\u003d\"https://databricks.com/wp-content/uploads/2015/02/Screen-Shot-2015-02-16-at-9.46.39-AM.png\" alt\u003d\"Mejora de rendimiento\" style\u003d\"width: 650px;\"/\u003e\n\u003cp\u003eFuente: \u003ca href\u003d\"https://databricks.com/blog/2015/04/24/recent-performance-improvements-in-apache-spark-sql-python-dataframes-and-more.html\" title\u003d\"Recent performance improvements in Apache Spark: SQL, Python, DataFrames, and More\"\u003eRecent performance improvements in Apache Spark: SQL, Python, DataFrames, and More\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1508243612755_34427729",
      "id": "20170329-142126_1062577642",
      "dateCreated": "2017-10-17 12:33:32.000",
      "dateStarted": "2021-10-11 14:16:35.581",
      "dateFinished": "2021-10-11 14:16:35.617",
      "status": "FINISHED"
    },
    {
      "text": "%md\n## Conceptos clave\n\u003chr /\u003e\n![sparkcontext](http://persoal.citius.usc.es/tf.pena/TCDM/figs/sparkcontext.png)\n(Fuente: H. Karau, A. Konwinski, P. Wendell, M. Zaharia, \"Learning Spark\", O\u0027Reilly, 2015)\n\u003chr /\u003e",
      "user": "anonymous",
      "dateUpdated": "2021-10-11 14:16:35.680",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12.0,
        "editorHide": true,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eConceptos clave\u003c/h2\u003e\n\u003chr /\u003e\n\u003cp\u003e\u003cimg src\u003d\"http://persoal.citius.usc.es/tf.pena/TCDM/figs/sparkcontext.png\" alt\u003d\"sparkcontext\" /\u003e\u003cbr/\u003e(Fuente: H. Karau, A. Konwinski, P. Wendell, M. Zaharia, \u0026ldquo;Learning Spark\u0026rdquo;, O\u0026rsquo;Reilly, 2015)\u003cbr/\u003e\u003chr /\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1508243612754_34812478",
      "id": "20170329-140013_258148990",
      "dateCreated": "2017-10-17 12:33:32.000",
      "dateStarted": "2021-10-11 14:16:35.685",
      "dateFinished": "2021-10-11 14:16:35.693",
      "status": "FINISHED"
    },
    {
      "text": "%md\n#### Driver\n\n-   Crea un `SparkContext`\n\n-   Convierte el programa de usuario en tareas:\n\n    -   `DAG` de operaciones lógico -\u003e plan de ejecución físico\n\n-   Planifica las tareas en los ejecutores\n\n#### SparkSession y SparkContext\n\n-   SparkSession: punto de entrada de todas las funcionalidades de Spark\n\n    -   Permite especificar la configuración de la aplicación Spark\n    -   En el notebook (o el shell de Spark), se define automáticamente (variable `spark`)\n\n-   SparkContext: realiza la conexión con el cluster\n\n    -   Se crea a partir del SparkSession\n    -   Punto de entrada para la API de bajo nivel\n    -   En el notebook (o el shell de Spark), se define automáticamente (variable `sc`)\n\n-   Creación en un script Python\n```python\nfrom pyspark.sql import SparkSession\n# Creamos un objeto SparkSession (o lo obtenemos si ya está creado)\nspark \u003d SparkSession \\\n.builder \\\n.appName(\"Mi aplicacion\") \\\n.config(\"spark.alguna.opcion.de.configuracion\", \"algun-valor\") \\\n.master(\"local[4]\") \\\n.getOrCreate()\n# Obtenemos el SparkContext\nsc \u003d spark.sparkContext\n```",
      "user": "anonymous",
      "dateUpdated": "2021-10-11 14:16:35.784",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 6.0,
        "editorHide": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 792.067,
              "optionOpen": false
            }
          }
        },
        "enabled": true,
        "tableHide": false,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch4\u003eDriver\u003c/h4\u003e\n\u003cul\u003e\n  \u003cli\u003e\n  \u003cp\u003eCrea un \u003ccode\u003eSparkContext\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eConvierte el programa de usuario en tareas:\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\n      \u003cp\u003e\u003ccode\u003eDAG\u003c/code\u003e de operaciones lógico -\u0026gt; plan de ejecución físico\u003c/p\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003ePlanifica las tareas en los ejecutores\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eSparkSession y SparkContext\u003c/h4\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003eSparkSession: punto de entrada de todas las funcionalidades de Spark\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003ePermite especificar la configuración de la aplicación Spark\u003c/li\u003e\n      \u003cli\u003eEn el notebook (o el shell de Spark), se define automáticamente (variable \u003ccode\u003espark\u003c/code\u003e)\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eSparkContext: realiza la conexión con el cluster\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eSe crea a partir del SparkSession\u003c/li\u003e\n      \u003cli\u003ePunto de entrada para la API de bajo nivel\u003c/li\u003e\n      \u003cli\u003eEn el notebook (o el shell de Spark), se define automáticamente (variable \u003ccode\u003esc\u003c/code\u003e)\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eCreación en un script Python\u003c/p\u003e\n  \u003cpre\u003e\u003ccode class\u003d\"python\"\u003efrom pyspark.sql import SparkSession\n# Creamos un objeto SparkSession (o lo obtenemos si ya está creado)\nspark \u003d SparkSession \\\n.builder \\\n.appName(\u0026quot;Mi aplicacion\u0026quot;) \\\n.config(\u0026quot;spark.alguna.opcion.de.configuracion\u0026quot;, \u0026quot;algun-valor\u0026quot;) \\\n.master(\u0026quot;local[4]\u0026quot;) \\\n.getOrCreate()\n# Obtenemos el SparkContext\nsc \u003d spark.sparkContext\n\u003c/code\u003e\u003c/pre\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1508243612754_34812478",
      "id": "20170329-141758_1410852644",
      "dateCreated": "2017-10-17 12:33:32.000",
      "dateStarted": "2021-10-11 14:16:35.789",
      "dateFinished": "2021-10-11 14:16:35.810",
      "status": "FINISHED"
    },
    {
      "text": "%md\n-   Creación en un programa Scala\n   \n        import org.apache.spark.sql.SparkSession\n        // Creamos un objeto SparkSession (o lo obtenemos si ya está creado)\n        val spark \u003d SparkSession\n           .builder()\n           .appName(\"Mi aplicacion\")\n           .config(\"spark.alguna.opcion.de.configuracion\", \"algun-valor\")\n           .master(\"local[4]\")\n           .getOrCreate()\n\n        // Obtenemos el SparkContext\n        val sc \u003d spark.sparkContext        \n        \n        \n#### Executors\n\n-   Ejecutan las tareas individuales y devuelven los resultados al\n    Driver\n\n-   Proporcionan almacenamiento en memoria para los datos de las tareas\n\n#### Cluster Manager\n\n-   Componente *enchufable* en Spark\n\n-   YARN, Mesos o Spark Standalone",
      "user": "anonymous",
      "dateUpdated": "2021-10-11 14:16:35.889",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 6.0,
        "editorHide": true,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cul\u003e\n  \u003cli\u003eCreación en un programa Scala\n    \u003cpre\u003e\u003ccode\u003eimport org.apache.spark.sql.SparkSession\n// Creamos un objeto SparkSession (o lo obtenemos si ya está creado)\nval spark \u003d SparkSession\n   .builder()\n   .appName(\u0026quot;Mi aplicacion\u0026quot;)\n   .config(\u0026quot;spark.alguna.opcion.de.configuracion\u0026quot;, \u0026quot;algun-valor\u0026quot;)\n   .master(\u0026quot;local[4]\u0026quot;)\n   .getOrCreate()\n\n// Obtenemos el SparkContext\nval sc \u003d spark.sparkContext        \n\u003c/code\u003e\u003c/pre\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eExecutors\u003c/h4\u003e\n\u003cul\u003e\n  \u003cli\u003e\n  \u003cp\u003eEjecutan las tareas individuales y devuelven los resultados al\u003cbr/\u003eDriver\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eProporcionan almacenamiento en memoria para los datos de las tareas\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eCluster Manager\u003c/h4\u003e\n\u003cul\u003e\n  \u003cli\u003e\n  \u003cp\u003eComponente \u003cem\u003eenchufable\u003c/em\u003e en Spark\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eYARN, Mesos o Spark Standalone\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1508243612754_34812478",
      "id": "20170329-142050_1602884490",
      "dateCreated": "2017-10-17 12:33:32.000",
      "dateStarted": "2021-10-11 14:16:35.895",
      "dateFinished": "2021-10-11 14:16:35.906",
      "status": "FINISHED"
    },
    {
      "text": "%md\n## Instalación de Spark\n1. Descargar Apache Spark de http://spark.apache.org/downloads.html\n    - La versión  \"Pre-built for Hadoop 2.x and later\" incorpora Hadoop\n    - También se puede descargar un versión sin Hadoop para usar una instalación de Hadoop ya disponible\n    - Alternativamente, es posible construir Spark desde el código fuentre\n2. Extraer el fichero descargado\n\n## Ejecución de Spark\n1. Usando consolas interactivas\n    - Scala: `spark-shell`\n    - Python: `pyspark`\n        - Python con [IPython](https://ipython.org/): `PYSPARK_DRIVER_PYTHON\u003dipython pyspark`\n        - Python con [Jupyter](https://jupyter.org/): `PYSPARK_DRIVER_PYTHON\u003djupyter PYSPARK_DRIVER_PYTHON_OPTS\u003d\"notebook\" pyspark`\n    - R: `sparkR`\n    - SQL: `spark-sql`\n    - Usando [Apache Zeppelin](https://zeppelin.apache.org/)\n2. Lanzando un script con `spark-submit`\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-11 14:16:35.994",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eInstalación de Spark\u003c/h2\u003e\n\u003col\u003e\n  \u003cli\u003eDescargar Apache Spark de \u003ca href\u003d\"http://spark.apache.org/downloads.html\"\u003ehttp://spark.apache.org/downloads.html\u003c/a\u003e\n    \u003cul\u003e\n      \u003cli\u003eLa versión \u0026ldquo;Pre-built for Hadoop 2.x and later\u0026rdquo; incorpora Hadoop\u003c/li\u003e\n      \u003cli\u003eTambién se puede descargar un versión sin Hadoop para usar una instalación de Hadoop ya disponible\u003c/li\u003e\n      \u003cli\u003eAlternativamente, es posible construir Spark desde el código fuentre\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003eExtraer el fichero descargado\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eEjecución de Spark\u003c/h2\u003e\n\u003col\u003e\n  \u003cli\u003eUsando consolas interactivas\n    \u003cul\u003e\n      \u003cli\u003eScala: \u003ccode\u003espark-shell\u003c/code\u003e\u003c/li\u003e\n      \u003cli\u003ePython: \u003ccode\u003epyspark\u003c/code\u003e\n        \u003cul\u003e\n          \u003cli\u003ePython con \u003ca href\u003d\"https://ipython.org/\"\u003eIPython\u003c/a\u003e: \u003ccode\u003ePYSPARK_DRIVER_PYTHON\u003dipython pyspark\u003c/code\u003e\u003c/li\u003e\n          \u003cli\u003ePython con \u003ca href\u003d\"https://jupyter.org/\"\u003eJupyter\u003c/a\u003e: \u003ccode\u003ePYSPARK_DRIVER_PYTHON\u003djupyter PYSPARK_DRIVER_PYTHON_OPTS\u003d\u0026quot;notebook\u0026quot; pyspark\u003c/code\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003eR: \u003ccode\u003esparkR\u003c/code\u003e\u003c/li\u003e\n      \u003cli\u003eSQL: \u003ccode\u003espark-sql\u003c/code\u003e\u003c/li\u003e\n      \u003cli\u003eUsando \u003ca href\u003d\"https://zeppelin.apache.org/\"\u003eApache Zeppelin\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003eLanzando un script con \u003ccode\u003espark-submit\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1530610701732_927810385",
      "id": "20180703-093821_1048639015",
      "dateCreated": "2018-07-03 09:38:21.000",
      "dateStarted": "2021-10-11 14:16:35.999",
      "dateFinished": "2021-10-11 14:16:36.018",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\n# Ejemplo: muestra la versión de PySpark\nprint(\"Versión de PySpark {0}\".format(spark.version))\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-11 14:16:36.099",
      "progress": 0,
      "config": {
        "colWidth": 6.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Versión de PySpark 2.4.5\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1541422833060_-608641500",
      "id": "20181105-130033_329612487",
      "dateCreated": "2018-11-05 13:00:33.000",
      "dateStarted": "2021-10-11 14:16:36.103",
      "dateFinished": "2021-10-11 14:16:36.329",
      "status": "FINISHED"
    },
    {
      "text": "%spark\n// Ejemplo: muestra la versión de Spark en Scala\nprintf(\"Versión de Spark %s\", spark.version)\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-11 14:16:36.403",
      "progress": 0,
      "config": {
        "colWidth": 6.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Versión de Spark 2.4.5"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1541422900183_-860075699",
      "id": "20181105-130140_1985422084",
      "dateCreated": "2018-11-05 13:01:40.000",
      "dateStarted": "2021-10-11 14:16:36.407",
      "dateFinished": "2021-10-11 14:16:36.732",
      "status": "FINISHED"
    },
    {
      "text": "%md\n## Documentación\nLa documentación oficial sobre Apache Spark esté en https://spark.apache.org/docs/latest/\n\nLa documentación de las APIS para los distintos lenguajes está en https://spark.apache.org/docs/latest/api.html\n\n  - Python: https://spark.apache.org/docs/latest/api/python/\n  - Scala: https://spark.apache.org/docs/latest/api/scala/\n  - Java: https://spark.apache.org/docs/latest/api/java/",
      "user": "anonymous",
      "dateUpdated": "2021-10-11 14:16:51.850",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eDocumentación\u003c/h2\u003e\n\u003cp\u003eLa documentación oficial sobre Apache Spark esté en \u003ca href\u003d\"https://spark.apache.org/docs/latest/\"\u003ehttps://spark.apache.org/docs/latest/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eLa documentación de las APIS para los distintos lenguajes está en \u003ca href\u003d\"https://spark.apache.org/docs/latest/api.html\"\u003ehttps://spark.apache.org/docs/latest/api.html\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003ePython: \u003ca href\u003d\"https://spark.apache.org/docs/latest/api/python/\"\u003ehttps://spark.apache.org/docs/latest/api/python/\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003eScala: \u003ca href\u003d\"https://spark.apache.org/docs/latest/api/scala/\"\u003ehttps://spark.apache.org/docs/latest/api/scala/\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003eJava: \u003ca href\u003d\"https://spark.apache.org/docs/latest/api/java/\"\u003ehttps://spark.apache.org/docs/latest/api/java/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1538561245115_723082757",
      "id": "20181003-100725_1619931971",
      "dateCreated": "2018-10-03 10:07:25.000",
      "dateStarted": "2021-10-11 14:16:51.851",
      "dateFinished": "2021-10-11 14:16:51.862",
      "status": "FINISHED"
    }
  ],
  "name": "3.1. Introducción a Apache Spark",
  "id": "2CX6XYA24",
  "defaultInterpreterGroup": "spark",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {
    "isRunning": false
  }
}