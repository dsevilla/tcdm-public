{
  "paragraphs": [
    {
      "text": "%md \n## Ejecución de un programa Spark en un cluster\n\n#### Comando `spark-submit`\n\n-   Permite lanzar programas Spark a un cluster\n\n-   Ejemplo:\n```sh\n$ spark-submit --master yarn --deploy-mode cluster \\  \n     --py-files otralib.zip,otrofich.py \\  \n     --num-executors 10 --executor-cores 2 \\  \n     mi-script.py opciones_del_script\n```\n\n#### Opciones de `spark-submit`\n\n-   `master`: cluster manager a usar (opciones: `yarn`, `mesos://host:port`, `spark://host:port`, `local[n]`)\n\n-   `deploy-mode`: dos modos de despliegue\n\n    -   `client`: ejecuta el driver en el nodo local\n\n    -   `cluster`: ejecuta el driver en un nodo del cluster\n\n-   `class`: clase a ejecutar (Java o Scala)\n\n-   `name`: nombre de la aplicación (se muestra en el Spark web)\n\n-   `jars`: ficheros jar a añadir al classpath (Java o Scala)\n\n-   `py-files`: archivos a añadir al PYTHONPATH (`.py`,`.zip`,`.egg`)\n\n-   `files`: ficheros de datos para la aplicación\n\n-   `executor-memory`: memoria total de cada ejecutor\n\n-   `driver-memory`: memoria del proceso driver\n\nPara más opciones: `spark-submit --help`",
      "user": "anonymous",
      "dateUpdated": "2020-11-18 15:37:48.778",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": true,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eEjecución de un programa Spark en un cluster\u003c/h2\u003e\n\u003ch4\u003eComando \u003ccode\u003espark-submit\u003c/code\u003e\u003c/h4\u003e\n\u003cul\u003e\n  \u003cli\u003e\n  \u003cp\u003ePermite lanzar programas Spark a un cluster\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eEjemplo:\u003c/p\u003e\n  \u003cpre\u003e\u003ccode class\u003d\"sh\"\u003e$ spark-submit --master yarn --deploy-mode cluster \\  \n --py-files otralib.zip,otrofich.py \\  \n --num-executors 10 --executor-cores 2 \\  \n mi-script.py opciones_del_script\n\u003c/code\u003e\u003c/pre\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eOpciones de \u003ccode\u003espark-submit\u003c/code\u003e\u003c/h4\u003e\n\u003cul\u003e\n  \u003cli\u003e\n  \u003cp\u003e\u003ccode\u003emaster\u003c/code\u003e: cluster manager a usar (opciones: \u003ccode\u003eyarn\u003c/code\u003e, \u003ccode\u003emesos://host:port\u003c/code\u003e, \u003ccode\u003espark://host:port\u003c/code\u003e, \u003ccode\u003elocal[n]\u003c/code\u003e)\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003ccode\u003edeploy-mode\u003c/code\u003e: dos modos de despliegue\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\n      \u003cp\u003e\u003ccode\u003eclient\u003c/code\u003e: ejecuta el driver en el nodo local\u003c/p\u003e\u003c/li\u003e\n      \u003cli\u003e\n      \u003cp\u003e\u003ccode\u003ecluster\u003c/code\u003e: ejecuta el driver en un nodo del cluster\u003c/p\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003e\u003ccode\u003eclass\u003c/code\u003e: clase a ejecutar (Java o Scala)\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003e\u003ccode\u003ename\u003c/code\u003e: nombre de la aplicación (se muestra en el Spark web)\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003e\u003ccode\u003ejars\u003c/code\u003e: ficheros jar a añadir al classpath (Java o Scala)\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003e\u003ccode\u003epy-files\u003c/code\u003e: archivos a añadir al PYTHONPATH (\u003ccode\u003e.py\u003c/code\u003e,\u003ccode\u003e.zip\u003c/code\u003e,\u003ccode\u003e.egg\u003c/code\u003e)\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003e\u003ccode\u003efiles\u003c/code\u003e: ficheros de datos para la aplicación\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003e\u003ccode\u003eexecutor-memory\u003c/code\u003e: memoria total de cada ejecutor\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003e\u003ccode\u003edriver-memory\u003c/code\u003e: memoria del proceso driver\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePara más opciones: \u003ccode\u003espark-submit --help\u003c/code\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1508256444935_1393174409",
      "id": "20170331-180337_378748956",
      "dateCreated": "2017-10-17 16:07:24.000",
      "dateStarted": "2020-11-18 15:37:48.779",
      "dateFinished": "2020-11-18 15:37:48.799",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\n/usr/local/spark/bin/spark-submit --help",
      "user": "anonymous",
      "dateUpdated": "2020-11-18 15:40:23.219",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/sh",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionSupport": false
        },
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Usage: spark-submit [options] \u003capp jar | python file\u003e [app arguments]\nUsage: spark-submit --kill [submission ID] --master [spark://...]\nUsage: spark-submit --status [submission ID] --master [spark://...]\nUsage: spark-submit run-example [options] example-class [example args]\n\nOptions:\n  --master MASTER_URL         spark://host:port, mesos://host:port, yarn, or local.\n  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\n                              on one of the worker machines inside the cluster (\"cluster\")\n                              (Default: client).\n  --class CLASS_NAME          Your application\u0027s main class (for Java / Scala apps).\n  --name NAME                 A name of your application.\n  --jars JARS                 Comma-separated list of local jars to include on the driver\n                              and executor classpaths.\n  --packages                  Comma-separated list of maven coordinates of jars to include\n                              on the driver and executor classpaths. Will search the local\n                              maven repo, then maven central and any additional remote\n                              repositories given by --repositories. The format for the\n                              coordinates should be groupId:artifactId:version.\n  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while\n                              resolving the dependencies provided in --packages to avoid\n                              dependency conflicts.\n  --repositories              Comma-separated list of additional remote repositories to\n                              search for the maven coordinates given with --packages.\n  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place\n                              on the PYTHONPATH for Python apps.\n  --files FILES               Comma-separated list of files to be placed in the working\n                              directory of each executor. File paths of these files\n                              in executors can be accessed via SparkFiles.get(fileName).\n\n  --conf PROP\u003dVALUE           Arbitrary Spark configuration property.\n  --properties-file FILE      Path to a file from which to load extra properties. If not\n                              specified, this will look for conf/spark-defaults.conf.\n\n  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n  --driver-java-options       Extra Java options to pass to the driver.\n  --driver-library-path       Extra library path entries to pass to the driver.\n  --driver-class-path         Extra class path entries to pass to the driver. Note that\n                              jars added with --jars are automatically included in the\n                              classpath.\n\n  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n\n  --proxy-user NAME           User to impersonate when submitting the application.\n                              This argument does not work with --principal / --keytab.\n\n  --help, -h                  Show this help message and exit.\n  --verbose, -v               Print additional debug output.\n  --version,                  Print the version of current Spark.\n\n Spark standalone with cluster deploy mode only:\n  --driver-cores NUM          Cores for driver (Default: 1).\n\n Spark standalone or Mesos with cluster deploy mode only:\n  --supervise                 If given, restarts the driver on failure.\n  --kill SUBMISSION_ID        If given, kills the driver specified.\n  --status SUBMISSION_ID      If given, requests the status of the driver specified.\n\n Spark standalone and Mesos only:\n  --total-executor-cores NUM  Total cores for all executors.\n\n Spark standalone and YARN only:\n  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,\n                              or all available cores on the worker in standalone mode)\n\n YARN-only:\n  --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n                              (Default: 1).\n  --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n  --num-executors NUM         Number of executors to launch (Default: 2).\n                              If dynamic allocation is enabled, the initial number of\n                              executors will be at least NUM.\n  --archives ARCHIVES         Comma separated list of archives to be extracted into the\n                              working directory of each executor.\n  --principal PRINCIPAL       Principal to be used to login to KDC, while running on\n                              secure HDFS.\n  --keytab KEYTAB             The full path to the file that contains the keytab for the\n                              principal specified above. This keytab will be copied to\n                              the node running the Application Master via the Secure\n                              Distributed Cache, for renewing the login tickets and the\n                              delegation tokens periodically.\n      \n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1508256444946_1401254136",
      "id": "20170719-201624_1770395113",
      "dateCreated": "2017-10-17 16:07:24.000",
      "dateStarted": "2020-11-18 15:40:23.231",
      "dateFinished": "2020-11-18 15:40:23.805",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\u003chr /\u003e\n![Modo cliente](http://persoal.citius.usc.es/tf.pena/TCDM/figs/client-mode.jpeg \"Modo cliente en YARN\")\n\n![Modo cluster](http://persoal.citius.usc.es/tf.pena/TCDM/figs/cluster-mode.jpeg \"Modo cluster en YARN\")\n\nFuente: [Spark-on-YARN: Empower Spark Applications on Hadoop Cluster](https://www.slideshare.net/Hadoop_Summit/sparkonyarn-empower-spark-applications-on-hadoop-cluster)\n\u003chr /\u003e",
      "user": "anonymous",
      "dateUpdated": "2017-10-17 16:22:45.000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003chr /\u003e\n\u003cp\u003e\u003cimg src\u003d\"http://persoal.citius.usc.es/tf.pena/TCDM/figs/client-mode.jpeg\" alt\u003d\"Modo cliente\" title\u003d\"Modo cliente en YARN\" /\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src\u003d\"http://persoal.citius.usc.es/tf.pena/TCDM/figs/cluster-mode.jpeg\" alt\u003d\"Modo cluster\" title\u003d\"Modo cluster en YARN\" /\u003e\u003c/p\u003e\n\u003cp\u003eFuente: \u003ca href\u003d\"https://www.slideshare.net/Hadoop_Summit/sparkonyarn-empower-spark-applications-on-hadoop-cluster\"\u003eSpark-on-YARN: Empower Spark Applications on Hadoop Cluster\u003c/a\u003e\u003cbr/\u003e\u003chr /\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1508256444947_1400869387",
      "id": "20170722-114953_1562581291",
      "dateCreated": "2017-10-17 16:07:24.000",
      "dateStarted": "2017-10-17 16:22:42.000",
      "dateFinished": "2017-10-17 16:22:42.000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Parámetros de configuración\n\nDiversos parámetros ajustables en tiempo de ejecución\n\n-   En el script\n```python\nfrom pyspark.sql import SparkSession\n# Creamos un objeto SparkSession (o lo obtenemos si ya está creado)\nspark \u003d SparkSession \\\n.builder \\\n.appName(\"Mi aplicacion\") \\\n.config(\"spark.ui.port\", \"3600\")\\    # Puerto del interfaz web de Spark (por defecto: 4040)\n.master(\"local[2]\") \\ # Cluster manager modo local con 2 hilos\n.getOrCreate()\n# Obtenemos el SparkContext\nsc \u003d spark.sparkContext\n```\n\n-   Mediante flags en el `spark-submit`\n```sh\n$ bin/spark-submit --master local[2] --name \"Mi apli\" \\  \n    --conf spark.ui.port\u003d3600 mi-script.py\n```    \n    \n-   Mediante un fichero de propiedades\n```sh\n$ cat config.conf\nspark.master     local[2] \nspark.app.name   \"Mi apli\" \nspark.ui.port 3600\n$ bin/spark-submit --properties-file config.conf mi-script.py\n```\n\nMás info: \u003chttp://spark.apache.org/docs/latest/configuration.html#spark-properties\u003e\n",
      "user": "anonymous",
      "dateUpdated": "2018-12-12 16:16:19.000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false,
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eParámetros de configuración\u003c/h3\u003e\n\u003cp\u003eDiversos parámetros ajustables en tiempo de ejecución\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n  \u003cp\u003e\n  \u003cp\u003eEn el script\u003c/p\u003e\n  \u003cpre\u003e\u003ccode class\u003d\"python\"\u003efrom pyspark.sql import SparkSession\n# Creamos un objeto SparkSession (o lo obtenemos si ya está creado)\nspark \u003d SparkSession \\\n.builder \\\n.appName(\u0026quot;Mi aplicacion\u0026quot;) \\\n.config(\u0026quot;spark.ui.port\u0026quot;, \u0026quot;3600\u0026quot;)\\    # Puerto del interfaz web de Spark (por defecto: 4040)\n.master(\u0026quot;local[2]\u0026quot;) \\ # Cluster manager modo local con 2 hilos\n.getOrCreate()\n# Obtenemos el SparkContext\nsc \u003d spark.sparkContext\n\u003c/code\u003e\u003c/pre\u003e\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eMediante flags en el \u003ccode\u003espark-submit\u003c/code\u003e\u003c/p\u003e\n  \u003cpre\u003e\u003ccode class\u003d\"sh\"\u003e$ bin/spark-submit --master local[2] --name \u0026quot;Mi apli\u0026quot; \\  \n--conf spark.ui.port\u003d3600 mi-script.py\n\u003c/code\u003e\u003c/pre\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eMediante un fichero de propiedades\u003c/p\u003e\n  \u003cpre\u003e\u003ccode class\u003d\"sh\"\u003e$ cat config.conf\nspark.master     local[2] \nspark.app.name   \u0026quot;Mi apli\u0026quot; \nspark.ui.port 3600\n$ bin/spark-submit --properties-file config.conf mi-script.py\n\u003c/code\u003e\u003c/pre\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMás info: \u003ca href\u003d\"http://spark.apache.org/docs/latest/configuration.html#spark-properties\"\u003ehttp://spark.apache.org/docs/latest/configuration.html#spark-properties\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1508256444947_1400869387",
      "id": "20170719-201838_664647966",
      "dateCreated": "2017-10-17 16:07:24.000",
      "dateStarted": "2018-12-12 16:16:10.000",
      "dateFinished": "2018-12-12 16:16:13.000",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Ejemplo de ejecución de un script Python",
      "dateUpdated": "2017-10-17 16:07:24.000",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eEjemplo de ejecución de un script Python\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1508256444947_1400869387",
      "id": "20170720-184451_1169230278",
      "dateCreated": "2017-10-17 16:07:24.000",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\n# No modificar la siguiente linea\ncat \u003c\u003c EOF \u003e /tmp/miscript.py\n# -*- coding: utf-8 -*-\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import sum,col\ndef main():\n    spark \u003d SparkSession \\\n    .builder \\\n    .appName(\"Mi script Python\") \\\n    .getOrCreate()\n\n    # Cambio la verbosidad para reducir el número de\n    # mensajes por pantalla\n    spark.sparkContext.setLogLevel(\"FATAL\")\n    \n    df1 \u003d spark.range(2, 10000000, 2)\n    df2 \u003d spark.range(2, 10000000, 4)\n    step1 \u003d df1.repartition(5)\n    step12 \u003d df2.repartition(6)\n    step2 \u003d step1.selectExpr(\"id * 5 as id\")\n    step3 \u003d step2.join(step12, [\"id\"])\n    step4 \u003d step3.select(sum(col(\"id\")))\n    \n    # step4 es un dataframe con una única fila\n    # que es un objeto Row.\n    # Con collect() obtengo la fila como una lista\n    # me quedo con el primer elemento (Row) de la lista\n    # y lo convierto a un diccionario Python\n    salida \u003d step4.collect()[0].asDict()[\u0027sum(id)\u0027]\n    print(\"Resultado final \u003d {0}\".format(salida))\n\nif __name__ \u003d\u003d \"__main__\":\n    main()\nEOF\n",
      "user": "anonymous",
      "dateUpdated": "2020-11-18 15:50:36.898",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1541587301514_-773365533",
      "id": "20181107-104141_1867226493",
      "dateCreated": "2018-11-07 10:41:41.000",
      "dateStarted": "2020-11-18 15:50:36.908",
      "dateFinished": "2020-11-18 15:50:36.965",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\ncat /tmp/miscript.py",
      "user": "anonymous",
      "dateUpdated": "2020-11-18 15:50:52.501",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "# -*- coding: utf-8 -*-\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import sum,col\ndef main():\n    spark \u003d SparkSession     .builder     .appName(\"Mi script Python\")     .getOrCreate()\n\n    # Cambio la verbosidad para reducir el número de\n    # mensajes por pantalla\n    spark.sparkContext.setLogLevel(\"FATAL\")\n    \n    df1 \u003d spark.range(2, 10000000, 2)\n    df2 \u003d spark.range(2, 10000000, 4)\n    step1 \u003d df1.repartition(5)\n    step12 \u003d df2.repartition(6)\n    step2 \u003d step1.selectExpr(\"id * 5 as id\")\n    step3 \u003d step2.join(step12, [\"id\"])\n    step4 \u003d step3.select(sum(col(\"id\")))\n    \n    # step4 es un dataframe con una única fila\n    # que es un objeto Row.\n    # Con collect() obtengo la fila como una lista\n    # me quedo con el primer elemento (Row) de la lista\n    # y lo convierto a un diccionario Python\n    salida \u003d step4.collect()[0].asDict()[\u0027sum(id)\u0027]\n    print(\"Resultado final \u003d {0}\".format(salida))\n\nif __name__ \u003d\u003d \"__main__\":\n    main()\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1592424254667_-1311196845",
      "id": "20200617-200414_1094055869",
      "dateCreated": "2020-06-17 20:04:14.667",
      "dateStarted": "2020-11-18 15:50:52.512",
      "dateFinished": "2020-11-18 15:50:52.521",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\n/usr/local/spark/bin/spark-submit --master local[8] /tmp/miscript.py",
      "user": "anonymous",
      "dateUpdated": "2020-11-18 15:51:06.243",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/sh",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionSupport": false
        },
        "fontSize": 9.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Using Spark\u0027s default log4j profile: org/apache/spark/log4j-defaults.properties\n20/11/18 15:51:07 INFO SparkContext: Running Spark version 2.2.0\n20/11/18 15:51:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n20/11/18 15:51:07 INFO SparkContext: Submitted application: Mi script Python\n20/11/18 15:51:07 INFO SecurityManager: Changing view acls to: root\n20/11/18 15:51:07 INFO SecurityManager: Changing modify acls to: root\n20/11/18 15:51:07 INFO SecurityManager: Changing view acls groups to: \n20/11/18 15:51:07 INFO SecurityManager: Changing modify acls groups to: \n20/11/18 15:51:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n20/11/18 15:51:08 INFO Utils: Successfully started service \u0027sparkDriver\u0027 on port 37333.\n20/11/18 15:51:08 INFO SparkEnv: Registering MapOutputTracker\n20/11/18 15:51:08 INFO SparkEnv: Registering BlockManagerMaster\n20/11/18 15:51:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n20/11/18 15:51:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n20/11/18 15:51:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-13bcb12d-1a0b-4bf1-9990-53f3d9cbf488\n20/11/18 15:51:08 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n20/11/18 15:51:08 INFO SparkEnv: Registering OutputCommitCoordinator\n20/11/18 15:51:08 WARN Utils: Service \u0027SparkUI\u0027 could not bind on port 4040. Attempting port 4041.\n20/11/18 15:51:08 INFO Utils: Successfully started service \u0027SparkUI\u0027 on port 4041.\n20/11/18 15:51:08 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.20.0.2:4041\n20/11/18 15:51:08 INFO SparkContext: Added file file:/tmp/miscript.py at file:/tmp/miscript.py with timestamp 1605714668692\n20/11/18 15:51:08 INFO Utils: Copying /tmp/miscript.py to /tmp/spark-5f4909e6-11b1-4f10-b2da-4e9f670fed8c/userFiles-f22072a2-d5bb-4d9d-b41a-57a4c9ac22fb/miscript.py\n20/11/18 15:51:08 INFO Executor: Starting executor ID driver on host localhost\n20/11/18 15:51:08 INFO Utils: Successfully started service \u0027org.apache.spark.network.netty.NettyBlockTransferService\u0027 on port 46305.\n20/11/18 15:51:08 INFO NettyBlockTransferService: Server created on 172.20.0.2:46305\n20/11/18 15:51:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n20/11/18 15:51:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.20.0.2, 46305, None)\n20/11/18 15:51:08 INFO BlockManagerMasterEndpoint: Registering block manager 172.20.0.2:46305 with 366.3 MB RAM, BlockManagerId(driver, 172.20.0.2, 46305, None)\n20/11/18 15:51:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.20.0.2, 46305, None)\n20/11/18 15:51:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.20.0.2, 46305, None)\n20/11/18 15:51:09 INFO SharedState: Setting hive.metastore.warehouse.dir (\u0027null\u0027) to the value of spark.sql.warehouse.dir (\u0027file:/zeppelin/spark-warehouse/\u0027).\n20/11/18 15:51:09 INFO SharedState: Warehouse path is \u0027file:/zeppelin/spark-warehouse/\u0027.\n20/11/18 15:51:09 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\nResultado final \u003d 2500000000000\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1508256444948_1398945643",
      "id": "20170720-185117_83384344",
      "dateCreated": "2017-10-17 16:07:24.000",
      "dateStarted": "2020-11-18 15:51:06.253",
      "dateFinished": "2020-11-18 15:51:16.094",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n",
      "user": "anonymous",
      "dateUpdated": "2020-11-05 17:33:55.457",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1545242109175_-266748551",
      "id": "20181219-175509_541785915",
      "dateCreated": "2018-12-19 17:55:09.000",
      "dateStarted": "2020-11-05 17:33:47.010",
      "dateFinished": "2020-11-05 17:33:47.177",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n",
      "user": "anonymous",
      "dateUpdated": "2020-11-05 17:33:46.987",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1604597626983_1326358572",
      "id": "20201105-173346_976355695",
      "dateCreated": "2020-11-05 17:33:46.983",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "3.8. Spark en un cluster",
  "id": "2CWPWRM93",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "sh:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}
