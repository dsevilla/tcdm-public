{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark: Aspectos avanzados\n",
    "\n",
    "En este tema trataremos algunos aspectos adicionales de Apache Spark\n",
    "\n",
    "- Cómo se ejecuta una aplicación Spark\n",
    "- Uso de variables de broadcast y acumuladores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución de una aplicación Spark\n",
    "\n",
    "Veremos conceptos relacionados con la ejecución de un código Spark\n",
    "\n",
    "  - Plan lógico y físico\n",
    "  - Trabajos, etapas y tareas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan lógico y físico\n",
    "A partir de un código de usuario, Spark genera un *plan lógico*\n",
    "\n",
    "  -  DAG (denominado *lineage graph*) con las operaciones a realizar\n",
    "  -  No incluye información sobre el sistema físico en el que se va a ejecutar\n",
    "  -  El optimizador *Catalyst* genera un plan lógico optimizado\n",
    "  \n",
    "A partir del plan lógico optimizado, se elabora un plan físico\n",
    "\n",
    "  - Especifica cómo se ejecutará el plan lógico en el cluster\n",
    "  - Se generan diferentes estrategias de ejecución que se comparan mediante un modelo de costes\n",
    "      - Por ejemplo, cómo realizar un join en función de las características de los datos (tamaño, particiones, ...)\n",
    "\n",
    "El plan físico se ejecuta en el cluster\n",
    "\n",
    "  - La ejecución se hace sobre RDDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajos, etapas y tareas\n",
    "\n",
    "Una acción genera un *trabajo* (Spark job)\n",
    "\n",
    "-   El job se descompone en una o más *etapas* (stages)\n",
    "-   Las etapas representan grupos de *tareas* (tasks) que se ejecutan en paralelo\n",
    "    - Cada tarea ejecuta una o más transformaciones sobre una partición\n",
    "    - Las tareas se ejecutan en los nodos del cluster\n",
    "- Una etapa termina cuando se realiza una operación de *barajado* (*shuffle*)\n",
    "    - Implica mover datos entre los nodos del cluster\n",
    "\n",
    "Pipelining: varios operaciones se pueden computan en una misma etapa\n",
    "\n",
    "-   Operaciones que no impliquen movimiento de datos (pe. `select`, `filter` o `map`)\n",
    "-   La salida de cada operación se pasa a la entrada de la siguiente sin ir a disco\n",
    "\n",
    "Persistencia en el barajado\n",
    "\n",
    "-  Antes de un barajado, los datos se escriben a disco local\n",
    "-  Permite relanzar tareas falladas sin necesidad de recomputar todas las transformaciones\n",
    "-  No se realiza si los datos a barajar ya han sido cacheados (con `cache` o `persist`)\n",
    "\n",
    "En el [interfaz web de Spark](http://localhost:4040 \"PySpark en localhost\") se muestran información sobre las etapas y tareas \n",
    "\n",
    "- El método `explain()` de los DataFrames o `toDebugString()` de los RDDs muestra el plan físico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Elegir el máster de Spark dependiendo de si se ha definido la variable de entorno HADOOP_CONF_DIR o YARN_CONF_DIR\n",
    "SPARK_MASTER: str = 'yarn' if 'HADOOP_CONF_DIR' in os.environ or 'YARN_CONF_DIR' in os.environ else 'local[*]'\n",
    "print(f\"Usando Spark Master en {SPARK_MASTER}\")\n",
    "\n",
    "# Creamos un objeto SparkSession (o lo obtenemos si ya está creado)\n",
    "spark: SparkSession = SparkSession \\\n",
    "  .builder \\\n",
    "  .appName(\"Mi aplicacion\") \\\n",
    "  .config(\"spark.rdd.compress\", \"true\") \\\n",
    "  .config(\"spark.executor.memory\", \"3g\") \\\n",
    "  .config(\"spark.driver.memory\", \"3g\") \\\n",
    "  .master(SPARK_MASTER) \\\n",
    "  .getOrCreate()\n",
    "\n",
    "sc: SparkContext = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import sum,col\n",
    "\n",
    "# Ejemplo para visualizar el plan físico\n",
    "df1: DataFrame = spark.range(2, 10000000, 2)\n",
    "df2: DataFrame = spark.range(2, 10000000, 4)\n",
    "step1: DataFrame = df1.repartition(5)\n",
    "step12: DataFrame = df2.repartition(6)\n",
    "step2: DataFrame = step1.selectExpr(\"id * 5 as id\")\n",
    "step3: DataFrame = step2.join(step12, [\"id\"])\n",
    "step4: DataFrame = step3.select(sum(col(\"id\")))\n",
    "\n",
    "print(step4.collect())\n",
    "step4.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables de broadcast\n",
    "\n",
    "-   Por defecto, todas las variables compartidas (no RDDs) son enviadas a todos los ejecutores\n",
    "\n",
    "    -   Se reenvían en cada operación en la que aparezcan\n",
    "\n",
    "-   Variables de broadcast: permiten enviar de forma eficiente variables de solo lectura a los workers\n",
    "\n",
    "    -   Se envían solo una vez\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark import RDD\n",
    "\n",
    "rdd: RDD[int] = sc.parallelize([1,2,3,4])\n",
    "x=5\n",
    "rdd2: RDD[int] = rdd.map(lambda n: x*n)\n",
    "rdd2: RDD[int] = rdd2.map(lambda n: x-n)\n",
    "print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from pyspark import Broadcast\n",
    "\n",
    "# dicc es una variable de broadcast\n",
    "dicc: dict[str, str] = {\"a\": \"alpha\", \"b\": \"beta\", \"c\": \"gamma\"}\n",
    "bcastDicc: Broadcast[dict[str, str]] = sc.broadcast(dicc)\n",
    "\n",
    "rdd: RDD[tuple[str, int]] = sc.parallelize([(\"a\", 1),(\"b\", 3),(\"a\", -4),(\"c\", 0)])\n",
    "reduced_rdd: RDD[tuple[str | None, int]] =  \\\n",
    "    rdd.reduceByKey(add).map(lambda tupla: (bcastDicc.value.get(tupla[0]), tupla[1]))\n",
    "\n",
    "print(reduced_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acumuladores\n",
    "\n",
    "Permiten agregar valores desde los *worker nodes*, que se pasan al *driver*\n",
    "\n",
    "-   Útiles para contar eventos\n",
    "\n",
    "-   Solo el driver puede acceder a su valor\n",
    "\n",
    "-   Acumuladores usados en transformaciones de RDDs pueden ser incorrectos\n",
    "\n",
    "    -   Si el RDD se recalcula, el acumulador puede actualizarse\n",
    "\n",
    "    -   En acciones, este problema no ocurre\n",
    "\n",
    "-   Por defecto, los acumuladores son enteros o flotantes\n",
    "    - Es posible crear “acumuladores a medida” usando [`AccumulatorParam`](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html#pyspark.AccumulatorParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark import Accumulator\n",
    "from pyspark.sql import Row\n",
    "from random import randint\n",
    "\n",
    "# Creamos el DataFrame a partir de una lista de objetos Row\n",
    "# con enteros aleatorios\n",
    "l: list[Row] = [Row(randint(1,10)) for n in range(10000)]\n",
    "df: DataFrame = spark.createDataFrame(l)\n",
    "\n",
    "# Definimos un acumulador\n",
    "npares: Accumulator[int] = sc.accumulator(0)\n",
    "\n",
    "# si el número en una fila es par, incrementamos en acumulador\n",
    "def contarPares(fila: Row) -> None:\n",
    "    global npares\n",
    "    if fila[\"_1\"]%2 == 0:\n",
    "        npares += 1\n",
    "\n",
    "# Ejecutamos la función una vez por fila\n",
    "df.foreach(contarPares)\n",
    "\n",
    "print(\"Numero de pares: {0}\".format(npares.value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
