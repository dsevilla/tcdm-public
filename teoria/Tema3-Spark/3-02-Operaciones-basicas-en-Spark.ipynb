{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkU_F8hZLeOL"
   },
   "source": [
    "# Operaciones básicas en Spark\n",
    "- Spark opera con colecciones **inmutables y distribuidas** de elementos, manipulándolos en paralelo\n",
    "    - API estructurada: DataFrames y DataSets\n",
    "    - API de bajo nivel: RDDs (ya obsoleto ya que el API estructurada es más eficiente y más de alto nivel)\n",
    "\n",
    "-   Operaciones sobre estas colecciones\n",
    "    -   Creación\n",
    "    -   Transformaciones (ordenación, filtrado, etc.)\n",
    "    -   Realización acciones para obtener resultados\n",
    "\n",
    "-   Spark automáticamente distribuye los datos y paraleliza las operaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bT8r0anjLeOW"
   },
   "source": [
    "### Ejemplo: creación de un DataFrame a partir de un fichero CSV\n",
    "En este ejemplo, Spark infiere el esquema de los datos de forma automática\n",
    "\n",
    "  - Es preferible especificar el esquema de forma explícita, como veremos más adelante\n",
    "\n",
    "También se especifica que la primera línea es la cabecera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "format": "text/plain",
    "id": "EwPKO0reLeOY",
    "outputId": "d4c99bfe-6467-46a2-97f1-f22f84e0ba06",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "wget -q \"https://raw.githubusercontent.com/dsevilla/tcdm-public/24-25/datos/2015-summary.csv\"\n",
    "ls -lh 2015-summary.csv\n",
    "head 2015-summary.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7SQV5O4sOcOn",
    "outputId": "2a2f7173-fb21-4a31-d3a5-d8c95cd4a396"
   },
   "outputs": [],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cZW1zRyPOo2e"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creamos un objeto SparkSession (o lo obtenemos si ya está creado)\n",
    "spark: SparkSession = SparkSession \\\n",
    "  .builder \\\n",
    "  .appName(\"Mi aplicacion\") \\\n",
    "  .config(\"spark.alguna.opcion.de.configuracion\", \"algun-valor\") \\\n",
    "  .master(\"local[*]\") \\\n",
    "  .getOrCreate()\n",
    "\n",
    "sc: SparkContext = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "id": "6nIgZApNLeOc"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "datosVuelos2015: DataFrame = (spark\n",
    "    .read\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(\"2015-summary.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3rdmzrqSLeOf",
    "outputId": "bf28e6c8-903d-487e-8faf-749d10c5a784"
   },
   "outputs": [],
   "source": [
    "datosVuelos2015.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DLGLlv8pLeOh",
    "outputId": "533cfc62-6e81-444a-e0d4-20c1befd0744"
   },
   "outputs": [],
   "source": [
    "datosVuelos2015.show()\n",
    "print(datosVuelos2015.count())\n",
    "\n",
    "assert(datosVuelos2015.count() == 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t4TRlMpzLeOh",
    "outputId": "fcda2346-d37d-43da-c4e6-2f3a2ad0a50e"
   },
   "outputs": [],
   "source": [
    "datosVuelos2015.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYZBVIqiLeOi"
   },
   "source": [
    "### Rows\n",
    "\n",
    "Las filas de un DataFrame son objetos de tipo `Row`\n",
    "\n",
    "- API de Row en Python: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Row.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "id": "eZZFV6aJLeOm"
   },
   "outputs": [],
   "source": [
    "# Obtenemos las dos primeras fila del DataFrame\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "rows1_2: list[Row] = datosVuelos2015.take(2)\n",
    "print(rows1_2)\n",
    "print(type(rows1_2))\n",
    "print(type(rows1_2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "id": "lNQfTYQWLeOn"
   },
   "outputs": [],
   "source": [
    "# Obtén la primera fila como un diccionario Python\n",
    "print(rows1_2[0].asDict())\n",
    "print(type(rows1_2[0].asDict()))\n",
    "\n",
    "assert(type(rows1_2[0].asDict()) is dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1QwipCULeOp"
   },
   "source": [
    "### Particiones\n",
    "\n",
    "Spark divide las filas DataFrame en un conjunto de particiones\n",
    "\n",
    "-   El número de particiones por defecto es función del tamaño del cluster (número total de cores en todos los ejecutores) y del tamaño de los datos (número de bloques de los ficheros en HDFS)\n",
    "-   Para RDDs se puede especificar otro valor en el momento de crearlos\n",
    "-   También se puede modificar una vez creados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "id": "oCKT8sLhLeOp"
   },
   "outputs": [],
   "source": [
    "print(\"Número de particiones: {0}\"\n",
    "    .format(datosVuelos2015.rdd.getNumPartitions()))\n",
    "\n",
    "# Creo un nuevo DataFrame con 4 particiones\n",
    "datosVuelos2015_4P: DataFrame = datosVuelos2015.repartition(4)\n",
    "print(\"Número de particiones: {0}\"\n",
    "    .format(datosVuelos2015_4P.rdd.getNumPartitions()))\n",
    "\n",
    "assert(datosVuelos2015_4P.rdd.getNumPartitions() == 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2rp6vPfLeOr"
   },
   "source": [
    "### Transformaciones\n",
    "\n",
    "Operaciones que transforman los datos\n",
    "\n",
    "  - No modifican los datos de origen (*inmutabilidad*)\n",
    "  - Se computan de forma “perezosa” (*lazyness*)\n",
    "\n",
    "Dos tipos:\n",
    "\n",
    "  - Transformaciones *estrechas* (narrow)\n",
    "    - Cada partición de entrada contribuye a una única partición de salida\n",
    "    - No se modifica el número de particiones\n",
    "    - Normalmente se realizan en memoria\n",
    "  - Transformaciones *anchas* (wide)\n",
    "    - Cada partición de salida depende de varias (o todas) particiones de entrada\n",
    "    - Suponen un barajado de datos\n",
    "    - Pueden implicar un cambio en el número de particiones\n",
    "    - Pueden suponer escrituras en disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "id": "boqxgnc7LeOs"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de una transformación narrow\n",
    "datosVuelos2015_EEUU: DataFrame = datosVuelos2015\\\n",
    "    .replace(\"United States\", \"Estados Unidos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "id": "RoFCSMYxLeOt"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de una transformación wide\n",
    "datosVuelos2015_Ord: DataFrame = datosVuelos2015_EEUU\\\n",
    "    .sort(\"count\", ascending=False)\n",
    "datosVuelos2015_Ord.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsvBBIjmLeOu"
   },
   "source": [
    "### Acciones\n",
    "\n",
    "Obtienen un resultado, forzando a que se realicen las transformaciones pendientes\n",
    "\n",
    "  - En el momento de disparar la *acción* se crea un *plan* con las transformaciones necesarias para obtener los datos solicitados\n",
    "    - Se crea un Grafo Dirigido Acíclico (DAG) conectando las transformaciones\n",
    "    - Spark optimiza ese grafo, para eliminar transformaciones innecesarias o unir las que sea posible\n",
    "  - Las acciones traducen el DAG en un plan de ejecución\n",
    "\n",
    "Tipos de acciones\n",
    "\n",
    "  - Acciones para mostrar datos por consola\n",
    "  - Acciones para convertir datos Spark en datos del lenguaje\n",
    "  - Acciones para escribir datos a disco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "id": "T9H6efssLeOv"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de acciones\n",
    "from pprint import pp\n",
    "\n",
    "print(\"Número de filas en la tabla: {0}\"\n",
    "    .format(datosVuelos2015_Ord.count()))\n",
    "\n",
    "pp(datosVuelos2015_Ord.take(3))\n",
    "\n",
    "datosVuelos2015_Ord.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
