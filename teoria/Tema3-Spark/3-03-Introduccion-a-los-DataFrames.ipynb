{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a los DataFrames\n",
    "\n",
    "En este tema veremos:\n",
    "\n",
    "  - Cómo crear un DataFrame\n",
    "  - Algunas operaciones básicas sobre DataFrames\n",
    "      - Mostrar filas\n",
    "      - Seleccionar columnas\n",
    "      - Renombrar, añadir y eliminar columnas\n",
    "      - Eliminar valores nulos y filas duplicadas\n",
    "      - Reemplazar valores\n",
    "  - Guardar los DataFrames en diferentes formatos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de DataFrames\n",
    "Un DataFrame puede crearse de distintas formas:\n",
    "\n",
    "  - A partir de una secuencia de datos\n",
    "  - A partir de objetos de tipo Row\n",
    "  - A partir de un RDD o DataSet\n",
    "  - Leyendo los datos de un fichero\n",
    "      - Igual que Hadoop, Spark soporta diferentes filesystems: local, HDFS, Amazon S3\n",
    "          - En general, soporta cualquier fuente de datos que se pueda leer con Hadoop\n",
    "      - Spark puede acceder a diferentes tipos de ficheros: texto plano, CSV, JSON, [Parquet](https://parquet.apache.org/), [ORC](https://orc.apache.org/), Sequence, etc\n",
    "        -   Soporta ficheros comprimidos\n",
    "  - Accediendo a bases de datos relacionales o NoSQL\n",
    "    -   MySQL, Postgres, etc. mediante JDBC/ODBC\n",
    "    -   Hive, HBase, Cassandra, MongoDB, AWS Redshift, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando DataFrames a partir de una secuencia o lista de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Elegir el máster de Spark dependiendo de si se ha definido la variable de entorno HADOOP_CONF_DIR o YARN_CONF_DIR\n",
    "SPARK_MASTER: str = 'yarn' if 'HADOOP_CONF_DIR' in os.environ or 'YARN_CONF_DIR' in os.environ else 'local[*]'\n",
    "\n",
    "# Creamos un objeto SparkSession (o lo obtenemos si ya está creado)\n",
    "spark: SparkSession = SparkSession \\\n",
    "  .builder \\\n",
    "  .appName(\"Mi aplicacion\") \\\n",
    "  .config(\"spark.rdd.compress\", \"true\") \\\n",
    "  .config(\"spark.executor.memory\", \"3g\") \\\n",
    "  .config(\"spark.driver.memory\", \"3g\") \\\n",
    "  .master(SPARK_MASTER) \\\n",
    "  .getOrCreate()\n",
    "\n",
    "sc: SparkContext = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pp\n",
    "\n",
    "pp(sc._conf.getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Creando un DataFrame desde un rango y añadiéndole dos columnas\n",
    "df: DataFrame = spark.range(1,7,2).toDF(\"n\")\n",
    "df.show()\n",
    "\n",
    "# Añadiendo dos columnas al DataFrame\n",
    "# La expresión para la columna puede incluir operadores.\n",
    "df.withColumn(\"n1\", col(\"n\")+1).withColumn(\"n2\", 2*col(\"n1\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# DataFrame a partir de una lista de tuplas\n",
    "l: list[tuple] = [\n",
    "     (\"Pepe\", 5.1, \"Aprobado\"),\n",
    "     (\"Juan\", 4.0, \"Suspenso\"),\n",
    "     (\"Manuel\", None, None)]\n",
    "dfNotas: DataFrame = spark.createDataFrame(l, schema=[\"nombre\", \"nota\", \"cal\"])\n",
    "dfNotas.show()\n",
    "dfNotas.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando DataFrames con esquema\n",
    "A la hora de crear un DataFrame, es conveniente especificar el esquema del mismo:\n",
    "\n",
    "  - El esquema define los nombres y tipos de datos de las columnas\n",
    "  - Se usa un objeto de tipo `StructType` para definir el nombre y tipo de las columnas, y un objeto de tipo `StructField` para definir el nombre y tipo de una columna\n",
    "  - Los tipos de datos que utiliza Spark están definidos en:\n",
    "      - Para PySpark: https://spark.apache.org/docs/latest/sql-ref-datatypes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, FloatType, StringType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Definimos el esquema del DataFrame\n",
    "esquemaNotas = StructType(fields=[\n",
    "    StructField(name=\"nombre\", dataType=StringType(), nullable=False),\n",
    "    StructField(name=\"nota\", dataType=FloatType(), nullable=True),\n",
    "    StructField(name=\"cal\", dataType=StringType(), nullable=True)\n",
    "    ])\n",
    "\n",
    "# Creamos el DataFrame a partir de una lista de objetos Row\n",
    "filas: list[Row] = [\n",
    "    Row(\"Pepe\", 5.1, \"Aprobado\"),\n",
    "    Row(\"Juan\", 4.0, \"Suspenso\"),\n",
    "    Row(\"Manuel\", None, None)\n",
    "    ]\n",
    "\n",
    "dfNotas: DataFrame = spark.createDataFrame(filas, schema=esquemaNotas)\n",
    "dfNotas.show()\n",
    "dfNotas.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando DataFrames a partir de un fichero de texto\n",
    "\n",
    "Cada línea del fichero se guarda como una fila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "# Añadimos un archivo al contexto de Spark (los descarga en cada nodo)\n",
    "sc.addFile(\"https://raw.githubusercontent.com/dsevilla/tcdm-public/refs/heads/24-25/datos/quijote.txt.gz\")\n",
    "\n",
    "dfQuijote: DataFrame = spark.read.text(\"file://\" + SparkFiles.get(\"quijote.txt.gz\"))\n",
    "dfQuijote.show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando DataFrames a partir de un fichero CSV\n",
    "\n",
    "Como ejemplo vamos a utilizar el fichero de preguntas y respuestas de Stack Overflow en Español, que hemos utilizado en otras ocasiones. Es un fichero CSV, con unos campos que son:\n",
    "\n",
    "- `Id`: integer: La identificación de la pregunta o respuesta\n",
    "- `AcceptedAnswerId`: integer: La identificación de la respuesta aceptada (si existe)\n",
    "- `AnswerCount`: integer: El número de respuestas\n",
    "- `Body`: string: El cuerpo de la pregunta o respuesta\n",
    "- `ClosedDate`: timestamp: Fecha de cierre de la pregunta (si está cerrada)\n",
    "- `CommentCount`: integer: Número de comentarios\n",
    "- `CommunityOwnedDate`: timestamp: (no se usará)  \n",
    "- `ContentLicense`: string: Licencia de contenido\n",
    "- `CreationDate`: timestamp: La fecha de creación\n",
    "- `FavoriteCount`: integer: Número de favoritos\n",
    "- `LastActivityDate`: timestamp: (no se usará)\n",
    "- `LastEditDate`: timestamp: (no se usará)\n",
    "- `LastEditorDisplayName`: string: (no se usará)\n",
    "- `LastEditorUserId`: integer: (no se usará)\n",
    "- `OwnerDisplayName`: string: El nombre del propietario (si se borró el usuario) \n",
    "- `OwnerUserId`: integer: El identificador del propietario\n",
    "- `ParentId`: integer: El identificador de la pregunta padre (si es una respuesta)\n",
    "- `PostTypeId`: integer: El tipo de post (1 = pregunta, 2 = respuesta, etc.)\n",
    "- `Score`: integer: La puntuación de la pregunta o respuesta\n",
    "- `Tags`: string: El conjunto de etiquetas\n",
    "- `Title`: string: El título de la pregunta\n",
    "- `ViewCount`: integer: El número de visitas\n",
    "\n",
    "Los campos se encuentran separados por el símbolo `\",\"`, y el carácter de escape de comillas es el propio carácter de comillas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leemos el fichero infiriendo el esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "wget -q https://github.com/dsevilla/bd2-data/raw/main/es.stackoverflow/es.stackoverflow.csv.7z.001 -O - > es.stackoverflow.csv.7z\n",
    "wget -q https://github.com/dsevilla/bd2-data/raw/main/es.stackoverflow/es.stackoverflow.csv.7z.002 -O - >> es.stackoverflow.csv.7z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "7zr x -aoa es.stackoverflow.csv.7z Posts.csv\n",
    "rm es.stackoverflow.csv.7z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPARK_MASTER == 'yarn':\n",
    "  !hdfs dfs -put Posts.csv /user/hdadmin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfSEInfered: DataFrame = spark.read.format(\"csv\")\\\n",
    "                    .option(\"mode\", \"FAILFAST\")\\\n",
    "                    .option(\"sep\", \",\")\\\n",
    "                    .option(\"escape\", \"\\\"\")\\\n",
    "                    .option(\"inferSchema\", \"true\")\\\n",
    "                    .option(\"lineSep\", \"\\r\\n\")\\\n",
    "                    .option(\"header\", \"true\")\\\n",
    "                    .option(\"nullValue\", \"\")\\\n",
    "                    .load(\"Posts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas opciones:\n",
    "\n",
    "1. ``mode``: especifica qué hacer cuando se encuentra registros corruptos\n",
    "    - ``PERMISSIVE``: pone todos los campos a null cuando se encuentra un registro corrupto (valor por defecto)\n",
    "    - ``DROPMALFORMED``: elimina las filas con registros corruptos\n",
    "    - ``FAILFAST``: da un error cuando se encuentra un registro corrupto\n",
    "2. ``sep``: separador entre campos (por defecto \",\")\n",
    "3. ``inferSchema``: especifica si se deben inferir el tipo de las columnas (por defecto \"false\")\n",
    "3. ``lineSep``: separador de líneas (por defecto \"\\n\"). Lo hemos cambiado a \"\\r\\n\" porque el fichero se ha creado en Windows, aunque de un warning, funciona correctamente\n",
    "4. ``header``: si \"true\" se toma la primera fila como cabecera (por defecto \"false\")\n",
    "5. ``nullValue``: carrácter o cadena que representa un NULL en el fichero (por defecto \"\")\n",
    "6. ``compression``: topo de compresión utilizada (por defecto \"none\")\n",
    "  \n",
    "Las opciones son similares para otros tipos de ficheros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Vemos 5 filas\n",
    "dfSEInfered.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Vemos como se ha inferido el esquema\n",
    "dfSEInfered.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Otra forma de verlo\n",
    "dfSEInfered.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leemos especificando el esquema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El esquema inferido tiene ciertos fallos, como considerar algunos campos como strings cuando deberían ser enteros, o tipos Timestamp en vez de Date. Por ello, vamos a especificar el esquema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, IntegerType, TimestampType, StringType\n",
    "\n",
    "# Defino el esquema para los elementos de la tabla\n",
    "# StructType -> Permite definir un esquema para el DF a partir de una lista de StructFields\n",
    "# StructField -> Definen el nombre y tipo de cada columna, así como si es nullable o no (campo True)\n",
    "dfSE_Schema = StructType([StructField('Id', IntegerType(), False),\n",
    "                          StructField('AcceptedAnswerId', IntegerType(), True),\n",
    "                          StructField('AnswerCount', IntegerType(), True),\n",
    "                          StructField('Body', StringType(), True),\n",
    "                          StructField('ClosedDate', TimestampType(), True),\n",
    "                          StructField('CommentCount', IntegerType(), True),\n",
    "                          StructField('CommunityOwnedDate', TimestampType(), True),\n",
    "                          StructField('ContentLicense', StringType(), True),\n",
    "                          StructField('CreationDate', TimestampType(), True),\n",
    "                          StructField('FavoriteCount', IntegerType(), True),\n",
    "                          StructField('LastActivityDate', TimestampType(), True),\n",
    "                          StructField('LastEditDate', TimestampType(), True),\n",
    "                          StructField('LastEditorDisplayName', StringType(), True),\n",
    "                          StructField('LastEditorUserId', IntegerType(), True),\n",
    "                          StructField('OwnerDisplayName', StringType(), True),\n",
    "                          StructField('OwnerUserId', IntegerType(), True),\n",
    "                          StructField('ParentId', IntegerType(), True),\n",
    "                          StructField('PostTypeId', IntegerType(), True),\n",
    "                          StructField('Score', IntegerType(), True),\n",
    "                          StructField('Tags', StringType(), True),\n",
    "                          StructField('Title', StringType(), True),\n",
    "                          StructField('ViewCount', IntegerType(), True)])\n",
    "\n",
    "# Creo el DataFrame con el esquema definido\n",
    "dfSE: DataFrame = spark.read.format(\"csv\")\\\n",
    "                    .option(\"mode\", \"FAILFAST\")\\\n",
    "                    .option(\"inferSchema\", \"false\")\\\n",
    "                    .option(\"sep\", \",\")\\\n",
    "                    .option(\"header\", \"true\")\\\n",
    "                    .option(\"nullValue\", \"\")\\\n",
    "                    .option(\"lineSep\", \"\\r\\n\")\\\n",
    "                    .option(\"escape\", \"\\\"\")\\\n",
    "                    .schema(dfSE_Schema)\\\n",
    "                    .load(\"Posts.csv\")\n",
    "dfSE.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfSE.sort(\"Id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfSE.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones básicas con DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostrar filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# show(n) permite mostrar las primeras n filas (por defecto, n=20)\n",
    "dfSE.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Podemos indicar que no trunque los campos largos\n",
    "dfSE.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# take(n) devuelve las n primeras filas como una lista Python de objetos Row\n",
    "lista: list[Row] = dfSE.take(5)\n",
    "pp(lista[1])\n",
    "# collect() devuelve todo el DataFrame como una lista Python de objetos Row\n",
    "# Si el DataFrame es muy grande podría colapsar al Driver\n",
    "#lista2 = dfSE.collect()\n",
    "#print(lista2[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# sample(withReplacement, fraction, seed=None) devuelve un nuevo Dataframe con una fracción de las filas\n",
    "dfSESampled: DataFrame = dfSE.sample(False, 0.1, seed=None)\n",
    "print(\"N de filas original = {0}; n de filas muestreadas = {1}\".format(dfSE.count(), dfSESampled.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# limit(n) limita a n el número de filas obtenidas\n",
    "dfSE_10filas: DataFrame = dfSE.sample(False, 0.1, seed=None).limit(10)\n",
    "print(\"N de filas muestreadas = {0}\".format(dfSE_10filas.count()))\n",
    "dfSE_10filas.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar una operación sobre cada una de las filas\n",
    "El método `foreach` aplica una función a cada una de las filas\n",
    "\n",
    "- El DataFrame no se modifica y no se crea ningún otro DataFrame\n",
    "- El `foreach`se ejecuta en los workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "def printid(f: Row) -> None:\n",
    "    print(f[\"Id\"])\n",
    "\n",
    "dfSE_10filas.foreach(printid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccionar columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Crea un nuevo DataFrame seleccionando columnas por nombre\n",
    "dfIdBody: DataFrame = dfSE.select(\"Id\", \"Body\")\n",
    "dfIdBody.show(5)\n",
    "\n",
    "print(\"El objeto dfIdCuerpo es de tipo {0}.\".format(type(dfIdBody)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Otra forma de indicar a las columnas\n",
    "dfIdBody2: DataFrame = dfSE.select(dfSE.Id, dfSE.Body)\n",
    "dfIdBody2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# También es posible indicar objetos de tipo Column\n",
    "from pyspark.sql.column import Column\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "colId: Column = col(\"Id\")\n",
    "colCreaDate: Column = col(\"CreationDate\")\n",
    "print(\"El objeto colId es de tipo {0}.\".format(type(colId)))\n",
    "print(\"El objeto colCreaDate es de tipo {0}.\".format(type(colCreaDate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Y crear un DataFrame a partir de objetos Column, renombrando columnas\n",
    "dfIdFechaCuerpo: DataFrame = dfSE.select(colId,\n",
    "                              colCreaDate.alias(\"Fecha_Creación\"),\n",
    "                              dfSE.Body.alias(\"Cuerpo\"))\n",
    "dfIdFechaCuerpo.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "# El DataFrame anterior usando expresiones\n",
    "dfIdFechaCuerpoExpr: DataFrame = dfSE.select(\n",
    "                           expr(\"Id AS ID\"),\n",
    "                           expr('CreationDate AS `Fecha_Creación`'),\n",
    "                           expr(\"Body AS Cuerpo\"))\n",
    "dfIdFechaCuerpoExpr.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Se pueden usar expresiones más complejas\n",
    "dfSE.selectExpr(\"*\", # Selecciona todas las columnas\n",
    "                \"(AnswerCount IS NOT NULL) as respuestaValida\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renombrar, añadir y eliminar columnas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Renombramos la columna creationDate\n",
    "dfSE: DataFrame = dfSE.withColumnRenamed(\"CreationDate\", \"Fecha_de_creación\")\n",
    "dfSE.select(\"Fecha_de_creación\",\n",
    "            dfSE.ViewCount.alias(\"Número_de_vistas\"),\n",
    "            \"Score\",\n",
    "            \"PostTypeId\")\\\n",
    "            .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Añadimos una nueva columna con todos sus valores iguales a 1\n",
    "from pyspark.sql.functions import lit\n",
    "# lit convierte un literal en Python al formato interno de Spark\n",
    "# (en este ejemplo IntegerType)\n",
    "dfSE: DataFrame = dfSE.withColumn(\"unos\", lit(1))\n",
    "dfSE.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Elimina una columna con drop\n",
    "dfSE: DataFrame = dfSE.drop(col(\"unos\"))\n",
    "dfSE.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminar valores nulos y duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Eliminamos todas las filas que tengan null en alguna de sus columnas\n",
    "dfNoNulls: DataFrame = dfSE.dropna(\"any\")\n",
    "print(\"Numero de filas inicial: {0}; número de filas sin null: {1}.\"\n",
    "       .format(dfSE.count(), dfNoNulls.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Elimina las filas que tengan null en todas sus columnas\n",
    "dfNingunNull: DataFrame = dfSE.dropna(\"all\")\n",
    "print(\"Número de filas con todo a null: {0}.\"\n",
    "       .format(dfSE.count() - dfNingunNull.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Elimina las filas duplicadas\n",
    "dfSinDuplicadas: DataFrame = dfSE.dropDuplicates()\n",
    "print(\"Número de filas duplicadas: {0}.\"\n",
    "       .format(dfSE.count() - dfSinDuplicadas.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Elimina las filas duplicadas en alguna columna\n",
    "dfSinUserDuplicado: DataFrame = dfSE.dropDuplicates([\"OwnerUserId\"])\n",
    "print(\"Número de usuarios únicos: {0}.\"\n",
    "       .format(dfSinUserDuplicado.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Otros ejemplos\n",
    "dfNoNullViewCountAcceptedAnswerId: DataFrame = dfSE\\\n",
    "        .dropna(\"any\", subset=[\"ViewCount\", \"AcceptedAnswerId\"])\n",
    "print(\"Número de filas con ViewCount y AcceptedAnswerId no nulo: {0}.\"\n",
    "       .format(dfNoNullViewCountAcceptedAnswerId.count()))\n",
    "dfNoNullViewCountAcceptedAnswerId = dfSE\\\n",
    "        .dropna(\"all\", subset=[\"ViewCount\", \"AcceptedAnswerId\"])\n",
    "print(\"Número de filas con ViewCount o AcceptedAnswerId no nulo: {0}.\"\n",
    "       .format(dfNoNullViewCountAcceptedAnswerId.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reemplazar valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Reemplazamos los null en los campos ViewCount y AnswerCount\n",
    "dfSE: DataFrame = dfSE.fillna(0, subset=[\"ViewCount\", \"AnswerCount\"])\n",
    "dfSE.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Reemplaza el valor 1 por 2 en una columna nueva añadida\n",
    "\n",
    "dfSE_w_unos: DataFrame = dfSE.withColumn(\"unos\", lit(1))\n",
    "\n",
    "dfSE_w_unos.select(\"Id\", \"unos\").show(10)\n",
    "dfSE_w_unos.replace(1, 2, subset=[\"unos\"])\\\n",
    "    .select(\"Id\", \"unos\")\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardando DataFrames\n",
    "\n",
    "Al igual que con la lectura, Spark puede guardar los DataFrames en múltiples formatos\n",
    "\n",
    "- CSV, JSON, Parquet, Hadoop...\n",
    "\n",
    "También puede escribir en bases de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Guardo el DataFrame dfSE en formato JSON\n",
    "dfSE.write.format(\"json\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save(\"dfSE.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "text/plain",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -lh dfSE.json\n",
    "head dfSE.json/part-*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Guardo el DataFrame usando Parquet\n",
    "dfSE.write.format(\"parquet\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"compression\", \"gzip\")\\\n",
    "    .save(\"dfSE.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(dfSE.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "text/plain",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Parquet usa por defecto formato comprimido snappy\n",
    "ls -lh dfSE.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crean tantos ficheros como particiones tenga el DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfSE2: DataFrame = dfSE.repartition(2)\n",
    "# Guardo el DataFrame  usando Parquet, con compresión gzip\n",
    "dfSE2.write.format(\"parquet\")\\\n",
    "     .mode(\"overwrite\")\\\n",
    "     .option(\"compression\", \"gzip\")\\\n",
    "     .save(\"dfSE2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "text/plain"
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -lh dfSE2.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Particionado\n",
    "Permite particionar los ficheros guardados por el valor de una columna\n",
    "\n",
    "- Se crea un directorio por cada valor diferente en la columna de particionado\n",
    "    - Todos los datos asociados a ese valor se guardan en ese directorio\n",
    "- Permite simplificar el acceso a los valores asociados a una clave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Guardo el DataFrame particionado por el PostTypeId (usando Parquet)\n",
    "dfSE.write.format(\"parquet\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .partitionBy(\"PostTypeId\")\\\n",
    "    .save(\"dfSE-particionado.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "text/plain",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls dfSE-particionado.parquet\n",
    "ls -lh dfSE-particionado.parquet/PostTypeId=2\n",
    "rm -rf dfSE-particionado.parquet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
