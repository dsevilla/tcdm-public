{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD: Resilient Distributed Datasets\n",
    "\n",
    "- Colección inmutable y distribuida de elementos que pueden manipularse en paralelo\n",
    "    - El tipo de datos más básico en Spark\n",
    "- Al igual que con los DataFrames, con los RDDs podemos:\n",
    "    - Crearlos\n",
    "    - Transformarlos en otros RDDs (o DataFrames)\n",
    "    - Realizar acciones para extraer información\n",
    "- Proporcionan un control más fino sobre el particionado y la distribución de datos\n",
    "\n",
    "En Scala y Java las operaciones que se pueden hacer sobre los RDDs y los DataSets son muy parecidas\n",
    "\n",
    "- API Python para RDDs: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html\n",
    "- API Scala para RDDs: https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Elegir el máster de Spark dependiendo de si se ha definido la variable de entorno HADOOP_CONF_DIR o YARN_CONF_DIR\n",
    "SPARK_MASTER: str = 'yarn' if 'HADOOP_CONF_DIR' in os.environ or 'YARN_CONF_DIR' in os.environ else 'local[*]'\n",
    "\n",
    "# Creamos un objeto SparkSession (o lo obtenemos si ya está creado)\n",
    "spark: SparkSession = SparkSession \\\n",
    "  .builder \\\n",
    "  .appName(\"Mi aplicacion\") \\\n",
    "  .config(\"spark.rdd.compress\", \"true\") \\\n",
    "  .config(\"spark.executor.memory\", \"3g\") \\\n",
    "  .config(\"spark.driver.memory\", \"3g\") \\\n",
    "  .master(SPARK_MASTER) \\\n",
    "  .getOrCreate()\n",
    "\n",
    "sc: SparkContext = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de RDDs\n",
    "\n",
    "Podemos crear RDDs a partir de una colección de datos o desde ficheros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs a partir de una colección\n",
    "\n",
    "  - Utilizan el SparkContext (sc en el terminal o el notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Ejemplo en PySpark\n",
    "from pyspark import RDD\n",
    "from pprint import pp\n",
    "\n",
    "rdd1: RDD[int] = sc.parallelize([1,2,3,4,5,6,7,8])\n",
    "print(rdd1.collect())\n",
    "\n",
    "import numpy as np\n",
    "rdd2: RDD[np.int64] = sc.parallelize(np.array(range(100)))\n",
    "pp(rdd2.collect())\n",
    "\n",
    "# Los RDDs aceptan listas de tipos diferentes\n",
    "rdd5: RDD[int | str] = sc.parallelize([1,2,\"tres\",4])\n",
    "pp(rdd5.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs y DataSets a partir de ficheros de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "from pprint import pp\n",
    "\n",
    "# Añadimos un archivo al contexto de Spark (los descarga en cada nodo)\n",
    "sc.addFile(\"https://raw.githubusercontent.com/dsevilla/tcdm-public/refs/heads/24-25/datos/quijote.txt.gz\")\n",
    "\n",
    "quijoteRDD: RDD[str] = sc.textFile(\"file://\" + SparkFiles.get(\"quijote.txt.gz\"))\n",
    "\n",
    "pp(quijoteRDD.take(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particiones\n",
    "\n",
    "El número de particiones de un RDD puede especificarse en el momento de crearse\n",
    "\n",
    "-   También se puede modificar una vez creados (`repartition` o `coalesce`)\n",
    "-   El método `glom` permite ver cómo se han creado las particiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd: RDD[int] = sc.parallelize([1,2,3,4], 2)\n",
    "print(rdd.glom().collect())\n",
    "print(rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs y DataFrames\n",
    "\n",
    "Un DataFrame tiene un RDD subyacente, al que podemos acceder de forma simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "sc.addFile(\"https://raw.githubusercontent.com/dsevilla/tcdm-public/refs/heads/24-25/datos/2015-summary.csv\")\n",
    "\n",
    "dfDatosVuelos2015: DataFrame = (spark\n",
    "    .read\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(\"file://\" + SparkFiles.get(\"2015-summary.csv\")))\n",
    "\n",
    "rddDatosVuelos2015: RDD[Row] = dfDatosVuelos2015.rdd\n",
    "\n",
    "rddDatosVuelos2015.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Se puede crear un DataFrame a partir de un rdd\n",
    "dfNuevo: DataFrame = rddDatosVuelos2015.toDF()\n",
    "dfNuevo.printSchema()\n",
    "dfNuevo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs (y DataSets) simples\n",
    "\n",
    "RDDs formados por elementos simples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones sobre un único RDD o DataSet\n",
    "\n",
    "Generan un nuevo RDD a partir de uno dado modificando cada uno de los elementos del original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `filter(func)` filtra los elementos de un RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "quijsRDD: RDD[str] = quijoteRDD.filter(lambda line: \"Quijote\" in line)\n",
    "sanchsRDD: RDD[str] = quijoteRDD.filter(lambda line: \"Sancho\" in line)\n",
    "quijssancsRDD: RDD[str] = quijsRDD.intersection(sanchsRDD)\n",
    "quijssancsRDD.cache()\n",
    "print(\"Líneas con Quijote y Sancho: {0}.\".format(quijssancsRDD.count()))\n",
    "for line in quijssancsRDD.takeSample(False, 10):\n",
    "    pp(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Obtener los valores positivos de un rango de números\n",
    "rdd: RDD[int] = sc.parallelize(range(-5,5))          # Rango [-5, 5)\n",
    "filtered_rdd: RDD[int] = rdd.filter(lambda x: x >= 0)   # Devuelve los positivos\n",
    "\n",
    "assert filtered_rdd.collect() == [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `map(func)` aplica una función a los elementos de un RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Añade 1 a cada elemento del RDD\n",
    "# Para cada elemento, obtiene una tupla (x, x**2)\n",
    "def add1(x: int) -> int:\n",
    "    return x+1\n",
    "\n",
    "squared_rdd: RDD[tuple] = (filtered_rdd\n",
    "               .map(add1)                 # Añade 1 a cada elemento del RDD\n",
    "               .map(lambda x: (x, x*x)))  # Para cada elemento, obtén una tupla (x, x**2)\n",
    "\n",
    "assert squared_rdd.collect() == [(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `flatMap(func)` igual que `map`, pero “aplana” la salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "squaredflat_rdd: RDD[int] = (filtered_rdd\n",
    "                   .map(add1)\n",
    "                   .flatMap(lambda x: (x, x*x)))  # Da la salida en forma de lista\n",
    "\n",
    "assert squaredflat_rdd.collect() == [1, 1, 2, 4, 3, 9, 4, 16, 5, 25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `sample(withReplacement, fraction, seed=None)` devuelve una muestra del RDD o DataSet\n",
    "    - `withReplacement` - si True, cada elemento puede aparecer varias veces en la muestra\n",
    "    - `fraction` - tamaño esperado de la muestra como una fracción del tamaño del RDD \n",
    "        -  **sin reemplazo**: probabilidad de seleccionar un elemento, su valor debe ser [0, 1]\n",
    "        -  **con reemplazo**: En RDD número esperado de veces que se escoge un elemento, su valor debe ser >= 0\n",
    "    - `seed` - semilla para el generador de números aleatorios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "srdd1: RDD[int] = squaredflat_rdd.sample(False, 0.5)\n",
    "srdd2: RDD[int] = squaredflat_rdd.sample(True, 2)\n",
    "srdd3: RDD[int] = squaredflat_rdd.sample(False, 0.8, 14)\n",
    "print('s1={0}\\ns2={1}\\ns3={2}'.format(srdd1.collect(), srdd2.collect(), srdd3.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `distinct()` devuelve un nuevo RDD o DataSet sin duplicados\n",
    "    - El orden de la salida no está definido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "distinct_rdd: RDD[int] = squaredflat_rdd.distinct()\n",
    "pp(distinct_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `groupBy(func)` devuelve un RDD con los datos agrupados en formato clave/valor, usando una función para obtener la clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "grouped_rdd: RDD[tuple[int, Iterable[int]]] = distinct_rdd.groupBy(lambda x: x%3)\n",
    "pp(grouped_rdd.collect())\n",
    "pp([(x, sorted(y)) for (x,y) in grouped_rdd.collect()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones sobre dos RDDs o DataSets\n",
    "\n",
    "Operaciones tipo conjunto sobre dos RDDs o DataSets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `rdda.union(rddb)` devuelve un RDD o DataSet con la unión de los datos de los dos de partida (se comparan como set porque no se garantiza el orden del resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdda: RDD[str] = sc.parallelize(['a', 'b', 'c'])\n",
    "rddb: RDD[str] = sc.parallelize(['c', 'd', 'e'])\n",
    "rddu: RDD[str] = rdda.union(rddb)\n",
    "assert set(rddu.collect()) == {'a', 'b', 'c', 'c', 'd', 'e'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `rdda.intersection(rddb)` devuelve un RDD o DataSet con los datos comunes en los dos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rddi: RDD[str] = rdda.intersection(rddb)\n",
    "assert rddi.collect() == ['c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `rdda.subtract(rddb)` devuelve un RDD con los datos del primero menos los del segundo (sólo RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdds: RDD[str] = rdda.subtract(rddb)\n",
    "assert set(rdds.collect()) == {'a', 'b'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rdda.cartesian(rddb)` producto cartesiano de ambos RDDs (operación muy costosa) (sólo RDDs) (de nuevo se usan conjuntos para no tener en cuenta el orden exacto si se hacen optimizaciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rddc: RDD[tuple[str, str]] = rdda.cartesian(rddb)\n",
    "assert set(rddc.collect()) == {('a','c'),('a','d'),('a','e'),('b','c'),('b','d'),\n",
    "                          ('b','e'),('c','c'), ('c','d'), ('c','e')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acciones sobre RDDs o DatSets simples\n",
    "\n",
    "Obtienen datos (simples o compuestos) a partir de un RDD o DataSet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principales acciones de agregación: `reduce` y `fold`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `reduce(op)` combina los elementos de un RDD o DataSet en paralelo, aplicando un operador\n",
    "    - El operador de reducción debe ser un *monoide conmutativo* (operador binario asociativo y conmutativo)\n",
    "    - Primero se realiza la redución a nivel de partición y luego se van reduciendo los valores intermedios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd: RDD[int] = sc.parallelize(range(1,10), 2)  # rango [1, 10)\n",
    "pp(rdd.glom().collect())\n",
    "\n",
    "# Reducción con una función lambda\n",
    "p = rdd.reduce(lambda x,y: x*y) # r = 1*2*3*4*5*6*7*8*9 = 362880\n",
    "pp(\"1*2*3*4*5*6*7*8*9 = {0}\".format(p))\n",
    "\n",
    "# Reducción con un operador predefinido\n",
    "from operator import add\n",
    "s = rdd.reduce(add) # s = 1+2+3+4+5+6+7+8+9 = 45\n",
    "pp(\"1+2+3+4+5+6+7+8+9 = {0}\".format(s))\n",
    "\n",
    "# Prueba con un operador no conmutativo\n",
    "p = rdd.reduce(lambda x,y: x-y) # r = 1-2-3-4-5-6-7-8-9 = -43\n",
    "pp(\"1-2-3-4-5-6-7-8-9 = {0}\".format(p))\n",
    "\n",
    "# No funciona con RDDs vacíos\n",
    "#sc.parallelize([]).reduce(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `fold(cero, op)` versión general de `reduce` (sólo con RDDs): \n",
    "    - Debemos proporcionar un valor inicial `cero` para el operador\n",
    "    - El valor inicial debe ser el valor identidad para el operador (p.e. 0 para suma; 1 para producto, o una lista vacía para concatenación de listas)\n",
    "        - Permite utilizar RDDs vacíos\n",
    "    - La función `op` debe ser un monoide conmutativo para garantizar un resultado consistente\n",
    "        - Comportamiento diferente a las operaciones `fold` de lenguajes como Scala\n",
    "        - El operador se aplica a nivel de partición (usando `cero` como valor inicial), y finalmente entre todas las particiones (usando `cero`de nuevo)\n",
    "        - Para operadores no conmutativos el resultado podría ser diferente del obtenido mediante un `fold` secuencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd: RDD[list] = sc.parallelize([[1,2,3,4], [-10, -9, -8, -7, -6], ['a', 'b', 'c']], 2)\n",
    "pp(rdd.glom().collect())\n",
    "\n",
    "f: list[int] = rdd.fold([], lambda x,y: x+y)\n",
    "pp(f)\n",
    "\n",
    "# Se puede hacer un fold de un RDD vacío\n",
    "sc.parallelize([]).fold(0, add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otras acciones de agregación: \n",
    "\n",
    "- `aggregate` (solo RDDs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - `aggregate(cero,seqOp,combOp)`: Devuelve una colección agregando los elementos del RDD usando dos funciones:\n",
    "    1. `seqOp` -  agregación a nivel de partición: se crea un acumulador por partición (inicializado a `cero`) y se agregan los valores de la partición en el acumulador\n",
    "    2. `combOp` - agregación entre particiones: se agregan los acumuladores de todas las particiones\n",
    "    -  Ambas agregaciones usan un valor inicial `cero` (similar al caso de `fold`).\n",
    " - Versión general de `reduce` y `fold`    \n",
    " - La primera función (`seqOp`) puede devolver un tipo, U, diferente del tipo T de los elementos del RDD\n",
    "    - `seqOp` agregar datos de tipo T y devuelve un tipo U\n",
    "    - `combOp` agrega datos de tipo U\n",
    "    - `cero` debe ser de tipo U\n",
    " - Permite devolver un tipo diferente al de los elementos del RDD de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "\n",
    "lista_enteros: list[int] = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "rdd: RDD[int] = sc.parallelize(lista_enteros)\n",
    "\n",
    "# acc es una tupla de tres elementos (List, Double, Int)\n",
    "# En el primer elemento de acc (lista) le concatenamos los elementos del RDD al cuadrado\n",
    "# en el segundo, acumulamos los elementos del RDD usando multiplicación\n",
    "# y en el tercero, contamos los elementos del RDD\n",
    "seqOp: Callable[[tuple, int], tuple]  = (lambda acc, val: (acc[0]+[val*val],\n",
    "                                                           acc[1]*val,\n",
    "                                                           acc[2]+1))\n",
    "# Para cada partición se genera una tupla tipo acc\n",
    "# En esta operación se combinan los tres elementos de las tuplas\n",
    "combOp: Callable[[tuple, tuple], tuple] = (lambda acc1, acc2: (acc1[0]+acc2[0],\n",
    "                                                               acc1[1]*acc2[1],\n",
    "                                                               acc1[2]+acc2[2]))\n",
    "\n",
    "a: tuple = rdd.aggregate(([], 1., 0), seqOp, combOp)\n",
    "\n",
    "pp(a)\n",
    "\n",
    "assert a[1] == 8.*7.*6.*5.*4.*3.*2.*1.\n",
    "assert a[2] == len(lista_enteros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acciones para contar elementos sobre RDDs\n",
    "\n",
    "- `count()` devuelve un entero con el número exacto de elementos del RDD (también DataSets)\n",
    "- `countApprox(timeout, confidence=0.95)` versión aproximada de `count()` que devuelve un resultado potencialmente incompleto en un tiempo máximo, incluso si no todas las tareas han finalizado. (Experimental).\n",
    "    - `timeout` es un entero largo e indica el tiempo en milisegundos\n",
    "    - `confidence` probabilidad de obtener el valor real. Si `confidence` es 0.90 quiere decir que si se ejecuta múltiples veces, se espera que el 90% de ellas se obtenga el valor correcto. Valor [0,1]\n",
    "- `countApproxDistinct(relativeSD=0.05)` devuelve una estimación del número de elementos diferentes del RDD.  (Experimental).\n",
    "    - `relativeSD` – exactitud relativa (valores más pequeños implican menor error, pero requieren más memoria; debe ser mayor que 0.000017).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd: RDD[int] = sc.parallelize([i % 20 for i in range(10000)], 16)\n",
    "print(\"Número total de elementos: {0}.\".format(rdd.count()))\n",
    "print(\"Número de elementos distintos: {0}.\".format(rdd.distinct().count()))\n",
    "\n",
    "print(\"Número total de elementos (aprox.): {0}.\".format(rdd.countApprox(1, 0.4)))\n",
    "print(\"Número de elementos distintos (approx.): {0}.\".format(rdd.countApproxDistinct(0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `countByValue()` devuelve el número de apariciones de cada elemento del RDD como un mapa (o diccionario) de tipo clave/valor\n",
    "    - Las claves son los elementos del RDD y cada valor, el número de ocurrencias de la clave asociada al mismo      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd: RDD[str] = sc.parallelize(list(\"abracadabra\")).cache()\n",
    "mimapa: dict[str, int] = rdd.countByValue()\n",
    "\n",
    "pp(mimapa.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acciones para obtener valores\n",
    "\n",
    "- Estos métodos deben usarse con cuidado, si el resultado esperado es muy grande puede saturar la memoria del _driver_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `collect()` devuelve una lista con todos los elementos del RDD o DataSet\n",
    "-   `show()` muestra los elementos del DataSet como una tabla "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "lista: list[str] = rdd.collect()\n",
    "print(lista)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `take(n)` devuelve los `n` primeros elementos del RDD o las n primeras filas del DataSet\n",
    "-   `takeAsList(n)` devuelve las `n` primeras filas del DataSet como una lista\n",
    "-   `takeSample(withRep, n, [seed])` devuelve `n` elementos aleatorios del RDD\n",
    "    - `withRep`: si True, en la muestra puede aparecer el mismo elemento varias veces\n",
    "    - `seed`: semilla para el generador de números aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "t: list[str] = rdd.take(4)\n",
    "print(t)\n",
    "s: list[str] = rdd.takeSample(False, 4)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `top(n)` devuelve una lista con los primeros `n` elementos del RDD ordenados en orden descendente\n",
    "-   `takeOrdered(n,[orden])` devuelve una lista con los primeros `n` elementos del RDD en orden ascendente (opuesto a `top`), o siguiendo el orden indicado en la función opcional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd: RDD[int] = sc.parallelize([8, 4, 2, 9, 3, 1, 10, 5, 6, 7]).cache()\n",
    "\n",
    "print(f\"4 elementos más grandes: {rdd.top(4)}\")\n",
    "\n",
    "print(f\"4 elementos más pequeños: {rdd.takeOrdered(4)}\")\n",
    "\n",
    "print(f\"4 elementos más grandes: {rdd.takeOrdered(4, lambda x: -x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs con pares clave/valor (aka *Pair RDDs*)\n",
    "\n",
    "-   Tipos de datos muy usados en Big Data (MapReduce)\n",
    "\n",
    "-   Spark dispone de operaciones especiales para su manejo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de *Pair RDDs*\n",
    "Los RDDs clave/valor pueden crearse a partir de una lista de tuplas, a partir de otro RDD o mediante un zip de dos RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   A partir de una lista de tuplas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "prdd: RDD[tuple[str, int]] = sc.parallelize([('a',2), ('b',5), ('a',3)])\n",
    "print(prdd.collect())\n",
    "\n",
    "prdd = sc.parallelize(zip(['a', 'b', 'c'], range(3)))\n",
    "print(prdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   A partir de otro RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Ejemplo usando un fichero\n",
    "# Para cada línea ontenemos una tupla, siendo el primer elemento\n",
    "# la primera palabra de la línes, y el segundo la línea completa\n",
    "linesrdd: RDD[str] = sc.textFile(\"file://\" + SparkFiles.get(\"quijote.txt.gz\"))\n",
    "prdd: RDD[tuple[str, str]] = linesrdd.map(lambda x: (x.split(\" \")[0], x))\n",
    "\n",
    "pp(f\"Par (1ª palabra, línea): {prdd.takeSample(False, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Usando keyBy(f): Crea tuplas de los elementos del RDD usando f para obtener la clave.\n",
    "nrdd: RDD[int] = sc.parallelize(range(2,5))\n",
    "prdd: RDD[tuple[int, int]] = nrdd.keyBy(lambda x: x*x)\n",
    "\n",
    "print(prdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# zipWithIndex(): Zipea el RDD con los índices de sus elementos.\n",
    "rdd: RDD[str] = sc.parallelize(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'], 3)\n",
    "prdd: RDD[tuple[str, int]] = rdd.zipWithIndex()\n",
    "print(rdd.glom().collect())\n",
    "\n",
    "print(prdd.collect())\n",
    "\n",
    "# Este método dispara un Spark job cuando el RDD tiene más de una partición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# zipWithUniqueId(): Zipea el RDD con identificadores únicos (long) para cada elemento.\n",
    "# Los elementos en la partición k-ésima obtienen los ids k, n+k, 2*n+k,... siendo n = nº de particiones\n",
    "# No dispara un trabajo Spark\n",
    "rdd: RDD[str] = sc.parallelize(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'], 3)\n",
    "print(\"Particionado del RDD: {0}\".format(rdd.glom().collect()))\n",
    "prdd: RDD[tuple[str, int]] = rdd.zipWithUniqueId()\n",
    "\n",
    "print(prdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mediante un zip de dos RDDs\n",
    "    - Los RDDs deben tener el mismo número de particiones y el mismo número de elementos en cada partición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd1: RDD[int] = sc.parallelize(range(0, 5), 2)\n",
    "rdd2: RDD[int] = sc.parallelize(range(1000, 1005), 2)\n",
    "prdd: RDD[tuple[int, int]] = rdd1.zip(rdd2)\n",
    "\n",
    "print(prdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones sobre un único RDD clave/valor\n",
    "Sobre un único RDD clave/valor podemos efectuar transformaciones de agregación a nivel de clave y transformaciones que afectan a las claves o a los valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformaciones de agregación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `reduceByKey(func)`/`foldByKey(func)`\n",
    "    -   Devuelven un RDD, agrupando los valores asociados a la misma clave mediante `func`\n",
    "    -   Similares a `reduce` y `fold` sobre RDDs simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "prdd: RDD[tuple[str, int]] = sc.parallelize([('a', 2), ('b', 5), ('a', 8), ('b', 6), ('b', 2),('c',13)],2).cache()\n",
    "print(prdd.glom().collect())\n",
    "redrdd: RDD[tuple[str, int]] = prdd.reduceByKey(add)\n",
    "\n",
    "print(redrdd.sortByKey().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `groupByKey()` agrupa valores asociados a misma clave\n",
    "    - Operación muy costosa en comunicaciones\n",
    "    - Mejor usar operaciones de reducción\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "grouprdd: RDD[tuple[str, Iterable[int]]] = prdd.groupByKey()\n",
    "\n",
    "print(grouprdd.collect())\n",
    "\n",
    "lista: list[tuple[str, list[int]]] = [(k, list(v)) for k, v in grouprdd.collect()]\n",
    "print(lista)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `combineByKey(createCombiner(func1), mergeValue(func2), mergeCombiners(func3))`\n",
    "    - Método general para agregación por clave, similar a `aggregate`\n",
    "    - Especifica tres funciones:\n",
    "\n",
    "     1.  `createCombiner` al recorrer los elementos de cada partición, si nos encontramos una clave nueva se crea un acumulador y se inicializa con `func1`\n",
    "\n",
    "     2.  `mergeValue` mezcla los valores de cada clave en cada partición usando `func2`\n",
    "\n",
    "     3.  `mergeCombiners` mezcla los resultados de las diferentes particiones mediante `func3`\n",
    "\n",
    "- Los valores del RDD de salida pueden tener un tipo diferente al de los valores del RDD de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Para cada clave, obten una tupla que tenga la suma y el número de valores\n",
    "sumCount: RDD[tuple[str, tuple[int,int]]] = prdd.combineByKey(\n",
    "                            (lambda x: (x, 1)),\n",
    "                            (lambda x, y: (x[0]+y, x[1]+1)),\n",
    "                            (lambda x, y: (x[0]+y[0], x[1]+y[1])))\n",
    "\n",
    "print(sumCount.collect())\n",
    "\n",
    "# Con el RDD anterior, obtenemos la media de los valores\n",
    "m: RDD[tuple[str, float]] = sumCount.mapValues(lambda v: float(v[0])/v[1])\n",
    "print(m.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformaciones sobre claves o valores\n",
    "-   `keys()` devuelve un RDD con las claves\n",
    "-   `values()` devuelve un RDD con los valores\n",
    "-   `sortByKey()` devuelve un RDD clave/valor con las claves ordenadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(\"RDD completo: {0}\".format(prdd.collect()))\n",
    "print(\"RDD con las claves: {0}\".format(prdd.keys().collect()))\n",
    "print(\"RDD con los valores: {0}\".format(prdd.values().collect()))\n",
    "print(\"RDD con las claves ordenadas: {0}\".format(prdd.sortByKey().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `mapValues(func)` devuelve un RDD aplicando una función sobre los valores\n",
    "-   `flatMapValues(func)` devuelve un RDD aplicando una función sobre los valores y “aplanando” la salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "mapv: RDD[tuple[str, tuple[int, int]]] = prdd.mapValues(lambda x: (x, 10*x))\n",
    "print(mapv.collect())\n",
    "\n",
    "fmapv: RDD[tuple[str , int]] = prdd.flatMapValues(lambda x: (x, 10*x))\n",
    "print(fmapv.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones sobre dos RDDs clave/valor\n",
    "Combinan dos RDDs de tipo clave/valor para obtener un tercer RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`join`/`leftOuterJoin`/`rightOuterJoin`/`fullOuterJoin` realizan inner/outer/full joins entre los dos RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd1: RDD[tuple[str, int]] = sc.parallelize([(\"a\", 2), (\"b\", 5), (\"a\", 8)]).cache()\n",
    "rdd2: RDD[tuple[str, int]] = sc.parallelize([(\"c\", 7), (\"a\", 1)]).cache()\n",
    "\n",
    "rdd3: RDD[tuple[str, tuple[int, int]]] = rdd1.join(rdd2)\n",
    "\n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd3: RDD[tuple[str, tuple[int, int | None]]] = rdd1.leftOuterJoin(rdd2)\n",
    "\n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd3: RDD[tuple[str, tuple[int | None, int]]] = rdd1.rightOuterJoin(rdd2)\n",
    "\n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd3: RDD[tuple[str, tuple[int | None, int | None]]] = rdd1.fullOuterJoin(rdd2)\n",
    "\n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `subtractByKey` elimina elementos con una clave presente en otro RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd3: RDD[tuple[str, int]] = rdd1.subtractByKey(rdd2)\n",
    "\n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `cogroup` agrupa los datos que comparten la misma clave en ambos RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.resultiterable import ResultIterable\n",
    "\n",
    "rdd3: RDD[tuple[str, tuple[ResultIterable[int], ResultIterable[int]]]] = rdd1.cogroup(rdd2)\n",
    "\n",
    "print(rdd3.collect())\n",
    "\n",
    "map: dict[str, list[list[int]]] = rdd3.mapValues(lambda v: [list(l) for l in v]).collectAsMap()\n",
    "\n",
    "print(map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acciones sobre RDDs clave/valor\n",
    "Sobre los RDDs clave/valor podemos aplicar las acciones para RDDs simples y algunas adicionales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `collectAsMap()` obtiene el RDD en forma de mapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "prdd: RDD[tuple[str, int]] = sc.parallelize([(\"a\", 7), (\"b\", 5), (\"a\", 8)]).cache()\n",
    "\n",
    "Mapa: dict[str, int] = prdd.collectAsMap()\n",
    "print(Mapa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `countByKey()` devuelve un mapa indicando el número de ocurrencias de cada clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "countMap: dict[str, int] = prdd.countByKey()\n",
    "\n",
    "print(countMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `lookup(key)` devuelve una lista con los valores asociados con una clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "listA: list[int] = prdd.lookup('a')\n",
    "\n",
    "print(listA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs numéricos\n",
    "--------------\n",
    "\n",
    "Funciones de estadística descriptiva implementadas en Spark\n",
    "\n",
    "  Método              |  Descripción                       \n",
    "  ------------------- | ---------------------------------- \n",
    "  stats()             | Resumen de estadísticas            \n",
    "  mean()              | Media aritmética\n",
    "  sum(), max(), min() | Suma, máximo y mínimo\n",
    "  variance()          | Varianza de los elementos\n",
    "  sampleVariance()    | Varianza de una muestra\n",
    "  stdev()             | Desviación estándar\n",
    "  sampleStdev()       | Desviación estándar de una muestra\n",
    "  histogram()         | Histograma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.statcounter import StatCounter\n",
    "\n",
    "# Un RDD con datos aleatorios de una distribución normal\n",
    "nrdd: RDD[float] = sc.parallelize(np.random.normal(size=100000)).cache()\n",
    "\n",
    "# Resumen de estadísticas\n",
    "sts: StatCounter = nrdd.stats()\n",
    "\n",
    "print(\"Resumen de estadísticas:\\n {0}\\n\".format(sts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from math import fabs\n",
    "\n",
    "# Filtra outliers\n",
    "stddev = sts.stdev()\n",
    "avg = sts.mean()\n",
    "\n",
    "frdd: RDD[float] = nrdd.filter(lambda x: fabs(x - avg) < 3*stddev).cache()\n",
    "\n",
    "print(\"Número de outliers: {0}\".format(sts.count() - frdd.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "\n",
    "# Obtiene un histograma con 10 grupos\n",
    "x,y = frdd.histogram(10)\n",
    "\n",
    "# Limpia la gráfica\n",
    "plt.gcf().clear()\n",
    "\n",
    "plt.bar(x[:-1], y, width=0.6)\n",
    "plt.xlabel(u'Valores')\n",
    "plt.ylabel(u'Número de ocurrencias')\n",
    "plt.title(u'Histograma')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar RDDs\n",
    "\n",
    "Los RDDs se pueden salvar a disco como ficheros de texto, ficheros Sequence y, en general, tipo de ficheros que pueda escribir Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Salva como fichero de texto\n",
    "# Se crea un fichero por partición\n",
    "nrdd.saveAsTextFile(\"file:///tmp/nrdd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "text/plain",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -lh /tmp/nrdd\n",
    "head /tmp/nrdd/part-00000\n",
    "rm -rf /tmp/nrdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Salva como fichero de texto comprimido\n",
    "nrdd.saveAsTextFile(\"file:///tmp/nrdd-bzip\", \"org.apache.hadoop.io.compress.BZip2Codec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "text/plain",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -lh /tmp/nrdd-bzip\n",
    "rm -rf /tmp/nrdd-bzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardar como fichero Sequence\n",
    "Tiene que ser un RDD clave valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd: RDD[tuple[str, int]] = sc.parallelize([(\"a\",2), (\"b\",5), (\"a\",8)], 2)\n",
    "\n",
    "# Salvamos el RDD clave valor como fichero Sequence\n",
    "rdd.saveAsSequenceFile(\"file:///tmp/rddsequence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "text/plain",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -lh /tmp/rddsequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Recuperamos en otro rdd\n",
    "rdd2: RDD = sc.sequenceFile(\"file:///tmp/rddsequence\",\n",
    "                       \"org.apache.hadoop.io.Text\",\n",
    "                       \"org.apache.hadoop.io.IntWritable\")\n",
    "\n",
    "print(\"Contenido del RDD {0}\".format(rdd2.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "text/plain",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "rm -rf /tmp/rddsequence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
