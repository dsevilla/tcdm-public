{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming\n",
    "\n",
    "Procesamiento escalable, *high-throughput* y tolerante a fallos de flujos de datos.\n",
    "\n",
    "![Flujo de Spark Streaming](http://persoal.citius.usc.es/tf.pena/TCDM/figs/streaming-flow.png)\n",
    "\n",
    "Entrada desde muchas fuentes: Kafka, Flume, Twitter, ZeroMQ, Kinesis o sockets TCP.\n",
    "\n",
    "## APIs SPARK para Streaming\n",
    "\n",
    "- DStream API (no soportada ya)\n",
    "      - API original, basada en RDDs\n",
    "- Structured Streaming\n",
    "      - Disponible desde la versión 2.2, basada en DataFrames\n",
    "\n",
    "Página de Spark Streaming: <https://spark.apache.org/streaming/>\n",
    "Documentación principal (de la última versión): <https://spark.apache.org/docs/latest/streaming-programming-guide.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DStream API\n",
    "\n",
    "Abstracción principal: DStream (*discretized stream*).\n",
    "\n",
    "-   Representa un flujo continuo de datos\n",
    "\n",
    "![dstreams](http://persoal.citius.usc.es/tf.pena/TCDM/figs/dstreams.png)\n",
    "\n",
    "Arquitectura *micro-batch*\n",
    "\n",
    "-   Los datos recibidos se agrupan en batches\n",
    "-   Los batches se crean a intervalos regulares (*batch interval*)\n",
    "-   Cada batch forma un RDD, que es procesado por Spark\n",
    "-   Adicionalmente: transformaciones con estado mediante\n",
    "    -   Operaciones con ventanas\n",
    "    -   Tracking del estado por cada clave\n",
    "\n",
    "Página de Spark Streaming: <https://spark.apache.org/streaming/>\n",
    "Documentación principal (de la última versión): <https://spark.apache.org/docs/latest/streaming-programming-guide.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Elegir el máster de Spark dependiendo de si se ha definido la variable de entorno HADOOP_CONF_DIR o YARN_CONF_DIR\n",
    "SPARK_MASTER: str = 'yarn' if 'HADOOP_CONF_DIR' in os.environ or 'YARN_CONF_DIR' in os.environ else 'local[*]'\n",
    "print(f\"Usando Spark Master en {SPARK_MASTER}\")\n",
    "\n",
    "# Creamos un objeto SparkSession (o lo obtenemos si ya está creado)\n",
    "spark: SparkSession = SparkSession \\\n",
    "  .builder \\\n",
    "  .appName(\"Mi aplicacion\") \\\n",
    "  .config(\"spark.rdd.compress\", \"true\") \\\n",
    "  .config(\"spark.executor.memory\", \"3g\") \\\n",
    "  .config(\"spark.driver.memory\", \"3g\") \\\n",
    "  .master(SPARK_MASTER) \\\n",
    "  .getOrCreate()\n",
    "\n",
    "sc: SparkContext = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Streaming \n",
    "\n",
    "Utiliza la API estructurada (DataFrames, DataSets y SQL)\n",
    "\n",
    "- Lee los datos a medida que llegan al sistema, los procesa y los añade a un DataFrame\n",
    "\n",
    "Fuentes de datos (*input sources*):\n",
    "\n",
    "- [Apache Kafka](https://kafka.apache.org/)\n",
    "- Ficheros (lee los ficheros en un directorio de forma continua)\n",
    "- Sockets\n",
    "\n",
    "Destino de datos (*sinks*):\n",
    "\n",
    "- Apache Kafka\n",
    "- Ficheros\n",
    "- Otras computaciones\n",
    "- Memoria (para depuración y testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: procesar los ficheros en el directorio `by-day/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "text/plain",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "wget -q https://raw.githubusercontent.com/dsevilla/tcdm-public/refs/heads/24-25/datos/by-day.zip\n",
    "unzip by-day.zip\n",
    "# Vemos el formato de un fichero\n",
    "ls by-day/\n",
    "head by-day/2010-12-01.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procesamos un fichero como DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Creamos un DataFrame normal con los datos de uno de los ficheros\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "dfEstatico: DataFrame = spark.read\\\n",
    "                  .format(\"csv\")\\\n",
    "                  .option(\"header\", \"true\")\\\n",
    "                  .option(\"inferSchema\", \"true\")\\\n",
    "                  .load(\"by-day/2010-12-01.csv\")\n",
    "print(\"Número de particiones del DataFrame = {0}.\".\n",
    "      format(dfEstatico.rdd.getNumPartitions()))\n",
    "\n",
    "dfEstatico.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Obtenemos un DataFrame con la compra por hora y por cliente durante ese día\n",
    "from pyspark.sql.functions import window, col, desc\n",
    "\n",
    "# Pedimos que se usen 4 particiones (opcional)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "\n",
    "dfCompraPorClientePorHoraEstatico: DataFrame =\\\n",
    "             dfEstatico.select(\n",
    "                                col(\"CustomerId\"),\n",
    "                                (col(\"UnitPrice\")*col(\"Quantity\")).alias(\"total_cost\"),\n",
    "                                col(\"InvoiceDate\"))\\\n",
    "                       .groupBy(col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 hour\"))\\\n",
    "                       .sum(\"total_cost\")\n",
    "\n",
    "print(\"Número de particiones del DataFrame = {0}.\".\n",
    "      format(dfCompraPorClientePorHoraEstatico.rdd.getNumPartitions()))\n",
    "\n",
    "print(\"Número de filas del DataFrame = {0}.\".\n",
    "      format(dfCompraPorClientePorHoraEstatico.count()))\n",
    "\n",
    "dfCompraPorClientePorHoraEstatico.show(15, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procesamos todos los ficheros en Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Definimos un DataFrame en Streaming que toma como fuente de datos\n",
    "# los ficheros en el directorio by-day/\n",
    "# Indicamos que se lea 1 fichero en cada activación\n",
    "\n",
    "dfStreaming: DataFrame = spark.readStream\\\n",
    "                   .schema(dfEstatico.schema)\\\n",
    "                   .option(\"maxFilesPerTrigger\", 1)\\\n",
    "                   .format(\"csv\")\\\n",
    "                   .option(\"header\", \"true\")\\\n",
    "                   .load(\"by-day/*.csv\")\n",
    "print(type(dfStreaming))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# A partir del anterior, obtenemos la compra por hora y por cliente\n",
    "dfCompraPorClientePorHoraStreaming: DataFrame = \\\n",
    "            dfStreaming.select(\n",
    "                               col(\"CustomerId\"),\n",
    "                              (col(\"UnitPrice\")*col(\"Quantity\")).alias(\"total_cost\"),\n",
    "                               col(\"InvoiceDate\"))\\\n",
    "                       .groupBy(col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 hour\"))\\\n",
    "                       .sum(\"total_cost\")\n",
    "\n",
    "print(type(dfCompraPorClientePorHoraStreaming))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.streaming.readwriter import DataStreamWriter\n",
    "\n",
    "# Creamos un objeto DataStreamWriter para escribir los valores del DataFrame previo\n",
    "# Los valores se escriben a una tabla en memoria\n",
    "# El modo de escritura es \"complete\": se reescribe la salida entera\n",
    "# Los datos se pueden acceder a traves de la tabla compras_por_hora\n",
    "# Se leen los datos de entrada cada segundo\n",
    "\n",
    "dswConsultaCompras: DataStreamWriter = dfCompraPorClientePorHoraStreaming\\\n",
    "                    .writeStream\\\n",
    "                    .format(\"memory\")\\\n",
    "                    .queryName(\"compras_por_hora\")\\\n",
    "                    .outputMode(\"complete\")\\\n",
    "                    .trigger(processingTime='1 seconds')\n",
    "print(type(dswConsultaCompras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Métodos definidos para un DataStreamWriter\n",
    "[method_name for method_name in dir(dswConsultaCompras)\n",
    " if callable(getattr(dswConsultaCompras, method_name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Iniciamos el acceso a los datos de entrada\n",
    "dswConsultaCompras.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Vamos mostrando la tabla cada segundo\n",
    "from time import sleep\n",
    "\n",
    "for x in range(20):\n",
    "    spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM compras_por_hora\n",
    "            ORDER BY `sum(total_cost)` DESC\n",
    "            \"\"\").show(5, truncate=False)\n",
    "    sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
