{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones con DataFrames\n",
    "Veremos distintas operaciones que se pueden hacer con los DataFrames:\n",
    "\n",
    "  - Filtrado de filas\n",
    "  - Ordenación y agrupamiento\n",
    "  - Joins\n",
    "  - Funciones escalares y agregados\n",
    "  - Manejo de tipos complejos\n",
    "  - Funciones de ventana\n",
    "  - Funciones definidas por el usuario\n",
    " \n",
    "Acabaremos viendo como usar consultas SQL sobre DataFrames\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Elegir el máster de Spark dependiendo de si se ha definido la variable de entorno HADOOP_CONF_DIR o YARN_CONF_DIR\n",
    "SPARK_MASTER: str = 'yarn' if 'HADOOP_CONF_DIR' in os.environ or 'YARN_CONF_DIR' in os.environ else 'local[*]'\n",
    "\n",
    "# Creamos un objeto SparkSession (o lo obtenemos si ya está creado)\n",
    "spark: SparkSession = SparkSession \\\n",
    "  .builder \\\n",
    "  .appName(\"Mi aplicacion\") \\\n",
    "  .config(\"spark.rdd.compress\", \"true\") \\\n",
    "  .config(\"spark.executor.memory\", \"3g\") \\\n",
    "  .config(\"spark.driver.memory\", \"3g\") \\\n",
    "  .master(SPARK_MASTER) \\\n",
    "  .getOrCreate()\n",
    "\n",
    "sc: SparkContext = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "rm -rf /tmp/tcdm-public\n",
    "git clone -b 24-25 --single-branch --depth 1 https://github.com/dsevilla/tcdm-public.git /tmp/tcdm-public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Recupero el DataFrame leyéndolo del formato parquet\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "dfSE: DataFrame = spark.read\\\n",
    "            .format(\"parquet\")\\\n",
    "            .option(\"mode\", \"FAILFAST\")\\\n",
    "            .load(\"/tmp/tcdm-public/datos/dfSE.parquet\")\n",
    "dfSE.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfSE.show(5)\n",
    "dfSE.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(dfSE.count() == 410346)\n",
    "dfSE.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones de filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Selecciona los post que tengan la palabra Italiano en su cuerpo\n",
    "from pyspark.sql.column import Column\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "colCuerpo: Column = col(\"Body\")\n",
    "dfConItaliano: DataFrame = dfSE.filter(colCuerpo.like('%Italiano%'))\n",
    "\n",
    "print(\"Número de posts con la palabra Italiano: {0}\\n\"\\\n",
    "      .format(dfConItaliano.count()))\n",
    "\n",
    "assert(dfConItaliano.count() == 32)\n",
    "\n",
    "print(\"Una de las filas\")\n",
    "dfConItaliano.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Obtenemos las preguntas (PostTypeId == 1) que tienen una respuesta aceptada (AcceptedAnswerId != null)\n",
    "# Nota: where() es un alias de filter()\n",
    "\n",
    "postTypeIdCol: Column = col(\"PostTypeId\")\n",
    "acceptedAnswerIdCol: Column = col(\"AcceptedAnswerId\")\n",
    "\n",
    "questionsWithAcceptedAnswersDf: DataFrame = (dfSE\n",
    "                    .where((postTypeIdCol == 1) & (acceptedAnswerIdCol.isNotNull()))\n",
    "                    .withColumnRenamed(\"CreationDate\", \"Fecha_de_creación\"))\n",
    "\n",
    "print(\"Número de preguntas con respuesta aceptada: {0}.\"\\\n",
    "      .format(questionsWithAcceptedAnswersDf.count()))\n",
    "\n",
    "questionsWithAcceptedAnswersDf.cache()\n",
    "\n",
    "(questionsWithAcceptedAnswersDf\n",
    "        .select(\"Fecha_de_creación\", postTypeIdCol.alias(\"Tipo Post\"), acceptedAnswerIdCol)\n",
    "        .show(truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Nos quedamos con las entradas correspondientes a junio de 2016\n",
    "from datetime import date\n",
    "\n",
    "fechaCreacionCol: Column = col(\"Fecha_de_creación\")\n",
    "\n",
    "dfPregConRespAceptJun16: DataFrame = questionsWithAcceptedAnswersDf\\\n",
    "                    .filter((fechaCreacionCol >= date(2016,6,1)) &\n",
    "                            (fechaCreacionCol <= date(2016,6,30)))\n",
    "\n",
    "dfPregConRespAceptJun16.select(fechaCreacionCol, postTypeIdCol, acceptedAnswerIdCol)\\\n",
    "        .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Añadimos una columna que contenga el ratio entre el número de vistas y el score\n",
    "\n",
    "colNumVistas: Column = col(\"ViewCount\")\n",
    "colPuntos: Column = col(\"Score\")\n",
    "dfPregConRespAceptyRatio: DataFrame = \\\n",
    "    questionsWithAcceptedAnswersDf.withColumn(\"ratio\", colNumVistas/colPuntos)\n",
    "\n",
    "# Muestra algunas columnas con ratio > 35\n",
    "colRatio: Column = col(\"ratio\")\n",
    "(dfPregConRespAceptyRatio.filter(colRatio > 35)\n",
    "                        .select(fechaCreacionCol, colNumVistas, colPuntos, colRatio)\n",
    "                        .show(truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones de ordenación y agrupamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Ordenamos por viewCount\n",
    "questionsWithAcceptedAnswersDf.orderBy(colNumVistas.desc())\\\n",
    "                  .select(fechaCreacionCol, colNumVistas)\\\n",
    "                  .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Creamos una agrupación por la columna OwnerUserId\n",
    "from pyspark.sql.group import GroupedData\n",
    "\n",
    "colUserId: Column = col(\"OwnerUserId\")\n",
    "grupoPorUsuario: GroupedData = questionsWithAcceptedAnswersDf.groupBy(colUserId)\n",
    "print(type(grupoPorUsuario))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(\"DataFrame con el número de posts por usuario.\")\n",
    "dfPostPorUsuario: DataFrame = grupoPorUsuario.count()\n",
    "dfPostPorUsuario.printSchema()\n",
    "\n",
    "colNPosts: Column = col(\"count\")\n",
    "dfPostPorUsuario.select(colUserId.alias(\"Número de usuario\"),\n",
    "                        colNPosts.alias(\"Número de posts\"))\\\n",
    "                .orderBy(colNPosts, ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(\"DataFrame con la media de vistas por usuario:\")\n",
    "dfAvgPorUsuario: DataFrame = grupoPorUsuario.avg(\"ViewCount\")\\\n",
    "                    .withColumnRenamed(\"avg(ViewCount)\", \"Media_vistas\")\n",
    "dfAvgPorUsuario.orderBy(\"Media_vistas\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# El método agg permite hacer varias operaciones de agrupamiento, expresadas como un diccionario {nombre_columna:operacion}\n",
    "print(\"Obtenemos las tablas anteriores con una sola operación.\")\n",
    "dfCountyAvg: DataFrame = grupoPorUsuario.agg({\"OwnerUserId\":\"count\", \"ViewCount\":\"avg\"})\n",
    "dfCountyAvg.printSchema()\n",
    "\n",
    "colCount: Column = col(\"count(OwnerUserId)\")\n",
    "colMedia: Column = col(\"avg(ViewCount)\")\n",
    "dfCountyAvg.select(colUserId.alias(\"Número de usuario\"),\n",
    "                   colCount.alias(\"Número de posts\"),\n",
    "                   colMedia.alias(\"Media de vistas\"))\\\n",
    "                  .orderBy(colUserId).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Agrupación sobre dos columnas\n",
    "dfSE.groupBy(colUserId, postTypeIdCol)\\\n",
    "    .count()\\\n",
    "    .orderBy(colUserId.asc(), postTypeIdCol.desc())\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una descripción de las funciones que se pueden usar con GroupedData está en https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensiones del groupBy\n",
    "\n",
    "Funciones `rollup` y `cube`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rollup\n",
    "\n",
    "Incluye filas adicionales con agregados por la primera columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Contar para cada usuario el número de preguntas (PostTypeId = 1) y el número de respuestas (PostTypeId = 2)\n",
    "rollupPorUsuarioyTipoPost: GroupedData = dfSE.rollup(\"OwnerUserId\", \"PostTypeId\")\n",
    "print(type(rollupPorUsuarioyTipoPost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# DataFrame con el número de post por usuario y tipo pregunta\n",
    "# Los campos a null son de agregación, por ejemplo:\n",
    "# null null = todos los posts\n",
    "# 4    null = todos los posts del usuario con id 4\n",
    "# 4    1    = todos los post de tipo 1 del usuario 4\n",
    "dfPostPorUsuarioyTipo: DataFrame = rollupPorUsuarioyTipoPost.count()\n",
    "dfPostPorUsuarioyTipo.printSchema()\n",
    "dfPostPorUsuarioyTipo.select(colUserId.alias(\"Número de usuario\"),\n",
    "                             postTypeIdCol.alias(\"Tipo de post\"),\n",
    "                             colNPosts.alias(\"Número de posts\"))\\\n",
    "                     .orderBy(colUserId, postTypeIdCol)\\\n",
    "                     .show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cubes\n",
    "\n",
    "Similar al Rollups, pero recorriendo todas las dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "grupoPorUsuarioyTipoPost: GroupedData = dfSE.cube(\"OwnerUserId\", \"PostTypeId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# DataFrame con el número de post por usuario y tipo pregunta\n",
    "# Los campos a null son de agregación, por ejemplo:\n",
    "# null null = todas los posts\n",
    "# null 1    = todos los post de tipo 1\n",
    "# 4    null = todos los posts del usuario con id 4\n",
    "# 4    1    = todos los post de tipo 1 del usuario 4\n",
    "dfPostPorUsuarioyTipo: DataFrame = grupoPorUsuarioyTipoPost.count()\n",
    "dfPostPorUsuarioyTipo.printSchema()\n",
    "dfPostPorUsuarioyTipo.select(colUserId.alias(\"Número de usuario\"),\n",
    "                             postTypeIdCol.alias(\"Tipo de post\"),\n",
    "                             colNPosts.alias(\"Número de posts\"))\\\n",
    "                     .orderBy(colUserId,postTypeIdCol)\\\n",
    "                     .show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins\n",
    "Spark ofrece la posibilidad de realizar múltiples tipos de joins\n",
    "\n",
    "  - inner, outer, left outer, right outer, left semi, left anti, cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Buscamos unir las preguntas con respuesta aceptada con la respuesta que se ha elegido como aceptada\n",
    "# Unimos el campo AcceptedAnswerId de las preguntas con el campo id de las respuestas\n",
    "dfPreguntas: DataFrame = questionsWithAcceptedAnswersDf\\\n",
    "                .select(colUserId, colCuerpo, acceptedAnswerIdCol)\\\n",
    "                .withColumnRenamed(\"OwnerUserId\", \"Usuario pregunta\")\\\n",
    "                .withColumnRenamed(\"Body\", \"Pregunta\")\\\n",
    "                .withColumnRenamed(\"AcceptedAnswerId\", \"ID Resp Aceptada\")\n",
    "\n",
    "colId: Column = col(\"Id\")\n",
    "dfRespuestas: DataFrame = dfSE\\\n",
    "                .select(colId, colUserId, colCuerpo)\\\n",
    "                .where(postTypeIdCol == 2)\\\n",
    "                .withColumnRenamed(\"Id\", \"ID Respuesta\")\\\n",
    "                .withColumnRenamed(\"OwnerUserId\", \"Usuario respuesta\")\\\n",
    "                .withColumnRenamed(\"Body\", \"Respuesta\")\n",
    "\n",
    "nPreguntas: int = dfPreguntas.count()\n",
    "AnswerCount: int = dfRespuestas.count()\n",
    "print(\"Número de preguntas con respuesta aceptada = {0}.\".format(nPreguntas))\n",
    "print(\"Número de respuestas = {0}.\".format(AnswerCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Expresión para el join\n",
    "joinExpression: Column = dfPreguntas[\"ID Resp Aceptada\"] == dfRespuestas[\"ID Respuesta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Inner join\n",
    "# Solo se incluyen las filas para las que la joinExpression es true\n",
    "joinType = \"inner\"\n",
    "dfInner: DataFrame = dfPreguntas.join(dfRespuestas, joinExpression, joinType)\n",
    "nFilas = dfInner.count()\n",
    "print(\"Número de filas = {0}.\".format(nFilas))\n",
    "dfInner.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Outer join\n",
    "# Incluye todas las filas de ambos DataFrames.\n",
    "# En el caso de que no haya equivalente el alguno de los DataFrame, se meten nulls\n",
    "joinType = \"outer\"\n",
    "dfOuter: DataFrame = dfPreguntas.join(dfRespuestas, joinExpression, joinType)\n",
    "nFilas: int = dfOuter.count()\n",
    "print(\"Número de filas = {0}.\".format(nFilas))\n",
    "dfOuter.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Left Outer join\n",
    "# Incluye todas las filas del DataFrame de la izquierda (primer DataFrame)\n",
    "# Si no hay equivalencia en el de la derecha, se pone null.\n",
    "joinType = \"left_outer\"\n",
    "dfLOuter: DataFrame = dfPreguntas.join(dfRespuestas, joinExpression, joinType)\n",
    "nFilas = dfLOuter.count()\n",
    "print(\"Número de filas = {0}.\".format(nFilas))\n",
    "dfLOuter.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Right Outer join\n",
    "# Incluye todas las filas del DataFrame de la derecha (segundo DataFrame)\n",
    "# Si no hay equivalencia en el de la izquierda, se pone null.\n",
    "joinType = \"right_outer\"\n",
    "dfROuter: DataFrame = dfPreguntas.join(dfRespuestas, joinExpression, joinType)\n",
    "nFilas = dfROuter.count()\n",
    "print(\"Número de filas = {0}.\".format(nFilas))\n",
    "dfROuter.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Left Semi join\n",
    "# El resultado incluyen los valores del primer DataFrame que existen en el segundo\n",
    "joinType = \"left_semi\"\n",
    "dfLSemi: DataFrame = dfRespuestas.join(dfPreguntas, joinExpression, joinType)\n",
    "nFilas = dfLSemi.count()\n",
    "print(\"Número de filas = {0}.\".format(nFilas))\n",
    "dfLSemi.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Left Anti join\n",
    "# El resultado incluyen los valores del primer DataFrame que NO existen en el segundo\n",
    "joinType = \"left_anti\"\n",
    "dfLAnti: DataFrame = dfRespuestas.join(dfPreguntas, joinExpression, joinType)\n",
    "nFilas = dfLAnti.count()\n",
    "print(\"Número de filas = {0}.\".format(nFilas))\n",
    "dfLAnti.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Cross join\n",
    "# Producto cartesiano, une cada fila del primer DataFrame con todas las del segundo\n",
    "# NO DEBE USARSE, EXTREMADAMENTE COSTOSO\n",
    "dfCross: DataFrame = dfRespuestas.crossJoin(dfPreguntas)\n",
    "# nFilas = dfCross.count()\n",
    "# print(\"Número de filas = {0}.\".format(nFilas))\n",
    "# dfCross.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones escalares y agregados\n",
    "\n",
    "Spark ofrece un ámplio abanico de funciones para operar con los DataFrames:\n",
    "- Funciones matemáticas: ``abs``, ``log``, ``hypot``, etc.\n",
    "- Operaciones con strings: ``lenght``, ``concat``, etc.\n",
    "- Operaciones con fechas: ``year``, ``date_add``, etc.\n",
    "- Operaciones de agregación: ``min``, ``max``, ``count``, ``avg``, ``sum``, ``sumDistinct``, ``stddev``, ``variance``, ``kurtosis``, ``skewness``, ``first``, ``last``, ``window``, etc.\n",
    "\n",
    "Una descripción de estas funciones se puede encontrar en <https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, col\n",
    "from pyspark.sql import Row\n",
    "\n",
    "colUltimaActividad: Column = col(\"LastActivityDate\")\n",
    "fechaCreacionCol: Column = col(\"Fecha_de_creación\")\n",
    "# Buscamos la pregunta con respuesta aceptada que estuvo más tiempo activa\n",
    "# (con la mayor diferencia entre los valores de LastActivityDate y Fecha de creacion)\n",
    "masActiva: Row | None = questionsWithAcceptedAnswersDf\\\n",
    "            .withColumn(\"tiempoActiva\",\n",
    "                        datediff(colUltimaActividad,\n",
    "                                 fechaCreacionCol))\\\n",
    "            .orderBy(\"tiempoActiva\", ascending=False)\\\n",
    "            .head()\n",
    "print(\"La pregunta \\n\\n{0}\\n\\nestuvo activa {1} días.\".\\\n",
    "      format(masActiva.Body.replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\"), masActiva.tiempoActiva))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "\n",
    "# Obtenemos el número de post por semana de cada usuario\n",
    "# Agrupamos por OwnerUserId y una ventana de fechas de creación de 1 semana\n",
    "questionsWithAcceptedAnswersDf.groupBy(\n",
    "                   colUserId, window(fechaCreacionCol, \"1 week\").alias(\"Semana\"))\\\n",
    "                  .count()\\\n",
    "                  .sort(\"count\", ascending=False)\\\n",
    "                  .show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Buscar la media y máximo de la columna \"Score\" de todas las filas y el número total del DataFrame completo.\n",
    "dfSE.select(F.avg(colPuntos), F.max(colPuntos), F.count(colPuntos)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Otra forma usando describe\n",
    "dfSE.select(colPuntos).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipos complejos\n",
    "\n",
    "Spark permite trabajar con tres tipos de datos complejos: `structs`, `arrays` y `maps`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structs\n",
    "\n",
    "DataFrames dentro de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct,col\n",
    "# Creamos un nuevo DF con una columna que combina dos columnas existentes\n",
    "colId: Column = col(\"id\")\n",
    "colNumVistas: Column = col(\"ViewCount\")\n",
    "colNRespuestas: Column = col(\"AnswerCount\")\n",
    "dfStruct: DataFrame = dfSE.select(colId, colNumVistas, colNRespuestas,\n",
    "                struct(colNumVistas, colNRespuestas).alias(\"Vistas_Respuestas\"))\n",
    "dfStruct.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfStruct.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Obtenemos un campo de la columna compuesta\n",
    "dfStruct.select(col(\"Vistas_Respuestas\").getField(\"ViewCount\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrays\n",
    "\n",
    "Permiten trabajar con datos como si fuera un array Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ejemplo*\n",
    "\n",
    "Obtener el número de *tags* para cada pregunta con respuesta aceptada y eliminar los símbolos ``&lt;`` y ``&gt;``\n",
    "\n",
    "  - Las \"tags\" de cada pregunta se guardan concatenadas, separadas por < y >, codificados como ``&lt;`` y ``&gt;``\n",
    "\n",
    "`&lt;english-comparison&gt;&lt;translation&gt;&lt;phrase-request&gt;`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Obtenemos un DataFrame sin tags nulas\n",
    "dfSE.show(10)\n",
    "dfNoNullTags = dfSE.dropna(\"any\", subset=[\"Tags\"])\n",
    "dfNoNullTags.select(\"Tags\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Añado una columna con las etiquetas separadas\n",
    "from pyspark.sql.functions import split,replace\n",
    "colTags: Column = col(\"Tags\")\n",
    "dfTags: DataFrame = dfNoNullTags.withColumn(\"tag_array\", split(colTags, \"><\"))\n",
    "dfTags.select(col(\"tag_array\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfTags.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size\n",
    "# Mostramos el número de etiquetas de cada entrada\n",
    "colTag_array: Column = col(\"tag_array\")\n",
    "dfTags.select(colTag_array, size(colTag_array)).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Mostramos la segunda etiqueta de cada entrada\n",
    "dfTags.selectExpr(\"tag_array\", \"tag_array[1]\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "# Miramos si en las tags aparece la palabra \"usage\"\n",
    "dfTags.withColumn(\"Con_usage\", array_contains(colTag_array, \"<usage\"))\\\n",
    "      .select(colTag_array, col(\"Con_usage\")).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "# Convertimos cada etiqueta en una fila\n",
    "dfTagsRows: DataFrame = dfTags.withColumn(\"Tags2\", explode(colTag_array))\n",
    "dfTagsRows.select(colTags, col(\"Tags2\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Elimina los símbolos &lt; y &gt; de las etiquetas (de otra forma diferente a la anterior)\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "dfTags: DataFrame = dfTagsRows.withColumn(\"Tags_separadas\",\n",
    "            regexp_replace(\"Tags2\", \"[<>]\", \"\")).drop(\"Tags2\")\n",
    "dfTags.select(colTags, col(\"Tags_separadas\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Número de entradas con la etiqueta word-choice\n",
    "print(\"Número de entradas con la etiqueta word-choice = {0}.\"\n",
    "      .format(dfTags\n",
    "        .filter(col(\"Tags_separadas\") == \"word-choice\")\n",
    "        .count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de ventana\n",
    "\n",
    "Similares a las de funciones de agregación, permiten operar en grupos de filas devolviendo un único valor para cada fila. Esto permite, entre otras cosas:\n",
    "\n",
    "  - Obtener medias móviles\n",
    "  - Calcular sumas acumuladas\n",
    "  - Acceder a los valores de una fila por encima de la actual\n",
    "\n",
    "Básicamente, una función de ventana (window function) calcula un valor para cada fila de entrada de una tabla en base a un grupo de filas, denominado *frame*.\n",
    "\n",
    "Como funciones de ventana se puede usar las funciones de agregación ya comentadas y otras funciones adicionales (``cume_dist``, ``dense_rank``, ``lag``, ``lead``, ``ntile``, ``percent_rank``, ``rank``, ``row_number``) especificadas como *Window functions* en <https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/window.html>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo 1\n",
    "A partir del DataFrame ``dfPregConRespAcept``, mostrar la puntuación (columna \"Score\") máxima por usuario, y, para cada pregunta, la diferencia de su puntuación con el máximo del usuario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window, WindowSpec\n",
    "\n",
    "# Especificamos la ventana que particiona las filas por la columna OwnerUserId\n",
    "ventana: WindowSpec = Window.partitionBy(colUserId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Creamos una columna con los máximos valores de Score por usuario\n",
    "colMaxPuntos: Column = F.max(colPuntos).over(ventana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Obtenemos un nuevo DataFrame incluyendo la puntuación máxima por usuario\n",
    "# y la diferencia entre este máximo y la puntuación de cada pregunta\n",
    "questionsWithAcceptedAnswersDf.select(colUserId, colId.alias(\"Pregunta\"),\n",
    "                          colPuntos, colMaxPuntos.alias(\"maxPorUsuario\"))\\\n",
    "                  .withColumn(\"Diferencia\", colMaxPuntos-colPuntos)\\\n",
    "                  .orderBy(colUserId, colId)\\\n",
    "                  .show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo 2\n",
    "Mostrar para cada usuario y pregunta del DataFrame ``dfPregConRespAcept`` el número de días que pasaron desde la anterior pregunta del usuario hasta la actual, y desde esta hasta la siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Especificamos la ventana que particiona las filas por la columna OwnerUserId y las ordena por fecha de creación\n",
    "from pyspark.sql.window import WindowSpec\n",
    "\n",
    "ventana: WindowSpec = Window.partitionBy(colUserId).orderBy(fechaCreacionCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Creamos una columna que referencia a la pregunta anterior por fecha\n",
    "colAnterior: Column = F.lag(fechaCreacionCol, 1).over(ventana)\n",
    "# Creamos una columna que referencia a la pregunta posterior por fecha\n",
    "colPosterior: Column = F.lead(fechaCreacionCol, 1).over(ventana)\n",
    "\n",
    "# Mostramos para cada usuario y pregunta el id de la pregunta anterior y posterior\n",
    "questionsWithAcceptedAnswersDf.select(colUserId, colId, fechaCreacionCol.alias(\"Fecha de creación\"),\n",
    "                          F.datediff(fechaCreacionCol,colAnterior).alias(\"Días desde\"),\n",
    "                          F.datediff(colPosterior,fechaCreacionCol).alias(\"Días hasta\"))\\\n",
    "                  .orderBy(colUserId, colId)\\\n",
    "                  .show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones definidas por el usuario (UDFs)\n",
    "\n",
    "Si queremos una función que no está implementada, podemos crear nuestra propia función que opere sobre columnas.\n",
    "\n",
    "  - Las UDFs en Python pueden ser bastante ineficientes, debido a la serialización de datos a Python\n",
    "  - Preferible programarlas en Scala o Java (se pueden usar desde Python)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo\n",
    "\n",
    "Usar UDFs para obtener el número de *tags* para cada pregunta y cambiar los ``&lt;`` y ``&gt;`` por < y >\n",
    "\n",
    "  - Las \"tags\" de cada pregunta se guardan concatenadas, separadas por < y >\n",
    "\n",
    "`&lt;english-comparison&gt;&lt;translation&gt;&lt;phrase-request&gt;`\n",
    "\n",
    "Para contar el número de tags, basta con contar el número de apariciones de ``&lt;`` en el string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "colTags: Column = col(\"Tags\")\n",
    "# Obtenemos un DataFrame sin tags nulas\n",
    "dfNoNullTags: DataFrame = dfSE.dropna(\"any\", subset=[\"Tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Se puede hacer de dos formas, o bien con una función o\n",
    "# con una anotación @udf\n",
    "\n",
    "# Definimos una función que devuelva el número de &lt; en un string\n",
    "@udf(returnType=IntegerType())\n",
    "def udfCuentaTags(tags):\n",
    "    return tags.count('&lt;')\n",
    "\n",
    "# Definimos una función que reemplace &lt y &gt por < y >\n",
    "def reemplazaTags(tags):\n",
    "    return tags.replace('&lt;', '<').replace('&gt;', '>')\n",
    "\n",
    "# Creamos udfs a partir de esta última función\n",
    "udfReemplazaTags= udf(reemplazaTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "dfNoNullTags.select(udfReemplazaTags(colTags).alias(\"Etiquetas\"),\\\n",
    "                    udfCuentaTags(colTags).alias(\"nEtiquetas\"))\\\n",
    "                  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Llamo a las UDFs Scala usando una expresión (si estuvieran definidas en Scala)\n",
    "#dfNoNullTags.selectExpr(\"udfReemplazaTagsSc(Tags) AS Etiquetas\",\n",
    "#                              \"udfCuentaTagsSc(Tags) AS nEtiquetas\")\\\n",
    "#                  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de sentencias SQL\n",
    "\n",
    "Las sentencias SQL ejecutadas desde Spark se trasladan a operaciones sobre DataFrames\n",
    "\n",
    " - Se pueden ejecutar sentencias remotas a través del servidor JDBC/ODBC [Thrift](https://spark.apache.org/docs/latest/sql-programming-guide.html#distributed-sql-engine)\n",
    " - También puede trabajar con datos almacenados en [Apache Hive](https://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables)\n",
    "\n",
    "Para usar sentencias SQL sobre un DataFrame, este tiene que registrarse como una *tabla* o *vista*\n",
    "\n",
    " - la vista puede crearse como temporal (desaparece al terminar la sesión) o global (se mantiene entre sesiones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Registra el DataFrame dfPregConRespAcept como una vista temporal\n",
    "questionsWithAcceptedAnswersDf\\\n",
    "    .createOrReplaceTempView(\"tabla_PregConRespAcept\")\n",
    "\n",
    "# Crea una tabla con los datops guardados en Parquet\n",
    "spark.sql(\"\"\"CREATE TABLE tabla_SE\n",
    "             USING PARQUET OPTIONS (path '/tmp/tcdm-public/datos/dfSE.parquet')\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM tabla_SE\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Ejecuta un comando SQL sobre la tabla\n",
    "dfUser100: DataFrame = spark.sql(\"\"\"SELECT OwnerUserId,Id FROM tabla_SE\n",
    "                         WHERE OwnerUserId >= 100\"\"\")\n",
    "dfUser100.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Podemos ver las tablas creadas\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Podemos crear un nuevo DataFrame a partir de una de la tablas\n",
    "dfFromTable = spark.sql(\"SELECT * FROM tabla_PregConRespAcept\")\n",
    "dfFromTable.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS tabla_PregConRespAcept\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS tabla_SE\")\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
