{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistencia y particionado\n",
    "En este tema trataremos dos aspectos de Apache Spark\n",
    "\n",
    "- **Persistencia**: cómo guardar DataFrames y RDDs de forma que no tengan que ser recalculados\n",
    "- **Particionado**: cómo especificar y cambiar las particiones de un DataFrame o RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistencia\n",
    "\n",
    "Problema al usar un DataFrame o un RDD varias veces:\n",
    "\n",
    "-   Spark recomputa el RDD y sus dependencias cada vez que se ejecuta una acción\n",
    "-   Muy costoso (especialmente en problemas iterativos)\n",
    "\n",
    "Solución\n",
    "\n",
    "-   Conservar el DataFrame o RDD en memoria y/o disco\n",
    "-   Métodos `cache()` o `persist()`\n",
    "\n",
    "### Niveles de persistencia (definidos en [`pyspark.StorageLevel`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.StorageLevel.html) y [`org.apache.spark.storage.StorageLevel`](https://spark.apache.org/docs/3.5.1/api/scala/org/apache/spark/storage/StorageLevel$.html))\n",
    "\n",
    " Nivel                | Espacio  | CPU     | Memoria/Disco   | Descripción\n",
    " :------------------: | :------: | :-----: | :-------------: | ------------------\n",
    " MEMORY_ONLY          |   Alto   |   Bajo  |     Memoria     | Guarda el RDD como un objeto Java no serializado en la JVM. Si el RDD no cabe en memoria, algunas particiones no se *cachearán* y serán recomputadas \"al vuelo\" cada vez que se necesiten. \n",
    " MEMORY_AND_DISK      |   Alto   |   Medio |     Ambos       | Guarda el RDD como un objeto Java no serializado en la JVM. Si el RDD no cabe en memoria, las particiones que no quepan se guardan en disco y se leen del mismo cada vez que se necesiten.\n",
    " DISK_ONLY            |   Bajo   |   Alto  |     Disco       | Guarda las particiones del RDD solo en disco.\n",
    " OFF_HEAP             |   Bajo   |   Alto  |   Memoria       | Similar a MEMORY_ONLY_SER pero guarda el RDD serializado usando memoria *off-heap* (fuera del heap de la JVM) lo que puede reducir el overhead del recolector de basura.\n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "### Nivel de persistencia\n",
    "\n",
    "-   El nivel por defecto para DataFrames es MEMORY_ONLY\n",
    "\n",
    "-   En Python, los datos siempre se guardan en memoria serializados (usando *pickled*)\n",
    "\n",
    "    - Es posible especificar serialización (la forma en la que se serializan los datos para mantenerlos en memoria o en disco). Por defecto se utiliza el serializador \"Pickle\" en Python\n",
    "\n",
    "    \n",
    "### Recuperación de fallos\n",
    "\n",
    "-   Si falla un nodo con datos almacenados, el DataFrame o RDD se recomputa\n",
    "\n",
    "    -   Añadiendo `_2` (ó `_3`) al nivel de persistencia (por ejemplo, MEMORY_ONLY_2), se guardan 2 copias del RDD\n",
    "        \n",
    "### Gestión de la cache\n",
    "\n",
    "-   Algoritmo LRU (Least Recently Used) para gestionar la cache\n",
    "\n",
    "    -   Para niveles *solo memoria*, los RDDs viejos se eliminan y se recalculan\n",
    "    -   Para niveles *memoria y disco*, las particiones que no caben se escriben a disco\n",
    "    \n",
    "### Importante:\n",
    "\n",
    "- La persistencia debe usarse solo cuando sea necesaria, puesto que puede implicar un coste importante\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistencia con DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Elegir el máster de Spark dependiendo de si se ha definido la variable de entorno HADOOP_CONF_DIR o YARN_CONF_DIR\n",
    "SPARK_MASTER: str = 'yarn' if 'HADOOP_CONF_DIR' in os.environ or 'YARN_CONF_DIR' in os.environ else 'local[*]'\n",
    "print(f\"Usando Spark Master en {SPARK_MASTER}\")\n",
    "\n",
    "# Creamos un objeto SparkSession (o lo obtenemos si ya está creado)\n",
    "spark: SparkSession = SparkSession \\\n",
    "  .builder \\\n",
    "  .appName(\"Mi aplicacion\") \\\n",
    "  .config(\"spark.rdd.compress\", \"true\") \\\n",
    "  .config(\"spark.executor.memory\", \"3g\") \\\n",
    "  .config(\"spark.driver.memory\", \"3g\") \\\n",
    "  .master(SPARK_MASTER) \\\n",
    "  .getOrCreate()\n",
    "\n",
    "sc: SparkContext = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "from numpy import dtype, float64, ndarray\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "np_array: ndarray[Any, dtype[float64]] = np.random.random_sample(100000) # generates an array of M random values\n",
    "row_type = Row(\"n\", \"x\")\n",
    "lista: list[Row] = [row_type(i, float(f)) for (i,f) in enumerate(np_array)]\n",
    "DF1: DataFrame = spark.createDataFrame(lista)\n",
    "\n",
    "DF1.printSchema()\n",
    "print(\"Cacheado: {0}.\".format(DF1.is_cached))\n",
    "print(\"Nivel sin persistencia: {0}.\".format(DF1.storageLevel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "DF1.cache()\n",
    "print(\"Cacheado: {0}.\".format(DF1.is_cached))\n",
    "print(\"Nivel de persistencia por defecto: {0}.\".format(DF1.storageLevel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# La persistencia no se hereda en las transformaciones\n",
    "DF2: DataFrame = DF1.groupBy(\"x\").count()\n",
    "print(\"Cacheado: {0}.\".format(DF2.is_cached))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Para cambiar el nivel de persistencia, primero tenemos que quitarlo de la cache\n",
    "DF1.unpersist()\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "DF1.persist(StorageLevel.MEMORY_ONLY_2)\n",
    "print(\"Cacheado: {0}.\".format(DF1.is_cached))\n",
    "print(\"Número de particiones: {0}.\".format(DF1.rdd.getNumPartitions()))\n",
    "print(\"Nuevo nivel de persistencia: {0}.\".format(DF1.storageLevel))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistencia con RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark import RDD\n",
    "\n",
    "rdd: RDD[int] = sc.parallelize(range(1000), 10)\n",
    "print(\"Cacheado: {0}\".format(rdd.is_cached))\n",
    "print(\"Particiones: {0}\".format(rdd.getNumPartitions()))\n",
    "print(\"Nivel de persistencia sin cachear: {0}\".format(rdd.getStorageLevel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd.cache()\n",
    "\n",
    "print(\"Cacheado: {0}\".format(rdd.is_cached))\n",
    "print(\"Nivel de persistencia por defecto: {0}\".format(rdd.getStorageLevel()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particionado\n",
    "\n",
    "El número de particiones es función del tamaño del cluster o el número de bloques del fichero en HDFS\n",
    "\n",
    "-   Es posible ajustarlo al crear u operar sobre un RDD\n",
    "    - Los RDDs ofrecen un mayor control sobre el particionado   \n",
    "-   En DataFrames es posible modificarlo una vez creados\n",
    "-   El paralelismo de RDDs que derivan de otros depende del de sus RDDs padre\n",
    "-   Propiedades útiles:\n",
    "    -    `spark.default.parallelism` Para RDDs, número de particiones por defecto devueltos por transformaciones como `parallelize`, `join` y `reduceByKey`\n",
    "        - Fijado para un `SparkContext`\n",
    "        - La propiedad `sc.defaultParallelism` indica su valor\n",
    "    -    `spark.sql.shuffle.partitions` Para DataFrames, número de particiones a usar al barajar datos en transformaciones *wide*\n",
    "        - Puede cambiarse usando `spark.conf.set`\n",
    "-   Funciones útiles:\n",
    "    -   `rdd.getNumPartitions()` devuelve el número de particiones del RDD\n",
    "    -   `rdd.glom()` devuelve un nuevo RDD juntando los elementos de cada partición en una lista\n",
    "    -   `repartition(n)` devuelve un nuevo DataFrame o RDD que tiene exactamente `n` particiones\n",
    "    -   `coalesce(n)` más eficiente que `repartition`, minimiza el movimiento de datos\n",
    "        - Sólo permite reducir el número de particiones\n",
    "    - `partitionBy(n,[partitionFunc])` Particiona por clave, usando una función de particionado (por defecto, un hash de la clave)\n",
    "        - Solo para RDDs clave/valor\n",
    "        - Asegura que los pares con la misma clave vayan a la misma partición\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particiones y RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(\"Número de particiones por defecto para RDDs: {0}.\"\n",
    "       .format(sc.defaultParallelism))\n",
    "rdd: RDD[int] = sc.parallelize([1, 2, 3, 4, 2, 4, 1], 2)\n",
    "pairs: RDD[tuple[int, int]] = rdd.map(lambda x: (x, x*x))\n",
    "\n",
    "print(\"RDD pairs = {0}.\".format(pairs.collect()))\n",
    "print(\"Particionado de pairs: {0}.\".format(pairs.glom().collect()))\n",
    "print(\"Número de particiones de pairs = {0}.\".format(pairs.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Reducción manteniendo el número de particiones\n",
    "from operator import add\n",
    "print(\"Reducción manteniendo particiones: {0}.\".format(\n",
    "        pairs.reduceByKey(add).glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Reducción modificando el número de particiones\n",
    "print(\"Reducción con 3 particiones: {0}.\".format(\n",
    "       pairs.reduceByKey(add, 3).glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de reparitición\n",
    "pairs4: RDD[tuple[int, int]] = pairs.repartition(4)\n",
    "print(\"pairs4 con {0} particiones: {1}.\".format(\n",
    "        pairs4.getNumPartitions(),\n",
    "        pairs4.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de coalesce\n",
    "pairs2: RDD[tuple[int, int]] = pairs4.coalesce(2)\n",
    "print(\"pairs2 con {0} particiones: {1}.\".format(\n",
    "        pairs2.getNumPartitions(),\n",
    "        pairs2.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Particionando por clave\n",
    "pairs_clave: RDD[tuple[int, int]] = pairs2.partitionBy(3)\n",
    "print(\"Particionado por clave ({0} particiones): {1}.\".format(\n",
    "        pairs_clave.getNumPartitions(),\n",
    "        pairs_clave.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Usando una función de particionado\n",
    "def particionadoParImpar(clave):\n",
    "    if clave%2:\n",
    "        return 0  # Las claves impares van a la partición 0\n",
    "    else:\n",
    "        return 1  # Las claves pares van a la partición 1\n",
    "\n",
    "pairs_parimpar: RDD[tuple[int, int]] = pairs2.partitionBy(2, particionadoParImpar)\n",
    "print(\"Particionado por clave ({0} particiones): {1}.\".format(\n",
    "        pairs_parimpar.getNumPartitions(),\n",
    "        pairs_parimpar.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particiones y DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Convertimos el RDD en un DataFrame\n",
    "dfPairs: DataFrame = pairs.toDF()\n",
    "dfPairs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# El DataFrame hereda el número de particiones del RDD\n",
    "print(\"Número de particiones del DataFrame: {0}.\"\n",
    "      .format(dfPairs.rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Una transformación narrow mantiene el número de particiones\n",
    "print(\"Número de particiones tras transformacion narrow: {0}.\"\n",
    "      .format(dfPairs.replace(1, 2).rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Una transformación wide no mantiene el número de particiones\n",
    "print(\"Número de particiones tras transformacion wide: {0}.\"\n",
    "      .format(dfPairs.sort(\"_1\").rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Es posible especificar el número de particiones a usar en la transformación wide\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 2)\n",
    "print(\"Número de particiones tras transformacion wide: {0}.\"\n",
    "      .format(dfPairs.sort(\"_1\").rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajando a nivel de partición\n",
    "\n",
    "Una operación `map` se hace para cada elemento de un RDD (o una `foreach` para cada fila del DataFrame)\n",
    "\n",
    "-   Puede implicar operaciones redundantes (p.e. abrir una conexión a una BD)\n",
    "\n",
    "-   Puede ser poco eficiente\n",
    "\n",
    "Se pueden hacer `map` y `foreach` una vez por partición:\n",
    "\n",
    "-   Métodos `mapPartitions()`, `mapPartitionsWithIndex()` y `foreachPartition()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de mapPartitions\n",
    "from collections.abc import Iterable\n",
    "\n",
    "nums: RDD[int] = sc.parallelize([1,2,3,4,5,6,7,8,9], 4)\n",
    "print(nums.glom().collect())\n",
    "\n",
    "def sumayCuenta(iter: Iterable[int]) -> list[int]:\n",
    "    sumaCuenta: list[int] = [0,0]\n",
    "    for i in iter:\n",
    "        sumaCuenta[0] += i\n",
    "        sumaCuenta[1] += 1\n",
    "    return sumaCuenta\n",
    "\n",
    "# Llama a la función sumayCuenta una vez por cada partición\n",
    "# El iterador incluye los valores de la partición\n",
    "print(nums.mapPartitions(sumayCuenta).glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de mapPartitionsWithIndex\n",
    "def sumayCuentaIndex(index: int, iter: Iterable[int]) -> tuple[str, list[int]]:\n",
    "    return \"Partición \"+str(index), sumayCuenta(iter)\n",
    "\n",
    "# index es el número de partición\n",
    "print(nums.mapPartitionsWithIndex(sumayCuentaIndex).glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Ejemplo de foreachPartition\n",
    "def f(iter: Iterable[int]) -> None:\n",
    "    _, tempname = tempfile.mkstemp(dir=tempdir, text=True)\n",
    "    with open(tempname, 'w') as fich:\n",
    "        for x in iter:\n",
    "            fich.write(str(x)+\"\\t\")\n",
    "\n",
    "tempdir = \"/tmp/foreachPartition\"\n",
    "\n",
    "if not os.path.exists(tempdir):\n",
    "    os.mkdir(tempdir)\n",
    "    # Para cada partición del RDD se crea un fichero temporal\n",
    "    # y se escribe en ese fichero los valores de la partición\n",
    "    # foreachPartitions permite efectos colaterales (que no permite mapPartition)\n",
    "    # y no devuelve nada\n",
    "    nums.foreachPartition(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "text/plain",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "TEMP=/tmp/foreachPartition\n",
    "echo \"Ficheros creados\"\n",
    "ls -l $TEMP\n",
    "echo\n",
    "echo \"Contenido de los ficheros\"\n",
    "for f in $TEMP/*;do cat $f; echo; echo \"========\"; done\n",
    "rm -rf $TEMP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
