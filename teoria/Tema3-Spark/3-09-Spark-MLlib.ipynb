{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark MLlib: Machine Learning Library\n",
    "\n",
    "Librería de algoritmos paralelos de ML para datos masivos\n",
    "\n",
    "-   Algoritmos clásicos de machine learning: clasificación, regresión, clustering, filtrado colaborativo\n",
    "-   Otros algoritmos: extracción de características, transformación, reducción de dimensionalidad y selección\n",
    "-   Herramientas para construir, evaluar y ajustar pipelines de ML\n",
    "-   Otras utilidades: álgebra lineal, estadística, manejo de datos, etc.\n",
    "\n",
    "\n",
    "Dos paquetes:\n",
    "\n",
    "-   spark.mllib: API original, basada en RDDs\n",
    "    - En *mantenimiento*  \n",
    "-   spark.ml: API de alto nivel, basada en DataFrames\n",
    "\n",
    "Documentación y APIs:\n",
    "\n",
    "- ML\n",
    "    - Guia: http://spark.apache.org/docs/latest/ml-guide.html\n",
    "    - API Python: https://spark.apache.org/docs/latest/api/python/pyspark.ml.html\n",
    "    - API Scala: https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.package\n",
    "- MLlib\n",
    "    - Guia: http://spark.apache.org/docs/latest/mllib-guide.html, con APIs Python y Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo\n",
    "\n",
    "Usa el algoritmo de clustering [KMeans](http://spark.apache.org/docs/latest/mllib-clustering.html#k-means) para agrupar datos de vectores dispersos en dos clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Elegir el máster de Spark dependiendo de si se ha definido la variable de entorno HADOOP_CONF_DIR o YARN_CONF_DIR\n",
    "SPARK_MASTER: str = 'yarn' if 'HADOOP_CONF_DIR' in os.environ or 'YARN_CONF_DIR' in os.environ else 'local[*]'\n",
    "\n",
    "# Creamos un objeto SparkSession (o lo obtenemos si ya está creado)\n",
    "spark: SparkSession = SparkSession \\\n",
    "  .builder \\\n",
    "  .appName(\"Mi aplicacion\") \\\n",
    "  .config(\"spark.rdd.compress\", \"true\") \\\n",
    "  .config(\"spark.executor.memory\", \"3g\") \\\n",
    "  .config(\"spark.driver.memory\", \"3g\") \\\n",
    "  .master(SPARK_MASTER) \\\n",
    "  .getOrCreate()\n",
    "\n",
    "sc: SparkContext = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import SparseVector, Vectors\n",
    "\n",
    "# Define un array de 4 vectores dispersos, de 3 elementos cada uno\n",
    "sparseData: list[SparseVector] = [\n",
    "     Vectors.sparse(3, {1: 1.2}),\n",
    "     Vectors.sparse(3, {1: 1.1}),\n",
    "     Vectors.sparse(3, {0: 0.9, 2: 1.0}),\n",
    "     Vectors.sparse(3, {0: 1.0, 2: 1.1})\n",
    " ]\n",
    "\n",
    "for i in range(4):\n",
    "    print(sparseData[i].toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Convierte el array en un DataFrame\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "dfSD: DataFrame = sc.parallelize([\n",
    "  (1, sparseData[0]),\n",
    "  (2, sparseData[1]),\n",
    "  (3, sparseData[2]),\n",
    "  (4, sparseData[3])\n",
    "]).toDF([\"fila\", \"características\"])\n",
    "\n",
    "dfSD.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Creamos un modelo KMeans sin entrenar, con 2 clusters\n",
    "# para más opciones ver https://spark.apache.org/docs/latest/ml-clustering.html\n",
    "# y https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.clustering.KMeans.html\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans: KMeans = KMeans()\\\n",
    "    .setInitMode(\"k-means||\")\\\n",
    "    .setFeaturesCol(\"características\")\\\n",
    "    .setPredictionCol(\"predicción\")\\\n",
    "    .setK(2)\\\n",
    "    .setSeed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Ajustamos el modelo al DataFrame anterior y mostramos los centros de los clusters\n",
    "from pyspark.ml.clustering import KMeansModel\n",
    "\n",
    "kmModel: KMeansModel = kmeans.fit(dfSD)\n",
    "print(\"Centros de los clusters: {0}.\".format(\n",
    "    kmModel.clusterCenters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Vemos cómo el modelo clusteriza los datos del array anterior\n",
    "predicciones: DataFrame = kmModel.transform(dfSD)\n",
    "predicciones.show()\n",
    "\n",
    "# Calcula el coste como la suma de la distancia al cuadrado entre los puntos de entrada\n",
    "# y los centros de los clusters correspondientes\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"características\", predictionCol=\"predicción\")\n",
    "silhouette = evaluator.evaluate(predicciones)\n",
    "print(\"Coste = {0}.\".format(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Probamos el modelo con otros puntos\n",
    "dfTest: DataFrame = sc.parallelize([\n",
    "  (1, Vectors.sparse(3, {0: 0.9, 1:1.0, 2: 1.0})),\n",
    "  (2, Vectors.sparse(3, {1: 1.5, 2: 0.3}))\n",
    "]).toDF([\"fila\", \"características\"])\n",
    "\n",
    "predicciones: DataFrame = kmModel.transform(dfTest)\n",
    "predicciones.show(truncate=False)\n",
    "\n",
    "# Calcula el coste como la suma de la distancia al cuadrado entre los puntos de entrada\n",
    "# y los centros de los clusters correspondientes\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"características\", predictionCol=\"predicción\")\n",
    "silhouette = evaluator.evaluate(predicciones)\n",
    "print(\"Coste = {0}.\".format(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Salva el modelo en un directorio\n",
    "kmModel.save(\"/tmp/kmModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Vuelve a cargar el modelo\n",
    "sameModel: KMeansModel = KMeansModel.load(\"/tmp/kmModel\")\n",
    "\n",
    "predicciones: DataFrame = kmModel.transform(dfTest)\n",
    "predicciones.show(truncate=False)\n",
    "\n",
    "# Calcula el coste como la suma de la distancia al cuadrado entre los puntos de entrada\n",
    "# y los centros de los clusters correspondientes\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"características\", predictionCol=\"predicción\")\n",
    "silhouette = evaluator.evaluate(predicciones)\n",
    "print(\"Coste = {0}.\".format(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "format": "text/plain",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -l /tmp/kmModel/data\n",
    "rm -rf /tmp/kmModel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
